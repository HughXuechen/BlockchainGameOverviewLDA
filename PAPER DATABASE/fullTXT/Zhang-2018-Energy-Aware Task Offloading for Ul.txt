Energy-Aware Task Offloading for Ultra-Dense Edge Computing
Energy-Aware Task Offloading for Ultra-Dense
Edge Computing
Jie Zhang∗, Hongzhi Guo∗, and Jiajia Liu∗
∗School of Cyber Engineering, Xidian University, Xi’an, Shaanxi, 710071, China
†E-mail: {hzguo,liujiajia}@xidian.edu.cn
Abstract—The rapid development of Internet of Things raises
higher requirements on traffic volume and user connectivity
density for 5G networks, and ultra-dense networks (UDN) are
envisioned to be a developing trend to meet these demands.
Meanwhile, with the increasing number of mobile devices (MDs)
and computationally intensive applications, the conflict between
the MDs limited computing and battery resources and the ever-
increasing resource demands from the mobile applications be-
comes more and more prominent. To resolve these issues, mobile
edge computing is expected to be a potential solution. Note that
existing works on computation offloading in ultra-dense networks
mostly focused on the problem of task offloading decision making
and paid little attention to the MDs’ status information, such
as remaining battery and computation frequency. Toward this
end, we provide this paper study the task offloading for ultra-
dense edge computing, where the MDs’ remaining battery and
computation frequency are taken into account, and an energy-
aware game theoretical offloading scheme is proposed as our
solution. Numerical results show that our energy-aware task
offloading scheme can not only save the MDs’ energy efficiently
but also achieve a delay-optimal task offloading decision profile
for the tasks.
Index Terms—Mobile edge computing, ultra-dense network,
energy-aware offloading, mobile edge computing offloading, game
theory.
I. INTRODUCTION
In recent year, with the rapid development of the Internet of
Things and the emergence of various kinds of smart devices
(e.g., smart phone, smart watch, and virtual reality glass), it
can be foreseen that each of us will have a large number of
devices and will need ubiquitous connections every now and
then. This will impose higher requirements (e.g., low latency,
and high reliability) and bring more challenges on many
aspects of 5G networks [1], [2], [3]. In order to meet these
requirements, ultra-dense networks are widely considered to
be a promising technology in the future [4], [5].
It is worth noting that a large number of computationally
intensive applications such as face recognition, real-time trans-
lation, and interactive games, etc., have emerged with the
increasing number of smart devices in out lives. Usually these
applications have high requirements on computing resources,
while the MDs have limited computing capability and battery
life [6], [7]. In order to resolve this contradiction, MEC
offloading is considered as a good solution, and there has been
a lot of research emerged in this area over the past few years
[8], [9], [10], [11], [12]. For example, in order to minimize
the energy consumption of the MDs, Mao et al. [8] studied
stochastic joint radio and computational resource management
in multi-user single-MEC scenario, and proposed an online
algorithm based on Lyapunov optimization to determine the
optimal CPU-cycle frequency for local execution, the optimal
transmission power and the radio bandwidth for task offload-
ing.
However, most existing research on MEC offloading only
considers the single-MEC server scenario. Although some
researchers have started to focus on the problem of MEC
offloading in ultra-dense networks recently [13], [14], [19],
they only focused on the task offloading decision making, and
paid little attention to the status information of the terminal
device itself. In fact, these information will have a great impact
on the performance of the MEC offloading. Toward this end,
we investigate a MEC offloading scenario with multiple MEC
servers in ultra-dense networks, where the MDs’ remaining
power and computation frequency are taken into considera-
tion. An energy-aware game-theoretic offloading algorithm is
proposed so as to minimize the task execution delay and the
MDs’ energy consumption, by jointly optimizing the CPU-
cycle frequency and the offloading decision. In particular, the
main contributions of this paper are summarized as follows.
• We investigate the task offloading problem for ultra-dense
edge computing with multiple edge servers, where the
remaining battery and the computation frequency of the
MDs are taken into account, and present our mathematical
model for this problem.
• For reducing the overall computation overhead of the
MDs, we introduce a CPU-cycle frequency scheduling
mechanisms so as to minimize the local computation
overhead of the tasks.
• In order to solve the optimization problem, we first
propose an enumeration offloading algorithm, which can
obtain an optimal solution but has a high complexity. To
reduce the complexity of the algorithm, we further present
a game theory-based offloading algorithm, and verify its
performance through simulation experiment.
The remainder of this paper is organized as follows. In
section II, we discuss the related works. In section III, we
introduce the system model. The problem formulation and the
solution of the problem are introduced in section IV. Section
V presents our numerical results and section VI concludes the
whole paper.
720
2018 IEEE Confs on Internet of Things, Green Computing and Communications, Cyber, Physical and Social Computing,
Smart Data, Blockchain, Computer and Information Technology, Congress on Cybermatics
978-1-5386-7975-3/18/$31.00 ©2018 IEEE
DOI 10.1109/Cybermatics_2018.2018.00144
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:48:23 UTC from IEEE Xplore.  Restrictions apply. 
II. RELATED WORKS
In the field of MEC, the computation offloading has been
attracting more and more attention in recent years [9], [12],
and there are many papers focusing on the computation
offloading problem [8], [10], [11]. Guo et al. [10] investi-
gated the problem of cloud-MEC collaboration computation
offloading over fiber-wireless networks, and proposed two
schemes to solve this problem, i.e., an approximation collab-
orative computation offloading scheme, and a game-theoretic
collaborative computation offloading scheme. The Numerical
results corroborate that their two solutions can achieve great
offloading performance. Zhang et al. [16] proposed an iter-
ative search algorithm to find the optimal solution for their
energy-aware computation task offloading schemes in multi-
cell networks, which jointly optimized computation offloading
decision, communication and computation resource allocation.
Mao et al. [8] studied a multi-user single-MEC computation
offloading scenario, and proposed an online algorithm based
on Lyapunov optimization to determine the optimal CPU-cycle
frequency for local execution, the optimal transmission power
and the radio bandwidth for task offloading.
Moreover, with the development of ultra-dense networks,
there are some researchers beginning to focus on the problem
of MEC offloading in ultra-dense networks recently [13], [19].
Chen et al. [13] studied the task offloading problem in ultra-
dense network with the goal of minimizing the execution
delay and energy consumption, and proposed an efficient
offloading scheme by transforming the optimization problem
into task placement and resource allocation sub-problems to
solve the problem. Guo et al. [19] proposed a two-tier game-
theoretic greedy offloading scheme in order to solve the MEC
offloading problem in ultra-dense IoT networks, and verify the
superior performance of multiple edge servers in ultra-dense
IoT networks.
Howerver, most researches of MEC offloading in ultra-
dense networks only focused on the task offloading decision
making, and paid little attention to the status information of
the terminal device itself. Different from these studies, we
take the MDs’ remaining power and computation frequency
into account, and investigate a MEC offloading scenario with
multiple MEC servers in ultra-dense networks. Furthermore,
we propose an energy-aware game-theory-based offloading
algorithm that jointly optimizes the CPU-cycle frequency and
the offloading decision of MDs in order to minimize the overall
computation overhead. At the same time, we also consider a
more reasonable weight factor that depends on the remaining
power of MDs for the trade-off between execution delay and
energy consumption.
III. SYSTEM MODEL
As shown in Fig. 1, we consider an ultra-dense edge
computing scenario which contains one MBS, N SCs and D
MDs in ultra-dense network, and use N = {1, 2, ..., N} and
D = {1, 2, ..., D} to represent the sets of SCs and MDs,
respectively. For each MD, we assume that there is only
one task to be executed at a time and we can denote it by
Fig. 1. Task Offloading for Ultra-Dense Edge Computing.
Ti = (mi, ci, T
MAX
i ), ∀i ∈ D, where mi is the input data
size of the task Ti, ci is the total CPU cycle required to finish
the task, and TMAX
i denotes the maximum execution delay
that the task can tolerate. Similar to many previous mobile
edge computing offloading research [17], [18], we consider
a quasi-static scenario where the set of the MDs D remains
unchanged during the computation offloading period.
Futhermore, we assume that there is only one MEC server
connected to each MBS and SC via an optical fiber link, which
can provides computing capabilities for the MDs. The MDs
communicate with the MBS and the SCs through the wireless
link, and the number of wireless channels of the MBS and
the SC are M and S, respectively. We can offload part of the
computation-intensive tasks of the MDs to the MEC server via
the MBS or SC, so that the tasks’ processing latency and the
MDs’ energy consumption can be reduced. Obviously, there
are N + 2 offloading strategies for each computation task,
that is, executing locally on the CPU of the MD, offloading
to the MEC server connected to the MBS, and offloading to
the MEC server connected to a certain one of the N SCs.
For the sake of simplicity, we use di ∈ {0,−1, 1, ..., N} to
represent the offloading strategy of MD i, where di = 0 means
that MD i decides to accomplish its computation task at its
own CPU, di = −1 means that MD i decides to offload its
computation task to the MEC server connected to the MBS,
and di = j(1 ≤ j ≤ N) means that MD i chooses to offload
its computation task to the MEC server connected to SC j The
correspondence between them is shown in Table I.
TABLE I
CORRESPONDENCE BETWEEN di AND MD i’S OFFLOADING
STRATEGY.
value of di offloading strategy of MDi
di = 0 executing locally
di = −1 offloading to the MEC server
connected to the MBS
di = j(1 ≤ j ≤ N )
offloading to the MEC server
connected to the SC j
721
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:48:23 UTC from IEEE Xplore.  Restrictions apply. 
A. Executing Locally
As Table I shows, when di = 0, MD i will execute its task
Ti on its own CPU. In this case, the execution time of task Ti
can be calculated as
tlocali =
ci
f l
i
(1)
where f l
i is the CPU frequency of MD i and its value depends
on the specific task as shown below. Meanwhile, the energy
consumption for executing the task can be given as
elocali = ci · γi (2)
where γi is the energy consumpution per CPU cycle of MD
i, which can be calculated as γi = κ · (f l
i )
2. κ is a factor that
depends on the CPU architecture [15], and we set κ = 10−14
in order to meet the actual situation.
In this paper, we consider the weigthed sum of the execution
delay and energy consumption of the task, and introduce the
weight parameter αt
i ∈ [0, 1] to represent the MD i’s require-
ment on execution delay. To represent the MDs’ remaining
power and its impact, the following parameter is introduced
[16]
rPi =
P remain
i
P total
i
where P remian
i denotes the remaining power of MD i
presently, and P total
i is the total power of MD i.
In particular, The parameter rPi represents a state of MD i
in which MD i will impose different requirements on the exe-
cution delay and energy consumption of its task. For example,
smaller value of rPi indicates that the MD i’s remaining battery
power is low, and MD i requires lower energy consumption
to extend the battery life. Then, we can redefine the weight
parameter of the execution delay as
αt′
i = αt
i · rPi (3)
and the weight parameter of the energy consumption can be
denoted as αe′
i = 1− αt′
i .
After that, the corresponding overhead of task Ti can be
calculated as
Δlocal
i = αt′
i · tlocali + αe′
i · elocali (4)
B. Edge Computing Model
If di �= 0, it means that MD i will offload its task Ti to
the MEC server. Considering the multi-MEC server model we
introduced, we will discuss the following two scenarios:
1) Executing on the MEC server connected to the MBS:
Here we consider the situation where MD i decides to offload
its computation task Ti to the MEC server via the MBS, at
which di = −1, and the processing time of a task contains
the transmission time of the input data of Ti and the execution
time of Ti, which will be calculated in following. Note that
we ignore the transmission of the output data, this is because
that the output data is usually very small for most mobile
applications such as face recognition, iris recognition, etc.,
and its impact on the overall computation overhead can be
neglected.
The transmission time of the input data consists of two
aspects: transmitting the data from MD i to the MBS via
the wireless network and transmitting it from the MBS to the
MEC server via the fiber link. We denote rmbs
i as the uplink
data transmission rate from MD i to the MBS, and it can be
calculated as
rmbs
i = ωmlog2(1 +
pi · gmi
σ +
∑
h∈D,\{i}:dh=di
ph · gmh
),
where ωm is the wireless channel bandwidth of the MBS, gmi
and σ denote the channel gain between MD i and the MBS
and the background noise power, respectively. We use c to
represent the uplink data transmission rate from the MBS to
the MEC server. After that, the time for transmitting the input
data from MD i to the MEC server can be given as
tmbstrans
i =
mi
rmbs
i
+
mi
c
(5)
For the execution of the task Ti, we apply the M/M/1 queue
model to calculate its execution time as
tmbsexe
i =
ci
fmbs − λm
, (6)
where fmbs denotes the computation capability of the MEC
server connected to the MBS, λm is the average arriving rate
of the tasks that are offloaded to the MEC server connected
to the MBS, which can be calculated as
λm =
D∑
i=1
ci · I{di=−1} (7)
here, I{#} is an indicator function, and when the condition #
is true, I{#} = 1, otherwise, I{#} = 0.
Finally, the execution delay of the task Ti can be calculated
as
tmbs
i =
mi
rmbs
i
+
mi
c
+
ci
fmbs − λm
. (8)
and the energy consumpution for executing the task is:
embs
i = pi · tmbs
i . (9)
where pi is the transmit power of the MD i.
Similar to the situation that Ti is executed locally, the overall
computation overhead can be obtained by
Δmbs
i = αt′
i · tmbs
i + αe′
i · embs
i . (10)
2) Executing on the MEC server connected to the SC:
In addition to offloading computation tasks via the MBS, the
MDs can also offload them via the SCs. In this case, di =
j (1 ≤ j ≤ N) denote that MD i decides to offload its task Ti
to the MEC server via the SC j. Referring the above case that
MD i offloads its task to the MEC server via the MBS, we
can obtain the uplink data transmission rate and the average
arriving rate of the tasks at the MEC server connected to the
SC j as
rscji = ωj log2(1 +
pi · gji
σ +
∑
h∈D,\{i}:dh=di
ph · gjh
),
722
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:48:23 UTC from IEEE Xplore.  Restrictions apply. 
and
λj =
D∑
i=1
ci · I{di=j},
where ωj and gsi are the wireless channel bandwidth of the
SCj, the wireless channel gain between MDi and the SCj
respectively.
Then the execution delay, the energy consumpution and the
overall computation overhead of the task Ti that offloaded to
the MEC server connected to the SCj can be given by the
following equation:
tscji =
mi
rsci
+
mi
c
+
ci
fsc − λs
(11)
escji = pi · tscji . (12)
Δscj
i = αt′
i · tscji + αe′
i · escji . (13)
IV. PROBLEM FORMULATION AND SOLUTIONS
A. Problem Formulation
From our above discussions of the system model, we can
see that different offloading decisions of the MDs will lead
to different overall computation overhead. So we can use the
function related to the offloading decision of MD i di to
represent the MD i’s overall computation overhead Δi, that is
Δi(di) =
⎧⎪⎨
⎪⎩
Δlocal
i , if di = 0,
Δmbs
i , if di = −1,
Δscj
i , if di = j, 1 ≤ j ≤ N.
(14)
Moreover, due to the wireless channel interference, the
uplink data transmission rate will decrease as more tasks are
offloaded to the MBS or SCs. Our goal is to design an efficient
computation offloading scheme that simultaneously optimizes
the MDs’ CPU-cycle frequency and offloading decisions of the
tasks so that the overall computational overhead of all MDs
can be minimized under given wireless channel and maximum
execution delay constraints. In particular, this problem can be
defined as follows.
Definition 1. computation Overhead Minimization in Ultra-
dense Edge Computing (OMUEC): Given the initial infor-
mation of the MDs and their computation tasks such as the
input data size, the required CPU cycles to accomplish the
tasks, etc., the computing capabilities of the MEC servers
connected to the MBS and the SCs, the uplink data rate
of the wireless, the problem of OMUEC is to minimize the
overall computation overhead in execution delay and energy
consumption while meeting the wireless channel and the
maximum execution delay constraints.
Based on this, we can mathematically formulated the prob-
lem of OMUEC as
minΔall =
D∑
i=1
Δi(di)
s.t. C1 :
D∑
i=1
I{di=−1} ≤M, ∀i ∈ D,
C2 :
D∑
i=1
I{di=j} ≤ S, 1 ≤ j ≤ N, ∀i ∈ D,
C3 : tlocali · I{di=0} + tmbs
i · I{di=−1} + tscji · I{di=j}
≤ TMAX
i , 1 ≤ j ≤ N, ∀i ∈ D,
C4 : elocali · I{di=0} + embs
i · I{di=−1} + escji · I{di=j}
≤ P remain
i , 1 ≤ j ≤ N, ∀i ∈ D,
C5 : f l
MIN ≤ fL
i ≤ f l
MAX , ∀i ∈ D,
C6 : di ∈ {1,−1, 1, 2, ..., N}, ∀i ∈ D.
(15)
Here, considering the limitations of the communication
system and the requirements of the tasks, a series of constraints
are included. C1 and C2 limit the number of the MDs that can
be connected to the MBS and SC j according to the number
of wireless channels of the MBS and SC j, C3 and C4 are the
limitations on the execution delay and energy consumputation
of tasks. As for C5 and C6, they denote the range of the
MDs’ CPU frequency and the set of the offloading decisions,
respectively.
B. Solutions
1) CPU-cycle frequency scheduling: The CPU-cycle fre-
quency of the MDs is the main factor that affects the local
overhead, so we can reduce the local overhead by scheduling
the CPU-cycle frequency of the MDs. Then the minimization
overhead of local computing on MD i can be denoted as
minΔlocal
i (f l
i ) = αt′
i ·
ci
f l
i
+ αe′
i · ci · κ · (f l
i )
2 (16)
Consider that the derivative of Δlocal
i (f l
i ) with respect to f l
i
is zero when
f l′
i = 3
√
αt′
i
2αe′
i κ
, (17)
we can obtain that the value of Δlocal
i (f l
i ) decreases
monotonously with the value of f l
i while f l
i < f l′
i , and
increases monotonously with the value of f l
i while f l
i > f l′
i .
According to the constraints C3, C4 and C5 in (15), we can
redefine the range of the value of f l
i as
fLMIN
i = max{f l
MIN ,
ci
TMAX
i
} ≤ f l
i
≤ min{f l
MAX ,
√
P remain
i
κci
} = fLMAX
i
(18)
723
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:48:23 UTC from IEEE Xplore.  Restrictions apply. 
After that, the Δlocal
i (f l
i ) can be calculated as
Δlocal∗
i (f l
i ) =
⎧⎪⎨
⎪⎩
Δlocal
i (fLMIN
i ) if f l′
i ≤ fLMIN
i
Δlocal
i (f l′
i ) if fLMIN
i < f l′
i ≤ fLMAX
i
Δlocal
i (fLMAX
i ) if f l′
i > fLMAX
i
(19)
2) Offloading decision: In order to solve the above
OMUEC problem, we first adopt a brute force method that
enumerates all optional computation offloading decisions of all
the tasks and picks out an optimal offloading decision profile
for all tasks. The details of this algorithm are briefly given in
Algorithm 1.
Algorithm 1 An enumeration offloading algorithm(EMOA)
Input: the sets of the SCs and the MDs N ,D, the information
of the MDs’ tasks, the computing capabilities of the MEC
servers connected to the MBS and the SCs, the uplink data
rate in wireless networks.
Output: the offloading decision profile and the overall mini-
mum computation overhead ΔMIN
1: enumerate all possible offloading decision combinations,
2: discard these offloading decision combinations that cannot
meet the constraints C1, C2, C3 and C4 in (15),
3: calculate the overhead of all remaining offloading decision
combinations and derive the minimum overhead ΔMIN .
4: return the offloading decision profile and the overall
minimum computation overhead ΔMIN .
Obviously, the EMOA in Algorithm 1 can obtain an optimal
solution to the OMUEC problem because it enumerates all
optional computation decisions combination. However, it has
extremely high complexity, i.e., O(3n), where n is the number
of the computation tasks. So this algorithm is only suitable for
scenarios with small number of computation tasks and cannot
be used in practice. Consider that there are usually a lot of
computation tasks in the actual situation, we further propose
another game theory-based algorithm. In this algorithm, each
MD is treated as a rational game player and chooses its optimal
offloading decision according to the current state of the system
at each decision slot. According to the game theory, all the
MDs will self-organize into Nash equilibrium within a limited
number of steps so as to minimize the overall computation
overhead.
To simplify our following algorithm discussions, some nota-
tions are predefined as follows. Let di(t) ∈ {0,−1, 1, 2, ..., N}
denotes the computation offloading decision of MD i in
decision slot t, where di(t) = 0 denotes that MD i decides
to execute its computation task Ti at its own CPU in slot
t, di(t) = −1 denotes that MD i decides to offload its
computation task Ti to the MEC server connected to the MBS
in slot t, and di(t) = j(1 ≤ j ≤ N) means that MD i
decides to offload its computation tasks Ti to the MEC server
connected to SC j in slot t. Let D(t) denotes the set of
MDs whose computation offloading decision profiles should
be updated in decision slot t.
The details of the game theory-based offloading algorithm
are briefly given in Algorithm 2. Firstly, we set that di =
0, ∀i ∈ D and D(t) = Ø, that is, all the MDs choose to
accomplish its computation tasks locally and no MD needs to
update its offloading decision profile. After that, the algorithm
starts the iterative process, and each MD chooses its optimal
offloading decision according to the decision profiles of other
MDs and the current wireless channel interference at each
decision slot t. Then, the MDs in D(t) contend for the
offloading decision update opportunity. The MD that obtains
the update opportunity will update its offloading decision and
broadcast the update message to other MDs. After a finite
number of iterations, all the MDs self-organizes into Nash
equilibrium. Finally, the overall computation overhead can be
calculated according to the final offloading decision profiles
of all MDs.
Algorithm 2 A game theory-based offloading algo-
rithm(GTOA)
Input: the sets of the SCs and the MDs N ,D, the information
of the MDs’ tasks, the computing capabilities of the MEC
servers connected to the MBS and the SCs, the uplink data
rate in wireless networks.
Output: the offloading decision profile and the overall mini-
mum computation overhead ΔMIN
Initialize: di = 0, ∀i ∈ D, D(t) = Ø,
1: for all i ∈ D do
2: call Procedure 1 to compute the optimal local CPU-
cycle frequency f l
i for task Ti;
3: end for
4: for each decision slot t do
5: for all i ∈ D do
6: compute current optimal offloading decision in next
slot t+ 1;
7: if di(t+ 1) �= di(t) then
8: store MD i to D(t);
9: end if
10: end for
11: while D(t) �= Ø do
12: each MD in D(t) contends for the offloading decision
profile update opportunity;
13: if MD i wins the update opportunity then
14: di = di(t+ 1);
15: broadcast the update message to other MDs;
16: else
17: di = di(t);
18: end if
19: end while
20: end for
21: compute the overall minimum computation overhead
ΔMIN ;
22: return the offloading decision profile and the overall
minimum computation overhead ΔMIN .
724
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:48:23 UTC from IEEE Xplore.  Restrictions apply. 
Procedure 1 A local computing optimization procedure
1: for all i ∈ D do
2: compute f l′
i , f
LMIN
i , fLMAX
i
3: if f l′
i ≤ fLMIN
i then
4: f l
i = fLMIN
i
5: else if fLMIN
i < f l′
i ≤ fLMAX
i then
6: f l
i = f l′
i
7: else
8: f l
i = fLMAX
i
9: end if
10: end for
11: return the optimal CPU-cycle frequency profile for all
the MDs.
V. NUMERICAL RESULTS
In this section, we give a series of simulation experiments
and numerical results in order to verify the performance of
our proposed game theory-based offloading algorithm. Without
loss of generality, we consider a multiple MEC servers sce-
nario where there are a number of MDs covered by an MBS
and N SCs. We assume that the MBS has 100 orthogonal
wireless channels, and each SC has 20 orthogonal wireless
channels. The wireless bandwidth of the MBS and the SCs
is set to 40 MHz and 20 MHz, respectively. For each MD,
we assume that there is only one task to be accomplished in a
time. The input data size of the task Ti(∀i ∈ D) is set between
300 KB and 800 KB, the number of its required CPU cycles
to accomplish the task is randomly generated between 100
Megacycles and 1000 Megacycles, and its maximum execution
delay is set between 0.5 s and 5 s. Moreover, the transmit
power of the MDs is set to 100 mW, and the computing
capabilities of the MEC servers connected to the MBS and the
SCs are separately 8 GHz and 4 GHz. The background noise
power σ is set to -100 dBm, and the uplink data rate of the
optical fiber link is 1 Gbps. In order to compute the overall
computation overhead of the task Ti, the weight factors αt
i
and αe
i are randomly assigned from the set [0, 1], and we have
that αt
i + αe
i = 1. We show the parameters settings in our
simulation experiments in Table II.
In order to verify the performance of our proposed GTOA
algorithm, we first compare the overall minimum computation
overhead and the running time obtained by adopting EMOA
algorithm and GTOA algorithm, and the number of MDs
increases from 3 to 12 due to the high complexity of EMOA
algorithm. The numerical results for these two groups of
experiments are shown in Fig. 2 and Fig. 3. From Fig. 2,
we can see that our proposed GTOA algorithm can obtain
a near-optimal solution to the OMUEC problem. Moreover,
the numerical results in Fig. 3 show that the running time
of GTOA algorithm does not increase significantly when the
number of the MDs increases, that is, our proposed GTOA
algorithm has a higher operating efficiency.
Furthermore, in order to evaluate the convergence of our
proposed GTOA algorithm, we conducted two groups of
TABLE II
PARAMETER SETTINGS IN OUR SIMULATION EXPERIMENTS.
notation description value
D number of MDs 50
N number of SCs 50
M
number of orthogonal wireless
100
channels owned by the MBS
S
number of orthogonal wireless
20
channels owned by the SC
ωm wireless bandwidth of the MBS 40 MHz
ωj wireless bandwidth of the SC 20 MHz
mi input data size of the task Ti 300 - 800 KB
ci
the number of required
100 - 1000 MegacyclesCPU cycle to accomplish
the task Ti
TMAX
i
maximum execution delay
0.5 - 5 s
of the task Ti
pi transmit power of the MD 100 mW
fmbs
computing capability of the
8 GHzMEC server connected to
the MBS
fscj
computing capability of the
4 GHzMEC server connected to
the SC j
σ background noise power -100 dBm
c
uplink data rate of the
1 Gbps
optical fiber link
Fig. 2. Comparisons of the overall minimum computation overhead between
EMOA and GTOA with different numbers of MDs.
Fig. 3. Comparisons of the running time between EMOA and GTOA with
different numbers of MDs.
725
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:48:23 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 4. Convergence behavior of GTOA in terms of the overall computation
overhead with D = 100.
Fig. 5. Average iterations for the convergence of GTOA with different number
of computation tasks.
experiments. In Fig. 4, we studied the convergence behavior
of our GTOA algorithm in terms of the overall computation
overhead with D = 100, the numerical results shows that all
the MDs will self-organize into a Nash equilibrium after a
finite number of iterations. Moreover, Fig. 5 is the average
iterations for the convergence of GTOA algorithm with dif-
ferent number of MDs. We can see that the average iterations
of GTOA are nearly linear with the number of MDs, and it
demonstrate that the convergence of GTOA algorithm is very
fast.
Finally, to demonstrate the necessity of introducing mobile
edge computing and the MEC servers connected to the SCs,
we compare the the overall computation overhead obtained
by computing all the tasks locally, offloading without SCs
and our proposed GTOA approach. The numerical results of
the experiment are shown in Fig. 6. We can see that the
introduction of mobile edge computing can greatly reduce
the overall computation overhead of MDs, and the offloading
solution with multiple MEC servers has superior performance
than the offloading solution with only one MEC server.
Fig. 6. Comparison results of the overall minimum computation overhead by
adopting different offloading schemes, i.e., computing locally at MDs’ CPU,
computation offloading without SCs, and our GTOA scheme.
VI. CONCLUSION
In this paper, we studied the task offloading problem for
ultra-dense edge computing with multiple edge servers, where
the remaining battery and the computation frequency of the
MDs are taken into account, and presented some mathematical
models for this problem. To solve this problem, we propose
a game-theory-based task offloading scheme as our solu-
tion, which jointly optimizes local execution and computation
resources allocation. Numerical results show that our task
offloading scheme can reduce the overall computation over-
head efficiently. For future work, it should be meaningful to
investigate the task offloading problem with moving users/edge
servers, and design contineous optimization schemes for them.
REFERENCES
[1] J. A. Stankovic, “Research Directions for the Internet of Things,” IEEE
Internet of Things Journal, vol. 1, no. 1, pp. 3-9, Feb. 2014.
[2] A. Gharaibeh, A. Khreishah, M. Mohammadi, A. Al-Fuqaha, I. Khalil,
and A. Rayes, “Online Auction of Cloud Resources in Support of the
Internet of Things,” IEEE Internet of Things Journal, vol. 4, no. 5, pp.
1583-1596, Oct. 2017.
[3] J. Liu, H. Guo, H. Nishiyama, H. Ujikawa, K. Suzuki, and N. Kato,
“New Perspectives on Future Smart FiWi Networks: Scalability, Relia-
bility, and Energy Efficiency,” IEEE Communications Surveys Tutorials,
vol. 18, no. 2, pp. 1045-1072, Seconquater 2016.
[4] S. Zhang, N. Zhang, S. Zhou, J.Gong, Z. Niu, and X. Shen, “Energy-
Sustainable Traffic Steering for 5G Mobile Networks,” IEEE Commu-
nications Magazine, vol. 55, no. 11, pp. 54-60, Nov.2017.
[5] L. P. Qian, Y. Wu, H. Zhou, and X. Shen, “Dynamic Cell Association
for Non-Orthogonal Multiple-Access V2S Networks,” IEEE Journal on
Selected Areas in Communications,, vol. 35, no. 10, pp.2342-2356, Oct.
2017.
[6] T. Soyata, R. Muraleedharan, C. Funai, M. Kwon, and W. Heinzelman,
“Cloud-Vision: Real-Time Face Recognition Using a Mobile-Cloudlet-
Cloud Acceleration Architecture,” in ISCC, 2012.
[7] H. Ujikawa, T. Yamada, K. I. Suzuki, A. Otaka, H. Nishiyama, and
N. Kato, “Stand-Alone and Cooperative Deep Sleep for Battery-Driven
Optical Network Unit,” IEEE Internet of Things Journal, vol. 3, no. 4,
pp. 494-502, Aug. 2016.
[8] Y. Mao, J. Zhang, S. H. Song, and K. B. Letaief, “Stochastic Joint Radio
and Computational Resource Management for Multi-User Mobile-Edge
Computing Systems,” IEEE Transactions on Wireless Communications,
vol. 16, no.9, pp. 5994-6009, Sept. 2017.
726
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:48:23 UTC from IEEE Xplore.  Restrictions apply. 
[9] P. Mach, and Z. Becvar, “Mobile edge computing: A survey on ar-
chitecture and computation offloading,” IEEE Communications Surveys
Tutorials, 2017, doi: 10.1109/COMST.2017.2682318.
[10] H. Guo, and J. Liu, “Collaborative Computation Offloading for Multi-
Access Edge Computing over Fiber-Wireless Networks,” IEEE Trans-
actions on Vehicular Technology, vol. 67, no. 5, pp. 4514-4526, May
2018.
[11] H. Guo, J. Liu, and J. Zhang, “Efficient Computation Offloading for
Multi-Access Edge Computing in 5G HetNets,” IEEE ICC, 2018.
[12] N. Abbas, Y. Zhang, A. Taherkordi, and T. Skeie, “Mobile edge
computing: A survey,” IEEE Internet of Things Journal, 2017, doi:
10.1109/JIOT.2017.2750180.
[13] M. Chen and Y. Hao, “Task Offloading for Mobile Edge Computing in
Software Defined Ultra-dense Network,” in IEEE Journal on Selected
Areas in Communications, 2018, doi: 10.1109/JSAC.2018.2815360.
[14] A. C. Pang, W. H. Chung, T. C. Chiu, and J. Zhang, “Latency-
Driven Cooperative Task Computing in Multi-user Fog-Radio Access
Networks,” in ICDCS, 2017.
[15] A. P. Miettinen, and J. K. Nurminen, “Energy efficiency of mobile clients
in cloud computing,” in Usenix Conference on Hot Topics in Cloud
Computing, 2010.
[16] J. Zhang, X. Hu, Z. Ning, E. C.-H. Ngai, L. Zhou, J. Wei, J. Cheng,
and B. Hu, “Energy-latency Trade-off for Energy-aware Offloading in
Mobile Edge Computing Networks,” IEEE Internet of Things Journal,
2017, doi: 10.1109/JIOT.2017.2786343.
[17] H. Guo, J. Liu, and H. Qin, “Collaborative Mobile-Edge Computation
Offloading for IoT over Fiber-Wireless Networks,” IEEE NETWORK,
vol. 32, no. 1, pp. 66-71, Jan./Feb. 2018.
[18] H. Guo, J. Liu, and H. Qin, “Collaborative Computation Offloading for
Mobile-Edge Computing over Fiber-Wireless Networks,” IEEE GLOBE-
COM, 2017.
[19] H. Guo, J. Liu, J. Zhang, W. Sun, and N. Kato, “Mobile-Edge Com-
putation Offloading for Ultra-Dense IoT Networks,” IEEE Internet of
Things Journal, 2018, doi: 10.1109/JIOT.2018.2838584.
727
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:48:23 UTC from IEEE Xplore.  Restrictions apply.