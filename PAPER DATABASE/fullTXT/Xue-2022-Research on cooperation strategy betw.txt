Research on cooperation strategy between wind power and electric vehicle aggregators based on multi-agent and Block chain
Research on cooperation strategy between wind power and
electric vehicle aggregators based on multi-agent and Block
chain
Honglin Xue
State Grid Shanxi Electric Power
Company Information and
Communication Branch, China
15698555280@sina.cn
Junwei Ma
State Grid Shanxi Electric Power
Company Information and
Communication Branch, China
506963618@qq.com
Jian Wu
State Grid Shanxi Electric Power
Company Information and
Communication Branch, China
271462605@qq.com
Jing Duan
State Grid Shanxi Electric Power
Company Information and
Communication Branch, China
catduanjing@163.com
Haitao Li
State Grid Shanxi Electric Power
Company Information and
Communication Branch, China
2197086883@qq.com
Jianfeng Meng
Shanxi Tongzhiyi Technology Co.,
Ltd., China
451832143@qq.com
ABSTRACT
With the large-scale network association of wind vitality, the secure
and steady operation of the control framework is confronting chal-
lenges, and it is troublesome for wind control to realize productivity
within the control advertise. In later a long time, with the fast ad-
vancement of V2G innovation and the development of blockchain
technology, the vitality capacity characteristics of electric vehicles
have been tapped as an rising way to fathom the over issues. To
reverse the drawback of wind ranches in competing within the con-
trol market, this paper ponders the amusement approach between
electric vehicles and wind ranches to realize the levelable charac-
teristics of electric vehicle charging stack to level out the anti-peak
direction characteristics of wind control beneath the preface of
maximizing their particular interface. Given this, this paper sets up
the non-cooperative diversion and agreeable diversion models of
wind ranches and electric vehicle aggregators separately based on
diversion harmony hypothesis. To compensate for the impediments
of the diversion hypothesis approach to unravel the non-complete
data issue, this paper applies the multi-agent fortification learning
strategy to unravel the models.
CCS CONCEPTS
• Applied computing; • Computers in other domains; • Per-
sonal computers and PC applications; • Computer games;
KEYWORDS
Multi-agent Reinforcement Learning, Game Model, EV Aggregator,
Wind farm
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ICBCT’22, March 25–27, 2022, Shanghai, China
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9576-2/22/03. . . $15.00
https://doi.org/10.1145/3532640.3532664
ACM Reference Format:
Honglin Xue, Junwei Ma, Jian Wu, Jing Duan, Haitao Li, and Jianfeng Meng.
2022. Research on cooperation strategy between wind power and electric
vehicle aggregators based on multi-agent and Block chain. In The 2022
4th International Conference on Blockchain Technology (ICBCT’22), March
25–27, 2022, Shanghai, China. ACM, New York, NY, USA, 5 pages. https:
//doi.org/10.1145/3532640.3532664
1 INTRODUCTION
With the expanding consumption of fossil vitality sources and the
expanding unmistakable quality of climate and natural issues, wind
vitality has been abused on a huge scale due to its clean and renew-
able characteristics. With the development of wind control innova-
tion and the diminishing taken a toll of control era, it’ll ended up a
future drift for wind ranches and other ordinary power generators
to compete within the power showcase and settle at showcase tar-
iffs [1, 2]. Be that as it may, wind control has expansive instability
and vulnerability, and large-scale wind control lattice association
will bring various challenges to the secure and dependable opera-
tion of the framework [3]. In addition, due to the anti-peak control
characteristics of wind control, wind power is at a disadvantage to
compete in the power market and is difficult to achieve profitability
[4]. In recent years, wind power has become a research hotspot
by associating with flexible resources such as energy storage to
mitigate the above adverse effects [5-8].
With the development of the keen lattice and the quick advance-
ment of Vehicle to Lattice innovation, the energy storage character-
istics of electric vehicles have been tapped and are also a flexible
resource that cannot be ignored in the power system [9, 10]. Us-
ing EV onboard batteries as the energy storage medium to solve
the wind power uncertainty problem has become a new research
direction. Druitt et al. examined the potential part of electric ve-
hicles within the lattice containing a tall rate of wind control and
affirmed that network integration of electric vehicles advanced
wind control utilization [11]. EVs expending wind control require
synergistic planning at the specialized level of both, and Lee et al.
proposed a stochastic planning strategy of wind control abridge-
ment combined with EV charging stack to lighten the circumstance
of wind control oversupply [12]. Zhu YS et al. established a joint
166
https://doi.org/10.1145/3532640.3532664
https://doi.org/10.1145/3532640.3532664
https://doi.org/10.1145/3532640.3532664
http://crossmark.crossref.org/dialog/?doi=10.1145%2F3532640.3532664&domain=pdf&date_stamp=2022-07-07
ICBCT’22, March 25–27, 2022, Shanghai, China Honglin Xue et al.
scheduling model of wind power and electric vehicles and applied
an improved decomposition-based multi-objective evolutionary
algorithm to solve it [13]. Since electric vehicles and wind power
are different subjects, based on achieving synergistic dispatch at
the technical level, joint operation at the economic level is also
required. For example, Lee et al. proposed a joint operation mode
of wind power-electric vehicle charging stations [14]. Vaya and Hu
WH investigated the operational strategies of EV aggregators to
purchase electricity in the power market and provide balancing
services to wind power providers. Vasirani and Alahyari proposed
the virtual power plant consisting of wind turbines and electric
vehicles, argued that this mode has better economic efficiency, and
studied the operational strategy of virtual power plant operators to
participate in both the electricity market and the reserve market.
The above research results have laid a broad theoretical foundation
for the problem of wind power consumption by electric vehicles
from the technical and economic aspects. However, the research
on the joint operation mode is mostly about the operation strategy
to face the external environment together, while the game between
WFs (WF) and electric vehicle aggregators is less studied.
Since the actual electricity market is characterized by oligopolis-
tic competition and multiple players maximize their profits through
gaming behavior, game equilibrium models have been widely used
to analyze the strategic behavior of each player in the electricity
market and its impact on the economic efficiency of the market.
Zou P and Shahmohammadi developed a two-layer equilibrium
model to study the interaction between energy storage and wind
power when they compete in the electricity market together with
traditional power producers in different ways and the impact on
the market equilibrium outcome.
Based on the over foundation, this paper thinks about the diver-
sion approach of wind ranches and EV aggregators from the point
of view of the most prominent individual interface to realize the EVs
expending wind control. This paper builds up the non-cooperative
amusement show and agreeable amusement show concurring to
the diverse amusement approaches of WFs and EV aggregators.
To create up for the impediments of the diversion hypothesis ap-
proach, this paper employments the multi-agent support learning
strategy, i.e., the natural combination of cleverly optimization cal-
culation and diversion hypothesis strategy, to unravel the issue of
non-complete data diversion between wind ranches and electric
vehicle aggregators in numerous complex situations of the control
showcase.
2 MULTI-AGENT REINFORCEMENT
LEARNING
2.1 Theory
In terms of technology, blockchain involves math, cryptography,
the Internet, computer programming and many other scientific
and technological issues. From an application perspective, to put
it simply, blockchain is a distributed shared ledger and database
that is decentralized, tamper-free, whole-process traces, traceable,
collectively maintained, and open and transparent. These features
ensure the "honesty" and "transparency" of blockchain and lay the
foundation for creating trust in blockchain. However, the rich appli-
cation scenarios of blockchain are basically based on the fact that
Figure 1: The process of agent and environment interaction
of RL
blockchain can solve the problem of information asymmetry and
realize collaboration trust and concerted action between multiple
subjects.
Fortification learning could be a department of machine learning
that centers on understanding decision-making issues. The essential
thought of support learning is that an specialist interatomic with
the environment, the environment returns a certain compensate to
the specialist, and the specialist learns ceaselessly by rehashing the
over process to maximize the long-term remunerate, and at long
last gets the ideal approach to realize the goal.
The cyclic interaction process is shown in Figure 1
However, most real-life problems involve multiple subjects,
whose behaviors interact with each other and each subject ad-
justs its behavior through continuous experimentation, thus jointly
driving the evolution of the whole system. In this manner, the pon-
der of complex multi-subject framework issues requires the utilize
of multi-agent support learning, i.e., the method by which each
operator learns to progress its arrangement by collaboration with
the environment to get remunerate values and hence get the ideal
arrangement for the framework. In any case, in a multi-subject
framework, the learning prepare of the specialist gets to be com-
plex. To begin with, the number of combinations of activities of all
specialists develops exponentially with the number of operators,
and the framework measurement is exceptionally huge and com-
putationally complex; second, each operator influences each other,
and the operator should investigate not as it were the environment
but moreover the approaches of its peers, which influences the
meeting of the calculation; at last, the assignment of each operator
may be distinctive and influence each other, the rewards’ plan gets
to be complicated and specifically influences the goodness of the
learning arrangement..
Since multi-agent frameworks may include connections between
specialists such as participation and competition, multi-agent sup-
port learning presents the concept of the amusement based on
single-agent fortification learning, and the combination of amuse-
ment hypothesis and fortification learning is conducive to fath-
oming the over issues in complex multi-subject frameworks. The
fundamental calculations of multi-agent fortification learning incor-
porate angle climb (plunge) calculation, Q-Learning, and method-
ology climbing calculation, etc. Unused multi-agent fortification
learning calculations are moreover rising, mostly as ceaseless en-
hancements to the essential calculations.
167
Research on cooperation strategy between wind power and electric vehicle aggregators based on multi-agent and Block chain ICBCT’22, March 25–27, 2022, Shanghai, China
Figure 2: Multi-agent Actor-Critic framework of the MAD-
DPG algorithm
2.2 MADDPG
This paper centers on the MADDPG calculation for multi-agent
fortification learning, i.e., the multi-agent profound deterministic
arrangement angle calculation. The MADDPG calculation may be
a characteristic expansion of the DDPG calculation for multi-agent
frameworks, whose algorithmic system is centralized preparing
and decentralized execution. It is moved forward by presenting
activities inspected from the current arrangements of other special-
ists as additional data at the input side within the modeling handle
of the Q-value work to illuminate the non-stationary issue of the
environment for multi-agent frameworks. The MADDPG calcula-
tion chosen in this paper has two major focal points: to begin with,
amid the preparing stage, the performing artist of each specialist
makes choices based on nearby data (i.e., the agents’ activities and
states); moment, the calculation does not require data around how
the environment is changing or the strategy of communication
between the operators. In this manner, the calculation isn’t as it
were appropriate for agreeable situations, but moreover compet-
itive environments. The multi-agent Actor-Critic system plan of
the MADDPG calculation is appeared in Figure 2
We use θ = [θ1, . . . ,θn ] to denote the parameters of n
agents’ policies, π = [π1, . . . , πn ] represents n agents’ policies.
The cumulative expectation reward for the agent i is J (θi ) =
Es∼ρπ ,ai∼πθi [
∞∑
t=0
γ t ri,t ], given the random policy, a policy gradient
for:
∇θi J (θi ) = Es∼ρπ ,ai∼πi
[
∇θi loдπi (ai |oi )Q
π
i (x ,a1, . . . ,an )
]
(1)
where oi is the observation of the agent i , and x =
[o1, . . . ,on ]represents the observation vector; Qπ
i (x ,a1, . . . ,an )
represents the state-action function of the agent i in the centralized
form. Since each agent learns their ownQπ
i function independently
and has different reward functions, so it can complete cooperative
or competitive tasks.
The following extends to the deterministic policy µθi , and the
gradient formula is as:
∇θi J (µi ) = Ex,a∼D
[
∇θi µi (ai |oi ) ∇aiQ
µ
i (x ,a1, . . . ,an ) |ai=µi (oi )
]
(2)
where D is an empirical store consisting of
(x ,x ′,a1, . . . ,an , r1, . . . , rn ). The centralized update method
of Critic draws on the TD and target network ideas in DQN is as:
L (θi ) = Ex,a,r,x ′
[ (
Qu
i (x ,a1, . . . ,an ) − y
)2] (3)
y = ri + γQ̄
µ′
i
(
x ′,a′1, . . . ,a
′
n
)
|a′j=µ
′
j (oj )
(4)
where Q̄µ′
i represents the target network, µ ′ = [µ ′1, . . . , µ
′
n ] is the
target policy with the delayed update parameter θ ′j . The approaches
of other specialists can be gotten through fitting estimation without
communication interaction. As you’ll be able see over, Faultfinder
learns fromworldwide data, whereas On-screen character as it were
employments nearby perception information.
In Equation 3), the policies of other agents are used, which re-
quires constant communication to obtain, but the MADDPG algo-
rithm realizes it by estimating the policies of other agents. Each
agent maintains n − 1 policy approximating function µ̂
∅
j
i
, which
represents the function approximation of the agent i to the policy
µ j of the agent j. Its approximating cost is the logarithmic cost
function, with the entropy of the policy, its cost function can be as:
L
(
∅
j
i
)
= −Eoj ,aj
[
loдµ̂
∅
j
i
(
aj |oj
)
+ λH
(
µ̂
∅
j
i
)]
(5)
As long as the above cost function is minimized, the approximations
of other agents’ policies can be obtained. Therefore, Equation 4)
can be replaced by
y = ri + γQ̄
µ′
i
(
x ′, µ̂
′1
∅
j
i
(o1) , . . . , µ̂
′n
∅
j
i
(on )
)
(6)
Before updating Qµ
i , update µ̂∅ji
employing a test clump of en-
counter playback.
Other than, the environment is powerfully unsteady for a specific
specialist since each agent’s arrangement is upgraded and iterated.
In this way, in a competitive environment, it is common for an
operator to over-fit a solid approach against its competitors. In
any case, this solid approach is exceptionally delicate, since, with
the overhaul and alter of the competitors’ arrangements, the solid
arrangement is troublesome to adjust to the competitors’ modern
approaches.
To cope with the above situation better, MADDPG proposed an
idea of policy set, in which the policy µi of the agent i consists
of a set with K sub-policies and only one sub-policy µ
(k)
i is used
in each training episode. For each agent, we maximize the overall
reward Je (µi ) = Ek∼unif (1,K ),s∼ρ µ ,a∼µ (k )i
[
∞∑
t=0
γ t ri,t ] of its policy
set. And we construct a memory store D
(k)
i for each sub-policy
k . We optimize the overall effect of the policy set, so the update
gradient for each sub-policy is
∇θ (k )
i
Je (µi ) =
1
K
Ex,a∼D (k )
i
[
∇θ (k )
i
µ (k )i (ai |oi ) ∇aiQ
µi (x, a1, . . . , an ) |ai=µ
(k )
i (oi )
]
(7)
168
ICBCT’22, March 25–27, 2022, Shanghai, China Honglin Xue et al.
Figure 3: Schematic diagram of scenario hypothesis
3 MULTI-AGENT REINFORCEMENT
LEARNING
3.1 Scenario hypothesis
This paper accept that the investigate situation is appeared in Fig-
ure 3. The wind cultivate can select to offer their power within the
power advertise or supply it to the EV aggregator. The EV aggrega-
tor supply EVs with power obtaining from the wind cultivate and
the grid. The wind cultivate has to choose how much power to sup-
ply to the EV aggregator and how much to offer within the power
advertise to maximize the wage; the EV aggregator has to make
choices on charging benefit estimating for EVs, to direct clients to
charge in an deliberate way and maximize the wage. Based on the
comparative investigation of whether the wind cultivate and the EV
aggregator coordinate or not, this paper sets the taking after two
scenarios to construct the non-cooperative diversion demonstrate
and agreeable amusement demonstrate separately.
Scenario 1: The wind ranches and the EV aggregators don’t
participate, and the two parties enter into a contract whereby the
wind cultivate offers power to the EV aggregators at time-of-use
power price. Scenario 2: The wind cultivate participates with the EV
aggregator, with the wind cultivate providing power inside to the
EV aggregator and both parties sharing the pay of the cooperation.
3.2 Non-cooperative game model
3.2.1 Income calculation model. This paper does not take into ac-
count the cost of generating electricity for the WF, which either
sells electricity on the electricity market or sells it to the EV aggre-
gator at contractual pricing. Therefore, the non-cooperative income
model of the WF can be expressed as:
Rwind =
T∑
t=1
[
P tmQt
m + P
t
con
(
Qt
w −Qt
m
) ]
(8)
where Rwind is the non-cooperative income of the WF in period
T ; P tm is the real-time electricity price at time t ; Qt
m is the power
sold by the WF in the power market at time t ; Qt
w is the power
generation of the WF at time t ; (Qt
w −Qt
m ) is the electricity sold
by the WF to the EV aggregator; P tcon is the time-of-use price of
the bilateral contract, namely peak, flat and valley pricing.
Moreover, this paper as it were considers the power buy fetched
of the EV aggregator, whose power is obtained from theWF and the
lattice. The EV aggregator makes cash by charging clients for charg-
ing their EVs. Subsequently, the non-cooperative salary demon-
strate of the EV aggregator can be communicated as:
REV =
T∑
t=1
[
P tEVQ
t
EV − P tcon
(
Qt
w −Qt
m
)
− P tдQ
t
д
]
(9)
P tEV = P tд + P
t
s (10)
Qt
EV = Q
t
w −Qt
m +Q
t
д (11)
where REV is the non-cooperative income of the EV aggregator in
period T; P tд is the price of the EV aggregator purchasing electricity
from the grid at time t ; P ts is the service fee charged by the EV
aggregator to the customers at time t ; P tEV is the charging service
pricing of the EV aggregator at time t .This paper expect that it
comprises of two parts, specifically the control buy cost of the
network and benefit expense, as appeared in Condition (10).Qt
д is
the electricity purchased by the EV aggregator from the grid at
time t ; Qt
EV is the charging amount of EVs at time t , and its source
is shown in Equation 11).
3.2.2 Decision model. TheWF and the EV aggregators arrange and
diversion each other to concur on reciprocal contract estimating.
The choice of the WF to offer power depends on the comparison
of advertise cost and the contract cost, that is, if P tm > P tcon , the
WF tends to sell electricity on the electricity market, instead, if
P tm < P tcon , it would prefer to sell electricity to the EV aggregators.
Signing the bilateral contract with the EV aggregator hedges to
some extent the revenue risk to theWF from downward fluctuations
in market electricity prices.
In conclusion, the sum of power exchanged between the WF and
the EV aggregator depends on the choice of the WF to offer power,
which influences the power buy taken a toll of the EV aggregator.
The EV aggregator makes charging benefit estimating based on
power buy fetched and guides clients to charge in an efficient way,
in this way affecting the power sold by the WF within the power
showcase. Hence, the choice show of the WF and the EV aggregator
could be a two-layer choice show when they are not collaborating.
The upper one is the control deal choice demonstrate of the WF,
specifically:
maxRwind =
T∑
t=1
[
P tmQt
m + P
t
con
(
Qt
w −Qt
m
) ]
(12)
s .t .Qt
m ≤ Qt
w (13)
P tcon < P tд (14)
The lower one is the pricing decision-making model of the EV
aggregator for customers, namely:
maxREV =
T∑
t=1
[
P tEVQ
t
EV − P tcon
(
Qt
w −Qt
m
)
− P tдQ
t
д
]
(15)
s .t .Pmin
s ≤ P ts ≤ Pmax
s (16)
Qmin
EV ≤ Qt
EV ≤ Qmax
EV (17)
Over, the objective work of the WF and the EV aggregator’s non-
cooperative choice is to maximize their particular pay. Imperative
(13) is that the power sold by the WF within the control advertise
isn’t more noteworthy than the creating capacity of theWF at time t .
Imperative (14) is that the estimating of the contract between theWF
169
Research on cooperation strategy between wind power and electric vehicle aggregators based on multi-agent and Block chain ICBCT’22, March 25–27, 2022, Shanghai, China
and the EV aggregator at time t cannot be higher than the control
purchase price from the framework, which is the fundamental
preface of the contract marked by the EV aggregator. Limitation
(16) is to set the upper and lower cost limits for the benefit charge
estimating at time t based on cost supervision. Limitation (17) is
that considering the restricted number of charging heaps of the
EV aggregator, the upper and lower limits are set for the charging
stack of EVs at time t .
4 CONCLUSION
In order to study how to suppress the anti-peak regulating charac-
teristics of wind power by electric vehicles, the non-cooperative
game model and cooperative game model of WF and electric ve-
hicle aggregator are established respectively based on the game
equilibrium theory. In order to make up for the limitations of the
game theory method, the MADDPG algorithm of the multi-agent
reinforcement learning method is applied to solve the model.
ACKNOWLEDGMENTS
This project is supported by the 2021 Science and Technology Back-
bone Innovation Project No. 52051C21004T of State Grid Shanxi
Electric Power Company.
REFERENCES
[1] The Power System Faces the Challenge of High Proportion of New Energy Con-
nected to the Grid. Available online:http://chuneng.bjx.com.cn/news/20201111/
1115129.shtml(accessed on 9 November 2020).
[2] Xian, W.; Huajun, Z.; Shaohua, Z., Game modle of electricity market involving
virtual power plant composed of wind power and electric vehicles. Automation
of Electric Power Systems 2019, 43(3), 155-164.
[3] Liu, X.; Xu, W., Economic Load Dispatch Constrained byWind Power Availability:
A Here-and-Now Approach. IEEE Trans. Sustain. Energy 2010, 1, (1), 2-9.
[4] Vasirani, M.; Kota, R.; Cavalcante, R. L. G.; Ossowski, S.; Jennings, N. R., An
Agent-Based Approach to Virtual Power Plants of Wind Power Generators and
Electric Vehicles. Ieee Transactions on Smart Grid 2013, 4, (3), 1314-1322.
[5] Yousefi, A.; Iu, H. H.-C.; Fernando, T.; Trinh, H., An Approach for Wind Power
Integration Using Demand Side Resources. IEEE Trans. Sustain. Energy 2013, 4,
(4), 917-924.
[6] Paterakis, N. G.; Erdinc, O.; Bakirtzis, A. G.; Catalao, J. P. S., Load-Following
Reserves Procurement Considering Flexible Demand-Side Resources Under High
Wind Power Penetration. Ieee Transactions on Power Systems 2015, 30, (3),
1337-1350.
[7] Wang, Y.; Zhou, Z.; Botterud, A.; Zhang, K. F.; Ding, Q., Stochastic coordinated
operation of wind and battery energy storage system considering battery degra-
dation. J. Mod. Power Syst. Clean Energy 2016, 4, (4), 581-592.
[8] de la Nieta, A. A. S.; Contreras, J.; Catalao, J. P. S., Optimal Single Wind Hydro-
Pump Storage Bidding in Day-Ahead Markets Including Bilateral Contracts. IEEE
Trans. Sustain. Energy 2016, 7, (3), 1284-1294.
[9] Ferdowsi, M.; Ieee, Vehicle Fleet as a Distributed Energy Storage System for the
Power Grid. In 2009 Ieee Power & Energy Society General Meeting, Vols 1-8,
2009; pp 2074-2075.
[10] Jampeethong, P.; Khomfoi, S., Coordinated Control of Electric Vehicles and Re-
newable Energy Sources for Frequency Regulation in Microgrids. IEEE Access
2020, 8, 141967-141976.
[11] Druitt, J.; Frueh, W.-G., Simulation of demand management and grid balancing
with electric vehicles. J. Power Sources 2012, 216, 104-116.
[12] Lee, J.; Lee, J.; Wi, Y.-M.; Joo, S.-K., Stochastic Wind Curtailment Scheduling for
Mitigation of Short-Term Variations in a Power System with High Wind Power
and Electric Vehicle. Applied Sciences 2018, 8, (9).
[13] Zhu, Y. S.; Gao, H. Y.; Xiao, J. M.; Qu, B. Y.; Zhu, F. B.; Yang, L., Dynamic Multi-
Objective Dispatch Considering Wind Power and Electric Vehicles With Proba-
bilistic Characteristics. IEEE Access 2019, 7, 185634-185653.
[14] Kou, P.; Liang, D.; Gao, L.; Gao, F., Stochastic Coordination of Plug-In Electric
Vehicles and Wind Turbines in Microgrid: A Model Predictive Control Approach.
Ieee Transactions on Smart Grid 2016, 7, (3), 1537-1551.)
170
http://chuneng.bjx.com.cn/news/20201111/1115129.shtml(accessed
http://chuneng.bjx.com.cn/news/20201111/1115129.shtml(accessed
	Abstract
	1 INTRODUCTION
	2 MULTI-AGENT REINFORCEMENT LEARNING
	2.1 Theory
	2.2 MADDPG
	3 MULTI-AGENT REINFORCEMENT LEARNING
	3.1 Scenario hypothesis
	3.2 Non-cooperative game model
	4 CONCLUSION
	Acknowledgments
	References