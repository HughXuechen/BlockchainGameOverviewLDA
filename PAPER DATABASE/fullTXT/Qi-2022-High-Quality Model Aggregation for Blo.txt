High-Quality Model Aggregation for Blockchain-Based Federated Learning via Reputation-Motivated Task Participation
18378 IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 19, 1 OCTOBER 2022
High-Quality Model Aggregation for
Blockchain-Based Federated Learning via
Reputation-Motivated Task Participation
Jiahao Qi , Feilong Lin , Member, IEEE, Zhongyu Chen, Changbing Tang , Member, IEEE,
Riheng Jia , and Minglu Li , Senior Member, IEEE
Abstract—Federated learning is an emerging paradigm to con-
duct the machine learning collaboratively but avoid the leakage
of original data. Then, how to motivate the data owners to par-
ticipate federated learning and contribute high-quality data is
the crucial issue. In this article, a blockchain-based federated
learning (BFL) with a reputation mechanism for high-quality
model aggregation is proposed. Specifically, the blockchain trans-
forms the federated learning into a decentralized and trustworthy
manner. Over the blockchain, federated learning tasks, under-
taken by smart contracts, can be conducted transparently and
fairly. Besides, a reputation-constrained data contribution and
reward allocation mechanism is designed to encourage data own-
ers to participate in BFL and contribute high-quality data. The
noncooperative game is adopted to analyze the behavior strate-
gies of data owners. The existence of the unique equilibrium is
proved and the equilibrium point indicates that the data own-
ers can acquire highest reward with the contribution of the
highest quality data. Thus, the model quality of BFL is guar-
anteed. Finally, simulations on the public data sets (MNIST and
CIFAR10) demonstrate that BFL with a reputation mechanism
can well promote the high-quality model aggregation of federated
learning as well as can prevent malicious nodes from corrupting
the training task.
Index Terms—Blockchain, federated learning, high-quality
model, reputation mechanism, smart contract.
I. INTRODUCTION
MACHINE learning can be easily used to train the func-
tional model by collecting amount of data but without
logical reasoning or complex computing. It has been exten-
sively applied for, such as image understanding in traffic
control [1], route planning in autonomous driving [2], intel-
ligent diagnosis in medical treatment [3], speech information
Manuscript received 14 September 2021; revised 16 February 2022;
accepted 14 March 2022. Date of publication 18 March 2022; date of cur-
rent version 23 September 2022. This work was supported in part by the
NSF of China under Grant 61877055 and Grant 61902358, and in part by
the NSF of Zhejiang Province under Grant LY22F030006. (Corresponding
author: Feilong Lin.)
Jiahao Qi, Feilong Lin, Zhongyu Chen, Riheng Jia, and Minglu Li
are with the Key Laboratory of Intelligent Education Technology and
Application of Zhejiang Province and the College of Mathematics and
Computer Science, Zhejiang Normal University, Jinhua 321004, China
(e-mail: qjh2020@zjnu.edu.cn; bruce_lin@zjnu.edu.cn; czy@zjnu.edu.cn;
rihengjia@zjnu.edu.cn; mlli@zjnu.edu.cn).
Changbing Tang is with the College of Physics and Electronic Information
Engineering, Zhejiang Normal University, Jinhua 321004, China (e-mail:
tangcb@zjnu.edu.cn).
Digital Object Identifier 10.1109/JIOT.2022.3160425
recognization and reconstruction [4], and so on. However, in
many application scenarios, it is faced the security issue that
privacy contents, such as personal daily route, health state, and
even identity information, may be leaked, which will result in
more serious consequences.
Recently, to deal with this security issue from machine
learning, McMahan et al. [5], [6] from Google proposed a
new paradigm called federated learning. Federated learning
no longer trains the model using centrally stored data. It calls
for participants to train the model locally using self-owned
data and send the local models without raw data to the model
aggregation server. Finally, the model aggregation server can
obtain an accurate global model meanwhile keep the personal
data secure. Since then, federated learning has received great
attention from both academia and industrial circles.
Naturally, a critical problem faced by federated learning
is how to encourage the data owners to join in the tasks,
since that data owners have to contribute both data and local
model training overhead in this new paradigm. An incentive
mechanism is needed to motivate data owners to take part
in federated learning tasks while pursuing the high quality
of the federated learning model. The common approach is to
reward each participant according to their respective contri-
bution. A lot of work on incentive mechanisms have been
done [7], such as individual profit sharing [8], Shapley game
profit sharing [9], and fair-value game [10]. Nevertheless,
how to design incentives for the appropriate federated learn-
ing scenario is still to be investigated. Furthermore, in such
a distributed federated learning, the task may be attacked
by malicious participants by such as giving weak or wrong
local model or overstating their contribution. To this point,
the appropriate model quality evaluation mechanism [11] and
task participant management rules are necessary. Commonly,
the reputation-based management rules [12], [13] together
with reward mechanism can be leveraged to encourage task
participants with honest behavior.
Although federated learning prevents the raw data leakage,
the central aggregation server also faces the risk of single-point
failure. Fortunately, the newly emerging blockchain technol-
ogy has the potential to address this issue. Blockchain is
essentially a collectively maintained data ledger by the peer-
to-peer network. It built a new decentralized trust without a
third-party credit endorsement [14]. In particular, with con-
sensus protocol, the data ledger over blockchain network
2327-4662 c© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
https://orcid.org/0000-0002-6016-3026
https://orcid.org/0000-0003-0981-3721
https://orcid.org/0000-0002-1641-2611
https://orcid.org/0000-0002-5741-5560
https://orcid.org/0000-0001-6547-3472
QI et al.: HIGH-QUALITY MODEL AGGREGATION FOR BLOCKCHAIN-BASED FEDERATED LEARNING 18379
exhibits the features in terms of tamper-resistant, nonre-
pudiable, and publicly verifiable [15]. The smart contract
technology [16] helps convert the traditional businesses to a
blockchain network environment and conduct the businesses
with a transparent, traceable, and unforgeable manner. For
example, Ferrag et al. [17], [18], and Xia [19] summarized
in detail the application of blockchain in the context of IoT
and considered very important factors, such as security issues,
efficiency issues, or in the choice of consensus algorithms in
the application process. To sum up, blockchain can be used to
solve the centralized services with high security risk or high
service charges [20].
In this article, with the purpose of increasing the model
quality of federated learning, the integration design of
blockchain technology and federated learning is considered.
Together with smart contract technology, blockchain can build
a trustworthy environment and make the federated learning
tasks be executed transparently and fairly. Specifically, the
local models contributed by data owners can be publicly evalu-
ated. The global model aggregation can be decentrally fulfilled
with properly designed blockchain consensus mechanism, thus
to make the global model aggregation safe. Moreover, the
reputation mechanism can be constructed in blockchain envi-
ronment. Based on the evaluation of the contributed local
model, one will be assigned with a certain reputation degree.
Then, data owners can be rewarded with the consideration
of their contributed data volume and reputation degree. With
the above ideas, the data owners are supposed to have fine
motivation to join the federated learning and contribute their
high-quality data to learn the local model. The innovation and
contribution of this article are summarized as follows.
1) A blockchain-based federated learning (BFL) mecha-
nism is proposed. It is supposed to solve the single-
point-failure security problem of the model aggrega-
tion center in traditional federated learning systems.
Systematic smart contracts are designed to conduct fed-
erated learning over blockchain network. BFL is capable
to improve the credibility and reliability of federated
learning.
2) A reputation mechanism is designed based on the
model quality and contribution evaluation, which also
builds the foundation to coordinate the effective data
contribution for local model, weighted global model
aggregation, and reward allocation. The above measures
can effectively encourage the data owners to join the
BFL. Additionally, an optional grouping mechanism is
proposed to cope with the high complexity brought by
a large number of participants.
3) The decentralized execution of federated learning task is
formulated by the noncooperative game, where the strat-
egy to maximize individual profit is adopted. The unique
existence of Nash equilibrium is proved. The equilib-
rium point shows that the data owner’s profit reaches
the maximum under the condition of contributing the
highest quality data, thereby ensuring the high quality
of the federated learning model.
4) Finally, extensive simulations have been conducted
based on the public data sets (MNIST and CIFAR10).
The results show that BFL can well promote the high-
quality model aggregation of federated learning as well
as can prevent malicious nodes from corrupting the
training task.
The remainder of this article is organized as follows.
Section II introduces the related work. The BFL system
will be described in Section III. Section IV introduces the
model quality-based reputation evaluation mechanism. Then,
the reputation-based reward allocation and model aggregation
algorithm is presented in Section V. The system implementa-
tion and simulation results are shown in Section VI. Finally,
Section VII concludes this article.
II. RELATED WORK
Since federated learning was first proposed and applied to
the automatic input completion system by Google [5], [6], it
has attracted much attention from both academic and industrial
areas [21]. Recently, federated learning is constantly exploring
applications in different fields. For example, in healthcare area,
Elayan et al. [22] proposed a deep federated learning frame-
work for sustainable healthcare data monitoring and analysis.
Lim et al. [23] utilized federated learning to enable privacy-
preserving collaborative model training with distributed IoT
network. In terms of transportation, Li et al. [24] intro-
duced the federated learning into autonomous driving to
preserve vehicular privacy by keeping original data locally.
Lu et al. [25] proposed a federated learning architecture to
relieve transmission load and address privacy concerns of
providers. In industrial IoT, Lu et al. [26] used the privacy-
preserved federated learning architecture for secure industrial
data modeling and sharing. Fu et al. [27] proposed a verifi-
able federated learning with privacy preserving for industrial
big data processing, where the correctness of the aggregated
gradients is verified by the Lagrange interpolation method.
In addition, Su et al. [28] introduced an edge-cloud-assisted
federated learning framework for communication-efficient and
privacy-preserving energy data sharing and Song et al. [29]
enhanced the users’ identity privacy of federated learning in
mobile-edge computing.
In federated learning networks, how to motivate users to par-
ticipate in task and contribute the high-quality data is one of
the most important research topic. Some researchers have paid
efforts to address this issue. Fan et al. [30] utilized the reverse
auction mechanism to motivate the users and introduced the
Earth mover’s distance model to measure the data with inde-
pendently identical distribution (IID). Zeng et al. [31] designed
a lightweight incentive mechanism with multidimensional pro-
curement auction that encouraged edge nodes participating
in the mobile-edge computing. Dong and Zhang [32] intro-
duced the game theory to design the strategy that motivated
nodes to participate in the model training task. Based on the
idea of a noncooperative game, Zhan et al. [33] set up a
total budget to reward the clients with their respective con-
tributions. Wu et al. [34] and Ding et al. [35] designed
incentive mechanisms for federated learning with the reason-
able consideration of various factors, such as task expenditure,
communication delay, and users’ privacy issue. Sun et al. [36]
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
18380 IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 19, 1 OCTOBER 2022
further proposed a contract-based incentive mechanism that
provided compensation for privacy leakage.
In addition to keeping clients motivated, federated learn-
ing calls for the distributed and secure method to coordinate
cooperation among the users. Blockchain has the potential to
well manage the federated learning. Kim et al. [37] applied
blockchain to coordinate the training process of federated
learning and completed the decentralization of the training pro-
cess. Kang et al. [12], [38] utilized smart contract to complete
the reputation management of clients, and record the reputation
evaluation through the blockchain ledger, thus to realize the
tamper proof of the reputation. Qu et al. [39] used blockchain
to fairly select the nodes participating in specified federated
learning tasks in Industry 4.0.
These efforts have focused on how to address the
performance, incentive, and management issues of federated
learning. However, few works have systematically consid-
ered the quality of contributed data, model aggregation, and
reward distribution in federated learning tasks. Based on the
existing research, this article works toward a high-quality fed-
erated learning model by combining data quality with model
aggregation and reward distribution.
III. BLOCKCHAIN-BASED FEDERATED LEARNING
In this section, the federated learning is preliminarily intro-
duced. Then, the illustration of the proposed BFL with
reputation mechanism is presented. Smart contracts to con-
duct federated learning with high-quality model aggregation
are finally provided.
A. Preliminaries on Federated Learning
Model aggregation server initializes a global model once a
new task is published, which announces the beginning of fed-
erated learning. Clients fetch the latest global model from the
server and update the model with their own data to produce
corresponding local models. The model aggregation server col-
lects these local models and aggregates them into the updated
global model. Such a process is considered as a single round.
Generally, the federated learning will be carried out with
multiple rounds.
Suppose there are N clients denoted by N = {1, 2, . . . , N}.
Each client n has a data set Dn of amount sn, Dn = {di|i =
1, 2, . . . , sn}. The single data is represented as di = (xi, yi),
where xi denotes the feature of the data and yi denotes the label
of the data. The model parameters to be trained are denoted
by w. For convenience, w will be used later to refer to the
model as well. Therefore, the local loss function of client n is
Fn(w) = 1
sn
sn∑
i=1
fi(w) (1)
where fi(w) = (xT
i w− yi
)2
. fi(w) reflects the distance between
the output of the model and the actual result.
For the global model, its global loss function can be
formulated as
F(w) =
N∑
n=1
sn
S
Fn(w) = 1
S
N∑
n=1
sn∑
i=1
fi(w) (2)
where S = ∑
n∈N sn. The whole task of federated learning
aims at minimizing the global loss function, i.e.,
min
ω
F(w). (3)
In this article, a gradient descent algorithm will be taken to
find the best w. The model waiting to be updated in the tth
round is denoted as wt. In the tth round, client n computes its
gradient gn = ∇Fn(wt) with the local data set and updates the
new local model wt
n under the learning rate η (a larger learning
rate means a faster rate of change of the loss function)
wt+1
n ← wt − ηgn. (4)
Client node n uploads wt+1
n to the model aggregation server
wt+1 ←
N∑
n=1
sn
S
wt+1
n . (5)
In practical applications, a threshold δ can be set to indicate
that the training results meet expectations. When f (wt) ≤ δ,
the model has reached a relatively good performance level and
the task is finished.
B. Network Model of BFL
In the proposed BFL, federated learning chooses the con-
sortium chain as its blockchain foundation. The consortium
chain contains supervisable participants and predefined ledger
nodes, enabling faster consensus and more secure operations
among nodes. This satisfies the requirements of BFL for node
security and task efficiency. The framework of BFL is illus-
trated by Fig. 1. A description of the members in BFL and
their networks is given in the following.
1) Data Owner: Such node has data that can be used for
model training but cannot be shared externally. The data
owner trains the local model with private data and con-
tributes data for a fee by sharing the local model. It is
served by client node n in federated learning, n ∈ N .
2) Model Aggregator: In BFL, a part of data is not used for
training but for testing. The client nodes holding the test
data can act as model aggregators. Suppose that there
are M model aggregators, who form the model aggre-
gation organization denoted by M = {1, 2, . . . , M}.
Members of the model aggregation organization collabo-
rate with each other and are jointly responsible for model
aggregation.
3) Network Establishment: In the BFL network, data own-
ers do not communicate directly, but obtain information
through synchronous blockchain. A star topology is
formed between the model aggregator and the surround-
ing data owners within the communication range. Model
aggregators communicate through broadcasting.
The BFL network can be logically divided into four layers:
1) the blockchain layer; 2) the model layer; 3) the reputation
layer; and 4) the reward layer, as follows.
1) Blockchain Layer: This layer is the basis of the BFL
network. A model aggregator collects the local models
of the data owners bound to it via peer-to-peer transfer.
In order to get all local models, the model aggrega-
tors will share their collected models with each other.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
QI et al.: HIGH-QUALITY MODEL AGGREGATION FOR BLOCKCHAIN-BASED FEDERATED LEARNING 18381
Fig. 1. Illustration of BFL with reputation mechanism.
Blockchain serves as a bridge to share global models
and reputation. Data owners get the latest global model
and their reputations from the blockchain ledger. After
aggregating the new global model, the model aggrega-
tor publishes the model and the reputation evaluation of
data owners to the blockchain.
2) Reputation Layer: The reputation layer completes the
evaluation of the data owners’ reputation. In the repu-
tation layer, one data owner first shares its local model
to all model aggregation nodes through the blockchain
layer. Each model aggregation node performs model
quality testing of the local model. By exchanging the test
results, the model aggregation organization will collab-
oratively complete a reputation evaluation of this data
owner and finally publish the reputation evaluation to
blockchain.
3) Model Layer: The training part of the federal learning
task is performed in the model training layer. Based
on the blockchain layer and the reputation layer, the
model training layer implements a decentralized model
aggregation task combined with reputation. In this layer,
after getting the global model, data owners update the
model with their local data to get new local models.
Then, data owners share their own local models across
the blockchain layer. The model aggregation organiza-
tion aggregates these collected local models with the
reputation evaluations into a new global model.
4) Reward Layer: In the reward layer, the system will
reward data owners with their contributions combin-
ing reputations. The existence of this layer encourages
the data owner to actively participate in the federated
learning task and contribute high-quality data.
C. Smart Contracts for Federated Learning
To enhance the trustworthiness and reliability of BFL, the
federated learning tasks are undertaken by smart contracts.
During task execution, nodes can complete the task by invok-
ing the corresponding smart contract. The smart contracts for
BFL include task initialization contract, member selection con-
tract, federated learning contract, reputation contract, reward
allocation contract, and query contract, which are presented in
detail in the following.
1) Task Initialization Contract: This contract is invoked
through the model aggregator when one user wants to issue
a federated learning task to BFL. The contract will assign a
unique task ID to this task and initialize the data type, model
structure, and payoff budget for this training task. The contract
writes the above information to the BFL and automatically
invokes the member selection contract after task initialization.
2) Member Selection Contract: Member selection contract
is used to determine the data owners to participate in federated
training according to their data amount and reputation values.
The reputation values of the data owners and the amount of
data contributions allowed are updated by invoking the model
processing contract. The contract provides the following two
interfaces.
1) MemberInitialization(): This interface can be called by
the task initialization contract and will broadcast the
information of this task to all data owners. Data owners
willing to participate will reply to the contract with the
amount of data they contributed in the first round. The
contract will receive responses within time Tmsc, then
verify the legitimacy of participating nodes, and submit
the reputation, data amount and public key of each node
to the BFL network. At this point the reputation of all
participating nodes is initialized to 1.
2) MemberUpdating(): The interface reads the latest
information of a node from the block, including the
node’s reputation value and the maximum amount of
data permitted for model training in its next round. The
permitted data amount is constrained by the reputation
value, which will be defined in Section IV. If the node’s
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
18382 IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 19, 1 OCTOBER 2022
maximum amount of data is less than a threshold value
smin, the node will be removed from the candidate list.
3) Federated Learning Contract: Before training the local
model, data owners check the latest member list to confirm
their eligibility and to obtain their reputation and the maximum
amount of data allowed to contribute. The federated learning
contract is divided into the following interfaces: local model
uploading and downloading, model testing, model aggregation,
and global model uploading and downloading.
1) LocalModelUploading(): When a data owner trains a
local model, it can upload the model parameters and
data amount to the bound model aggregator by calling
this interface.
2) LocalModelDownloading(): During a local model train-
ing cycle Tlocal, the model aggregators call this interface
to collect the local models. At the end of Tlocal, the local
models collected by each other will be shared within the
model aggregation organization.
3) TestResultUploading(): When the model aggregation
organization collects all the uploaded information or sat-
isfies the collection cycle, the aggregators collaborate
with each other to perform model quality testing for
data owners. When the test is finished, this interface will
be called to upload the test results and the reputation
contract is invoked based on the testing results.
4) ModelAggregation(): Model aggregators can aggregate
local models into global models by calling this interface.
The model aggregation organization internally decides
an aggregator for the model aggregation in this round
with a round-robin manner.
5) GlobalModelUploading(): The model aggregator
responsible for model aggregation can pack the global
model, all local models, and data contributions into
a new block by calling this interface after the model
aggregation.
6) GlobalModelDownloading(): Data owners waiting to
start the next round of training can download the latest
global model by calling this interface.
The contract will check whether the data owner is in the
member list. If not, the contract will reject the calling from
the data owner.
4) Reputation Contract: The contract is used to update the
reputation of data owners. After completing the model testing
by federated learning contract, the model aggregator can call
the reputation update interface in this contract to complete the
reputation update of each data owner.
1) ReputationUpdating(): The interface takes the model
quality testing results as input and combines the his-
torical reputation to calculate the latest reputation value
for each data owner. The contract utilizes the new rep-
utation value to calculate the maximum amount of data
allowed to contribute in the next round.
5) Reward Allocation Contract: Data owners will be paid
for their participation in the current round by invoking the
reward allocation interface.
1) RewardAllocation(): The reward allocation interface will
call the reputation query interface and the historical
data contribution query interface in the query contract
to get the reputation value and data amount, and use
this information to calculate the reward for each node.
The reward determination algorithm will be presented in
Section V.
6) Query Contract: The block of BFL records the histor-
ical data contribution and reputation of each data owner and
also maintains the member list for each round. The query
contract provides the corresponding query interface for this
information.
1) HistoricalDataSizeQuery(): This interface provides a
service for querying the amount of historical data con-
tribution.
2) HistoricalReputationQuery(): This interface provides a
service for querying historical reputation.
3) MemberListQuery(): This interface provides a service
for querying the member list.
With the above contracts, the data owners and the model
aggregators can interact reliably with each other over BFL
network. Now, the BFL process can be described based on its
contracts as follows.
1) Start Task: When BFL receives a federated learning
task, it initiates the task by calling the task initialization
contract.
2) Select Member: When the task starts, BFL automati-
cally calls memberInitialization() to initialize the mem-
bers participating in the task. Before the start of each
round, the system updates the member list by calling
memberUpdating().
3) Train Local Model: Data owners call memberList-
Query() to confirm their legal identity and get the latest
global model by calling globalModelDownloading() and
then use their data to train the model locally.
4) Upload Local Model: After training is completed, the
node calls localModelUploading() to upload the local
models and corresponding contributions.
5) Evaluate Reputation: The model aggregation organiza-
tion tests the uploaded local models by calling testResul-
tUploading() and then updates the reputation by calling
reputationUpdating().
6) Aggregate Global Models: The model aggregation orga-
nization calls modelAggregation() to aggregate the mod-
els and publishes them to BFL through globalModelU-
ploading().
7) Allocate Reward: Data owners get their respective
rewards by calling rewardAllocation().
Steps 2)–7) will be performed repeatedly until the federated
learning is as effective as expected.
IV. MODEL QUALITY-BASED REPUTATION EVALUATION
In this section, the reputation evaluation based on model
testing for BFL is introduced. First, a local reputation evalua-
tion based on model quality is presented. Then, a global eval-
uation depended on local reputation evaluations is provided.
A. Local Reputation Evaluation Based on Model Quality
After acquiring all the local models, the model aggregator
uses the owned data set to test the local models. The fair-value
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
QI et al.: HIGH-QUALITY MODEL AGGREGATION FOR BLOCKCHAIN-BASED FEDERATED LEARNING 18383
game [10], a loss-based marginal approach, is applied to test
the quality of local models. It measures the impact of the local
model on the global model aggregation. Let At
m,n denote the
evaluation result of model aggregator m on data owner n, i.e.,
At
m,n = G
(
wt)− G
(
wt−n
)
(6)
where wt denotes the global model aggregated by using the
local models from all nodes in the set N , while wt−n denotes
the global model aggregated by the set without the local
model of node n. G(·) is the function for model accuracy
measurement.
In the following, the local reputation evaluation mechanism
based on these testing results is constructed. At the beginning,
the system initializes the global reputation value λ = 1 for
each data owner. At tth round, the local reputation value λt
m,n
of node n is updated according to the testing result At
m,n and
the historical reputation value, which can be formulated by
λt
m,n =
⎧
⎪⎪⎨
⎪⎪⎩
λH + ρ1e
At
m,n−σ1
θ1 , At
m,n > σ1 > 0
λH − ρ2e
σ2−At
m,n
θ2 , At
m,n ≤ σ2 ≤ 0
λH, others.
(7)
Due to that a lot of parameters are introduced by (7), in the fol-
lowing, we present the parameter interpretation and particular
function of this reputation evaluation.
1) λH is the historical reputation value. Considering that
the reference value of reputation decreases with time, the
time decay function e−α(t−ι) is used here to weight the
historical reputation, i.e.,
λH =
∑ι=t−1
ι=1 e−α(t−ι)λι
n∑ι=t
ι=1 e−α(t−ι)
. (8)
2) e(·) in (7) is applied to transform activation levels to
updating strength. Based on its curve feature, a higher
activation level means a larger updating strength and
curve slope. The variational curve slope makes the rep-
utation updating more flexible in response to different
levels of model quality. ρ1 and ρ2 are the reputation
update value of unit updating strength.
3) The thresholds σ1 and σ2 are introduced for activating
reputation update. At
m,n − σ1 and σ2 − At
m,n reflect the
activation levels. θ1 and θ2 denote the counting parame-
ters for scaling the activation levels to limit the value to a
small interval. The combination of {σ1, σ2} and {θ1, θ2}
is called tolerance of this reputation mechanism.
Based on the historical reputation, combined with the rep-
utation update value of unit strength, this mechanism updates
the reputation by using the update strength converted from
the model quality. A node trains a local model and shares it.
The model quality testing mechanism gives the corresponding
test result. When the model quality reaches the threshold for
activation, the corresponding reputation update is activated.
For instance, if the node contributes a bad model, the repu-
tation penalty update is activated. The worse the quality of
the model, the higher the strength of the reputation update,
and this strength changes exponentially as the quality of the
model gets worse. This property causes the reputation of the
node with very bad model to quickly become 0.
B. Global Reputation Determination
Since each model aggregator has different test data, BFL
gives a comprehensive reputation evaluation of the data own-
ers by multiple model aggregators. When model aggregator m
performs the tth round task of reputation evaluation, it also
adopts the opinions of other model aggregators. Model aggre-
gator m is required to judge the reference values of these
recommendations before adopting them. ηt
m′,n(m) indicates the
reference value provided by model aggregator m′ to model
aggregator m in the tth regarding the reputation evaluation of
data owner n, i.e.,
ηt
m′,n(m) = 1−
∣∣∣∣∣λ
t
m′,n −
∑
i∈M λt
i,n
M
∣∣∣∣∣, ηt
m′,n(m) > 0 (9)
where |λt
m′,n − [(
∑
i∈M λt
i,n)/M]| represents the distance
between the reputation evaluation from model aggregator m′
and the average reputation evaluation from the model aggrega-
tion organization M. The closer the value of ηt
m′m(n)
is to 1, the
higher the reference value of node m′ is. When ηt
m′,n(m) ≤ 0,
the reputation evaluation provided by m′ does not have any
reference value.
Model aggregator m collects the local reputation evaluations
of all model aggregators for the data owner n, thus generating
an indirect reputation evaluation of node m for node n by
λt
M,n(m) =
∑
i∈M ηt
i,n(m)λt
i,n∑
i∈M ηt
i,n(m)
. (10)
Finally, model aggregator m can obtain the tth global
reputation evaluation of node n by
λ̃t
m,n = μλt
m,n + (1− μ)λt
M,n(m) (11)
where μ is used to balance the two reputation evaluations.
If the model aggregator m pays more attention to its own
reputation evaluation, then the value of μ can be increased
appropriately.
The model aggregation organization takes turns to select
one of the model aggregators to carry out this round of rep-
utation updates, e.g., the node m is selected at the tth round
to update λ̃m,n. The updated reputation values are recorded
in the blockchain through the model aggregation organization.
After that, the publicly accepted new reputation evaluation of
node n is
λt
n = λ̃t
m,n. (12)
For ease of expression, the superscript t will be omitted in the
analysis of the next section.
C. Grouping Mechanism for Efficient Reputation Evaluation
Note that the reputation evaluation mechanism will confront
high complexity and low efficiency when the number of par-
ticipants is large. To deal with it, a grouping mechanism is
designed. The data owners and model aggregators can be sep-
arated into a certain groups according to specified metric, e.g.,
hops from data owner to model aggregator in the communica-
tion topology. Then, the local model quality testing is carried
out within the group. The reputation is also first evaluated
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
18384 IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 19, 1 OCTOBER 2022
within the group, then obtain a whole network evaluation by
model aggregation organization. When the grouping mecha-
nism is used, model aggregators can perform model quality
testing in groups in parallel, which theoretically can greatly
improve efficiency. Note that the grouping mechanism rela-
tively narrows the data samples for learning within the group.
Hence, the grouping mechanism is optional. Users can choose
to turn on the grouping mechanism if the number of nodes
is relatively large and sufficient to compensate for the lack of
samples caused by grouping.
V. REPUTATION-BASED REWARD ALLOCATION AND
MODEL AGGREGATION ALGORITHM
In this section, a reward allocation algorithm based on the
reputation mechanism is first introduced. Then, the optimal
data contribution is derived based on the reward allocation.
Finally, a reputation weighted model aggregation algorithm is
presented to improve the security and robustness of BFL.
A. Reward Allocation
The most intuitive way to allocate rewards is by the weight
of the data amount sn of each node to the total data amount.
However, since data contributions are reported by data own-
ers, dishonest owners have sufficient reasons to inflate their
contributions. Therefore, a reward allocation algorithm based
on reputation-weighted contribution is designed. If malicious
node n exaggerates its contribution, then BFL will give it a
relatively poor reputation evaluation based on the reputation
mechanism introduced in Section IV. The poor reputation eval-
uation, as the weight for reward allocation, will decrease the
reward for n. Based on this, the utility function of data owner
n is expressed as
u(sn) = λnsn∑N
i=1 λisi
R− Cnsn (13)
where R is the budget of the learning task and Cn denotes the
unit cost of node n in training the model. The unit cost usually
includes computational loss Ccom
n , communication loss Ccmp
n ,
and storage loss Cstg
n , expressed as follows: Cn = Ccom
n +
Ccmp
n + Cstg
n .
In particular, in order to continuously reduce the partici-
pation of malicious nodes, such that the amount of data that
node n can provide in each round must not be more than
the reputation-weighted contribution in the previous round.
To facilitate the derivation, the discrete data contribution is
approximated as continuous values, expressed as
sn ∈
[
0, λt−1
n st−1
n
]
. (14)
Furthermore, a threshold smin of the minimum data contribu-
tion is set. When sn is less than smin, the node is not allowed
to participate in the task.
B. Optimal Data Contribution
Since all data owners jointly participate in the allocating the
budget, the competition for reward can be viewed as noncoop-
erative games and each node peruses more reward. Therefore,
for node n, its objective function for solving the optimal
amount of contribution in the tth round can be expressed as
follows:
max
sn
u(sn) = λnsn∑N
i=1 λisi
R− Cnsn (15a)
s.t. sn ≤ λt−1
n st−1
n (15b)
sn > 0 (15c)
λnsn∑N
i=1 λiNi
R− Cnsn ≥ 0 (15d)
where (15b) and (15c) denote the limits on the data contri-
bution reported, and (15d) means that nodes are individually
rational (IR) in order to ensure their nonnegative returns.
Before going further analysis, the Nash equilibrium of the
game is first introduced.
Definition 1 (Nash Equilibrium): For the data contribu-
tions {s1, s2, . . . , sN} provided by all nodes N , if the benefit
obtained by any node n(n ∈ N ) after selecting the data amount
sn is not less than that when it selects any other data amount
s′n, i.e.,
u(sn) ≥ u
(
s′n
) ∀n ∈ N . (16)
At this time, the system reaches the Nash equilibrium. Under
this equilibrium condition, all nodes are stable in a strategic
choice.
Based on the definition of the optimization problem above,
the following theorems and corollaries can be derived.
Theorem 1: Subject to constraints (15b)–(15d), there exists
a unique Nash equilibrium for the noncooperative game con-
sisting of data owners N . The equilibrium point indicates that
node n’s optimal data contribution s∗n is independent of others
and is related to the unit resource consumption {Ci|i ∈ N } of
each node, i.e.,
s∗n =
(N − 1)R
λn
∑N
i=1 Ci
(
1− (N − 1)Cn∑N
i=1 Ci
)
. (17)
Proof: To obtain the first-order and second-order deriva-
tives of sn for the utility function u(sn), respectively
∂u(sn)
∂sn
= −Rλnsn
(∑N
i=1 λisi
)2
+ R
∑N
i=1 λisi
− Cn (18)
and
∂2u(sn)
∂sn
2
= − 2R
∑
i �=n λisi
(∑N
i=1 λisi
)3
< 0. (19)
From (18) and (19), the utility function u(sn) is a convex
function with respect to sn. According to the conditions for
the existence of Nash equilibrium, it is proved that there is
a Nash equilibrium of the game. Because the utility function
is convex and its constraint also satisfies convexity, the KKT
condition can be used to solve for the optimal amount of data
contribution.
Reform (15a)–(15d) into the standard form of the convex
problem
min
sn
U(sn) = −u(sn) = − λnsn∑N
i=1 λnsi
R+ Cnsn (20a)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
QI et al.: HIGH-QUALITY MODEL AGGREGATION FOR BLOCKCHAIN-BASED FEDERATED LEARNING 18385
s.t. sn − λt−1
n st−1
n ≤ 0 (20b)
−sn ≤ 0 (20c)
− λnsn∑N
i=1 λiNi
R+ Cnsn ≤ 0. (20d)
Let α, β, and γ be the Lagrange multipliers. Then, the
Lagrange function can be formulated by
L(sn, α, β, γ ) = U(sn)+ α
(
sn − λt−1
n st−1
n
)
+ β(−sn)
+ γ
(
− λnsn∑N
i=1 λiNi
R+ Cnsn
)
. (21)
In order to satisfy the KKT condition, the following constraints
need to be satisfied:
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
∂L(sn,α,β,γ )
∂sn
= 0
α
(
sn − λt−1
n st−1
n
) = 0
β(−sn) = 0
γ
(
− λnsn∑N
i=1 λnNi
R+ Cnsn
)
= 0
α ≥ 0, β ≥ 0, γ ≥ 0.
(22)
Solve (21) under the constraint of (22), and get the optimal
solution that relies on knowing the data contributions of other
data owners, i.e.,
s∗n =
√
R
∑
i �=n λisi
Cn
−∑i �=n λisi
λn
. (23)
Inspired by [33], for all i ∈ N , the following equation can
be got from (23):
N∑
i=1
λisi =
√
R
∑
i �=n λisi
Cn
. (24)
Let X = ∑N
i=1 λisi, then the above equation can be trans-
formed as
X2 = R(X − λnsn)
Cn
. (25)
Further transformations lead to
CnX2 = R(X − λnsn), n ∈ N . (26)
Sum both sides of (26) for n = 1, 2, . . . , N, that is
N∑
i=1
CiX
2 =
N∑
i=1
R(X − λisi)
=
N∑
i=1
RX − R
N∑
i=1
λisi
= RXN− RX. (27)
Therefore
X2
∑
i∈N
Ci = RX(N − 1) (28)
and hence, (28) can be converted to
X = R(N − 1)∑
i∈N Ci
. (29)
Substituting (29) into (25), s∗n by (17) can be solved.
Note that the quality of the data in local model training
greatly affects the quality of the overall federated learning
model. Hence, it is vital to urge nodes to contribute high-
quality data.
Theorem 2: Within a round of federated learning task, for
the high-quality data and low-quality data denoted by sh
n and
sl
n, u(sh
n) > u(sl
n) holds.
Proof: In the tth round, the reputation evaluation λh
n
obtained by node n after contributing high-quality data is
greater than the reputation evaluation λl
n after contributing
low-quality data, i.e., λh
n > λl
n. From the optimal contribu-
tion (17), it follows that:
λh
nsh
n =
(N − 1)R
∑N
i=1 Ci
(
1− (N − 1)Cn∑N
i=1 Ci
)
λl
nsl
n =
(N − 1)R
∑N
i=1 Ci
(
1− (N − 1)Cn∑N
i=1 Ci
)
.
It can be seen that the right-hand side of the equation is a
constant. Therefore, derived from λh
nsh
n = λl
nsl
n, sh
n < sl
n and
Cnsh
n < Cnsl
n hold. Substituting them into the utility function
u, respectively. u(sh
n) and u(sl
n) can be derived as
u
(
sh
n
)
= λh
nsh
n∑N
i=1 λisi
R− Cnsh
n
u
(
sl
n
)
= λl
nsl
n∑N
i=1 λisi
R− Cnsl
n
and hence, u(sh
n) > u(sl
n).
If there are malicious nodes among the data owners, such
nodes will maliciously exaggerate their contributions in order
to get more rewards. This algorithm gives BFL the ability to
counter with malicious nodes.
Corollary 1: Suppose that data owner n is a malicious node,
which will contribute data of bad quality. n will be banned
from participating in tasks in a given round. The worse its
quality is, the faster it is banned.
Proof: If the malicious node chooses to continue destroy-
ing the task, its reputation will continue declining based on the
reputation mechanism, i.e., 1 > λ1
n > λ2
n > · · · > λt−1
n > λn.
If the amount of contribution reported in the first round is s1
n
and its reputation is λ1
n < 1 according to the data contribution
constraints by (15b)–(15d), the amount of data it can report in
the next round must satisfy s2
n ≤ λ1
ns1
n. Hence, there must exist
t ∈ N+, and at the tth round, sn ≤ λt−1
n st−1
n ≤ ∏t−1
i=1 λi
ns1
n <
smin. In addition, the greater the node malice, the greater the
penalty given by the reputation mechanism, the fewer rounds
it takes to reach sn < smin. When the amount of reported
data is less than the threshold smin, the malicious node will be
disqualified and leave the task.
C. Reputation Weighted Model Aggregation
To address the problem that the traditional federated aver-
age algorithm (FedAvg) is strongly influenced by the quality
of local models and the authenticity of the contributed data
volume when aggregating local models, reputation evaluation
will be introduced here to assist in aggregation. Therefore, the
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
18386 IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 19, 1 OCTOBER 2022
optimized model aggregation method is as follows:
w←
N∑
i=1
λisi∑N
j=1 λjsj
wi. (30)
Corollary 2: Suppose that data owner n is a malicious node,
it has a limited and decreasing corrupting effect on the global
model.
Proof: Since model aggregation employs reputation to
weight the reported contributions, the impact of n’s malicious
behavior on model aggregation is reduced because of λn < 1
and sn < λnsn. From Corollary 1’s proof, there must exist
t ∈ N+, and at tth round, sn ≤ λt−1
n st−1
n ≤ ∏t−1
i=1 λi
ns1
n < smin.
That is, as the training continues, λnsn has a decreasing share
in aggregating the model until it exits the task.
D. Adaptability Analysis With Non-IID Data Set
In this part, the adaptability of BFL to the non-IID data
set is analyzed, since the owner’s data is often non-IID in the
realistic scenario. The analysis is presented as follows.
1) The model aggregation organization consisting of all
model aggregators has a complete test sample set, which
ensures that there is no mistake due to insufficient
samples when facing the non-IID data set.
2) A final reputation evaluation result is obtained by com-
bining multiple model aggregators based on the refer-
ence value, i.e., (9) and (10) in this article. This greatly
reduces the evaluation errors caused by the non-IID data
set for each different local model.
Accordingly, the proposed BFL remains well adaptability to
cases with non-IID data sets. More experimental verifications
are presented in the following section.
VI. PERFORMANCE EVALUATION
A. Simulation Settings
1) Federated Learning Settings: In our experiments, ten
data owners and five model aggregation nodes are set. Two
public data sets, MNIST and CIFAR10, are selected for fed-
erated learning testing. According to the proposed BFL, each
data owner reads a number of data from the prepared training
set in each round of the task. To illustrate the adaptivity of
BFL in the case of the non-IID data set, the data assigned
to nodes are artificially set to be non-IID. In addition to
this, the data from malicious nodes will tamper with the
data labels according to their maliciousness, thus simulating
data of different quality. 0%, 25%, 50%, 75%, and 100%
error rates correspond to opposite data quality are consid-
ered, respectively. For example, if the experiment requires
simulating data with an accuracy rate of 75%, then the 25%
data label is artificially set to be wrong. Similarly, the test
sets are equally distributed to the model aggregation nodes,
and the data sets obtained by each node have different dis-
tributions. Two tolerances were prepared for the experiment:
σ1 = 0.01, σ2 = −0.01, θ1 = 0.01, θ2 = 0.004 and
σ1 = 0.02, σ2 = −0.002, θ1 = 0.01, θ2 = 0.004. Although
the data owners have non-IID data sets, the system still ensures
that all data covered the complete sample space.
Algorithm 1 Federated Learning Contract: Data Owner
Input: Data owner n, query contract Cq, model aggregators M
Initialization: Global model w = 0, new local model
wn
1: for t = 1→ T do
2: if n /∈ Cq.memberListQuery() then
3: break
4: end if
globalModelDownloading(n)→ w
contribute sn
w
sn→ wn
localModelUploading(M, wn)
5: end for
Local training of nodes is performed for MNIST and
CIFAR10 using two deep learning methods, respectively. The
experiment utilizes the simplest three-layer fully connected
network to train the MNIST model. The deep learning struc-
ture for training CIFAR10 consists of a convolutional layer
and a fully connected layer. The convolutional layer consists
of two convolutional operations and two pooling operations.
The fully connected layer is a two-layer structure. Set their
learning rate to 0.01. The times of the entire federated learn-
ing training are 10 and the epoch number in local training
is 20.
2) Blockchain Settings: There are many existing
blockchain platforms that support smart contracts, e.g.,
Ethereum [40] and Hyperledger Fabric [41]. Their own smart
contract mechanism already does a good job of implementing
the contract design of our proposed solution. The experiment
implements the BFL based on Hyperledger Fabric.
Data owners and model aggregators exist as peer nodes in
the Fabric network. Meanwhile, model aggregators are set up
in the network as orderer nodes in Fabric. They perform the
task of packing the blocks. Orderer creates a channel and all
nodes join the channel to form a consortium. Set the maxi-
mum size of a transaction to 512 kB, the maximum size of a
block to 10M, and the blockout period to 10 s. Configure the
Fabric with MaxMessageCount = 20 so that when the number
of transactions reaches 20, the block can still be packed even
if the blockout period is not met. In combination with smart
contracts, Algorithms 1 and 2 show the pseudocode imple-
mentations of the most important function, federated learning
in BFL.
B. Results and Analysis
1) Performance of BFL: The experiments first evaluate the
accuracy performance of Google’s proposed FedAvg and our
proposed BFL-based Federated Learning Algorithm when dif-
ferent numbers of malicious nodes are present in the network.
The experiments are set to have 0%, 10%, 30%, and 50%
of the number of malicious nodes, respectively. To make the
effect most visible, each malicious node provides data of 0%
quality. Fig. 2 shows the performance of the two algorithms
executing MNIST and CIFAR10 training tasks in this environ-
ment. It can be seen that the task is indeed better trained under
the independent identical distribution than under the non-IID.
However, our proposed scheme obtains better performance
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
QI et al.: HIGH-QUALITY MODEL AGGREGATION FOR BLOCKCHAIN-BASED FEDERATED LEARNING 18387
Fig. 2. Performance of our proposed BFL algorithm and FedAVG algorithm is compared in two data sets with different degrees of malicious node occupancy.
(Both IID and non-IID cases are considered, respectively.)
Algorithm 2 Federated Learning Contract: Model Aggregator
Input: model aggregator m, data owners N , model aggregators M,
query contract Cq, reputation constract Cr, reward constract Cre
Initialization: Global model w = 0, local models ws = {}, contribu-
tions ss = {}, testing result rs = {}, reputations reps = {}
Output: w
1: for t = 1→ T do
2: for n ∈ N do
3: if n /∈ Cq.memberListQuery() then
4: N − n→ N
5: end if
6: end for
7: localModelDownloading(N )→ ws
8: testResultUploading(ws)→ rs
9: Cr.reputationUpdating(N , rs)→ reps
10: modelAggregation(ws, reps, ss)→ w
11: globalModelUploading(w)
12: Cre.rewardAllocation(N , reps, ss)
13: end for
14: return w
than FedAVG in different environments. Comparing the impact
of different levels of malicious node occupancy on the task,
it is clear from the figure that our proposed algorithm is rel-
atively resistant to malicious nodes. In particular, when the
malicious node percentage reaches 50%, our proposed scheme
still maintains a high performance, while FedAVG is severely
damaged.
In addition to accuracy, the experiments also evaluate the
time cost of the three most critical components of the proposed
algorithm: 1) model aggregation; 2) model training; and 3) rep-
utation evaluation mechanism. As can be seen from Fig. 3, our
proposed algorithm has more time cost on reputation evalu-
ation compared to the traditional FedAVG algorithm. It can
be seen that when the number of nodes is 10, the time cost
of reputation evaluation is close to half of the training time.
To optimize the performance of reputation evaluation, optional
Fig. 3. Comparison of the time cost between our proposed scheme and the
FedAVG algorithm.
Fig. 4. Comparison of system time cost with and without the grouping
mechanism turned on in the reputation evaluation mechanism in BFL.
grouping schemes are provided in our algorithm. Fig. 4 rep-
resents the comparison of time cost when grouping scheme
is selected or not in different number of nodes. It can be
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
18388 IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 19, 1 OCTOBER 2022
Fig. 5. Average throughput of each interface in the smart contracts when
the number of client requests is 1000.
Fig. 6. Average response latency of each interface in the smart contracts
when the number of data owners is 100.
clearly seen that as the number of nodes increases, the time
cost of reputation evaluation without taking a grouping scheme
becomes large. When the number of nodes is 100, it even
takes close to 160 s, which is already much larger than the
time cost of the training task itself. If the grouping scheme is
turned on, the time cost of reputation evaluation is still kept
small. Therefore, when the number of nodes is too large, the
grouping scheme can be turned on, which improves the effi-
ciency and also compensates the impact of grouping on the
reputation evaluation accuracy due to the excessive number of
nodes.
To further illustrate that our proposed scheme works prop-
erly on Hyperledger Fabric, experiments are conducted to test
the performance of smart contracts using Hyperledger Caliper.
Fig. 5 represents the throughput of the interfaces provided by
each of the major smart contracts when the number of requests
reaches 1000. It can be seen that the throughput of a single
interface of the system is in the range of 200 times per sec-
ond to 400 times per second. Fig. 6 shows the latency of each
interface when the number of nodes is 100. The experiments
demonstrate that the federated learning operations have rela-
tively low latency in smart contracts. Thus, the lower latency
and reasonable throughput indicate that the existing Fabric
platform is well adapted to the proposed approach.
2) Performance of the Reputation Evaluation Mechanism:
To show the performance of the proposed reputation evalua-
tion mechanism, the experiments first test its node reputation
variation under different levels of tolerance. The next exper-
iment sets the percentage of malicious nodes to 10%. The
experiment makes one node out of ten nodes as a malicious
node and observes its performance. Fig. 7(a) represents the
average reputation change for malicious (quality = 0%) and
honest nodes with high and low tolerance, respectively. The
reputation changes of malicious and honest nodes show very
different trends at the same tolerance level. The reputation of
the honest node grows slowly and is always greater than 1,
while the reputation of the malicious node decreases rapidly
and finally reaches 0. From another perspective, with different
tolerance levels, the higher the tolerance level, the higher the
reputation evaluation of honest nodes, while the reputation of
malicious nodes decreases more slowly.
To illustrate the impact of the data quality on the reputation
evaluation for a single node. Since there will be cases where
the system holds nodes with poor data quality without mali-
cious intent, it is necessary to make clear that our proposed
scheme will not disadvantage such nodes. Fig. 7(b) shows the
variation of nodes’ reputation at different data quality. The
higher the data quality is offered, the higher overall reputation
is obtained. When nodes contribute 100% data quality, their
reputation is always greater than 1. When nodes’ data quality is
too low, such as 0% and 25%, their reputation drops quickly
and eventually becomes 0. The focus is on 50% and 75%
data quality, where their reputation drops relatively slowly and
eventually stays at a more suitable reputation evaluation. This
is probably due to the fact that most of the data still have a
positive effect on the task.
In the proposed scheme, the amount of data contribution
that a node can report per round is limited by its reputation
value. Fig. 7(c) represents the optimal amount of data for a
single node when all the others contribute data with 100%
data quality. It can be seen that the worse the data quality is,
the faster its optimal data amount decreases. It is worth noting
that when the data quality is 100%, the optimal data amount
of a node decreases because its reputation value is greater than
1. This distinguishes it from the other cases. The data amount
allowed to be reported will be decreased due to the decline of
reputation.
Meanwhile, for different amounts of data contribution, the
benefits of the node show corresponding differences. From
an overall perspective, the higher the data quality is offered,
the higher the benefit can be obtained, as shown in Fig. 7(d).
When the data quality is 100%, the node’s benefit continues to
rise and finally tends to be stable. Other than that, the rest of
the cases carrying wrong data, the node utilities show a trend
of increasing and then decreasing. This is because although
the data contribution of the malicious nodes is decreasing, at
first the contribution of the honest nodes is also decreasing
due to their rising reputation. This instead leads to an increase
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
QI et al.: HIGH-QUALITY MODEL AGGREGATION FOR BLOCKCHAIN-BASED FEDERATED LEARNING 18389
(a) (b)
(c) (d)
Fig. 7. Variation of node reputation, data contribution, and utilities under different scenarios. (a) Variation of average reputation of malicious and honest
nodes under different tolerance levels. (b) Variation of reputation of a single node when contributing data of different quality. (c) Variation of optimal data
contribution of a single node when contributing data of different quality. (d) Comparison of utilities of a single node when contributing data of different
quality.
in their benefits in the short term. However, when the rise
in reputation of honest nodes slows down, the decrease in
their data contribution also starts to slow down. Therefore, the
benefits of malicious nodes start to decrease, and the worse
the data quality is the faster it decreases. The utility of a node
with very poor quality will become zero.
C. Security and Reliability Analysis
Compared to traditional centralized federated learning
systems, the proposed BFL improves the reliability and secu-
rity of federated learning from the following aspects.
1) Smart Contract: Blockchain technology provides a
decentralized environment for the deployment and execution of
federated learning smart contracts. In BFL, smart contracts are
backed up by the entire network. Smart contracts do not need
to worry about malicious tampering and also do not need to
worry about the contract not being executed after the contract
conditions are met.
2) Model: In BFL, both the local and global models gener-
ated during the training process are stored in the blockchain.
The tamper-proof feature of blockchain keeps these models
from being maliciously broken and also enables the models to
be audited at any time.
3) Reputation: The security and reliability are the corner-
stone of the proper execution of the entire BFL. The reputation
evaluations generated in BFL’s reputation mechanism are
backed up by nodes across the blockchain network, which
prevents nodes from conspiring with each other to tamper with
the reputation of others.
VII. CONCLUSION
In this article, the issue of how to improve the model
quality of federated learning is concerned. The BFL with
a reputation mechanism is proposed for high-quality model
aggregation. A consortium blockchain with systematic smart
contracts is designed to conduct the decentralized federated
learning tasks in a trustworthy and reliable way. A reputation
mechanism together with reputation constrained reward alloca-
tion is developed, which not only motivates the data owners to
participate the learning tasks but also to contribute high-quality
data. The behavior strategies of data owners are formulated
by the noncooperative game. The derived unique equilibrium
proves that the data owners can acquire highest reward with
contribution of highest quality data. Simulations based on the
public data sets show that the model accuracy by BFL can
approach the ideal state and has advantage over some existing
methods. In future work, more security issues on the BFL will
be considered, such as identity privacy protection.
REFERENCES
[1] C. Zhang, H. Zhang, J. Qiao, D. Yuan, and M. Zhang, “Deep transfer
learning for intelligent cellular traffic prediction based on cross-domain
big data,” IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1389–1401,
Jun. 2019.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
18390 IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 19, 1 OCTOBER 2022
[2] S. Biswas, S. G. Anavatti, and M. A. Garratt, “Multiobjective mission
route planning problem: A neural network-based forecasting model for
mission planning,” IEEE Trans. Intell. Transp. Syst., vol. 22, no. 1,
pp. 430–442, Jan. 2021.
[3] T. Yang et al., “Intelligent imaging technology in diagnosis of colorectal
cancer using deep learning,” IEEE Access, vol. 7, pp. 178839–178847,
2019.
[4] Z. Ba, T. Zheng, X. Zhang, Z. Qin, B. Li, X. Liu, and K. Ren, “Learning-
based practical smartphone eavesdropping with built-in accelerometer,”
in Proc. Netw. Distrib. Syst. Symp., San Diego, CA, USA, 2020,
pp. 23–26.
[5] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. Y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in Proc. 20th Int. Conf. Artif. Intell. Statist., Fort Lauderdale, FL,
USA, 2017, pp. 1273–1282.
[6] H. B. McMahan, E. Moore, D. Ramage, and B. A. Y. Arcas. “Federated
Learning of Deep Networks using Model Averaging.” 2017. [Online].
Available: https://uk.arxiv.org/abs/1602.05629v1
[7] H. Yu et al., “A fairness-aware incentive scheme for federated learning,”
in Proc. AAAI/ACM Conf. AI Ethics Soc., New York, NY, USA, 2020,
pp. 393–399.
[8] S. Yang, F. Wu, S. Tang, X. Gao, B. Yang, and G. Chen, “On design-
ing data quality-aware truth estimation and surplus sharing method for
mobile crowdsensing,” IEEE J. Sel. Areas Commun., vol. 35, no. 4,
pp. 832–847, Apr. 2017.
[9] J. Augustine, N. Chen, E. Elkind, A. Fanelli, N. Gravin, and D. Shiryaev,
“Dynamics of profit-sharing games,” Internet Math., vol. 11, no. 1,
pp. 1–22, 2015.
[10] S. Gollapudi, K. Kollias, D. Panigrahi, and V. Pliatsika, “Profit sharing
and efficiency in utility games,” in Proc. Annu. Eur. Symp. Algorithms,
Dagstuhl, Germany, 2017, pp. 1–14.
[11] Y. Zhan, J. Zhang, Z. Hong, L. Wu, P. Li, and S. Guo, “A
survey of incentive mechanism design for federated learning,”
IEEE Trans. Emerg. Topics Comput., early access, Mar. 3, 2021,
doi: 10.1109/TETC.2021.3063517.
[12] J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, “Incentive mech-
anism for reliable federated learning: A joint optimization approach
to combining reputation and contract theory,” IEEE Internet Things J.,
vol. 6, no. 6, pp. 10700–10714, Dec. 2019.
[13] Z. Song, H. Sun, H. H. Yang, X. Wang, Y. Zhang, and T. Q. S. Quek,
“Reputation-based federated learning for secure wireless networks,”
IEEE Internet Things J., vol. 9, no. 2, pp. 1212–1226, Jan. 2022,
doi: 10.1109/JIOT.2021.3079104.
[14] S. Nakamoto. “Bitcoin: A Peer-to-peer Electronic Cash System.” 2008.
[Online]. Available: https://bitcoin.org/bitcoin.pdf
[15] Y. Yuan and F.-Y. Wang, “Blockchain and cryptocurrencies: Model,
techniques, and applications,” IEEE Trans. Syst., Man, Cybern., Syst.,
vol. 48, no. 9, pp. 1421–1428, Sep. 2018.
[16] K. Upadhyay, R. Dantu, Z. Zaccagni, and S. Badruddoja, “Is your legal
contract ambiguous? Convert to a smart legal contract,” in Proc. IEEE
Int. Conf. Blockchain, Rhodes, Greece, 2020, pp. 273–280.
[17] M. A. Ferrag, M. Derdour, M. Mukherjee, A. Derhab, L. Maglaras,
and H. Janicke, “Blockchain technologies for the Internet of Things:
Research issues and challenges,” IEEE Internet Things J., vol. 6, no. 2,
pp. 2188–2204, Apr. 2019.
[18] M. A. Ferrag and L. Shu, “The performance evaluation of blockchain-
based security and privacy systems for the Internet of Things: A tutorial,”
IEEE Internet Things J., vol. 8, no. 24, pp. 17236–17260, Dec. 2021.
[19] S. Xia, F. Lin, Z. Chen, C. Tang, Y. Ma, and X. Yu, “A Bayesian
game based vehicle-to-vehicle electricity trading scheme for blockchain-
enabled Internet of Vehicles,” IEEE Trans. Veh. Technol., vol. 69, no. 7,
pp. 6856–6868, Jul. 2020.
[20] S. Wang, L. Ouyang, Y. Yuan, X. Ni, X. Han, and F.-Y. Wang,
“Blockchain-enabled smart contracts: Architecture, applications, and
future trends,” IEEE Trans. Syst., Man, Cybern., Syst., vol. 49, no. 11,
pp. 2266–2277, Nov. 2019.
[21] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” IEEE Signal Process. Mag.,
vol. 37, no. 3, pp. 50–60, May 2020.
[22] H. Elayan, M. Aloqaily, and M. Guizani, “Sustainability of health-
care data analysis IoT-based systems using deep federated learn-
ing,” IEEE Internet Things J., early access, Aug. 9, 2021,
doi: 10.1109/JIOT.2021.3103635.
[23] W. Y. B. Lim et al., “Dynamic contract design for federated learning in
smart healthcare applications,” IEEE Internet Things J., vol. 8, no. 23,
pp. 16853–16862, Dec. 2021, doi: 10.1109/JIOT.2020.3033806.
[24] Y. Li, X. Tao, X. Zhang, J. Liu, and J. Xu, “Privacy-preserved federated
learning for autonomous driving,” IEEE Trans. Intell. Transp. Syst., early
access, Jun. 1, 2021, doi: 10.1109/TITS.2021.3081560.
[25] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, “Blockchain
empowered asynchronous federated learning for secure data sharing
in Internet of Vehicles,” IEEE Trans. Veh. Technol., vol. 69, no. 4,
pp. 4298–4311, Apr. 2020.
[26] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain and
federated learning for privacy-preserved data sharing in industrial IoT,”
IEEE Trans. Ind. Informat., vol. 16, no. 6, pp. 4177–4186, Jun. 2020.
[27] A. Fu, X. Zhang, N. Xiong, Y. Gao, H. Wang, and J. Zhang,
“VFL: A verifiable federated learning with privacy-preserving for big
data in industrial IoT,” IEEE Trans. Ind. Informat., vol. 18, no. 5,
pp. 3316–3326, May 2022, doi: 10.1109/TII.2020.3036166.
[28] Z. Su et al., “Secure and efficient federated learning for smart grid with
edge-cloud collaboration,” IEEE Trans. Ind. Informat., vol. 18, no. 2,
pp. 1333–1344, Feb. 2022, doi: 10.1109/TII.2021.3095506.
[29] M. Song et al., “Analyzing user-level privacy attack against federated
learning,” IEEE J. Sel. Areas Commun., vol. 38, no. 10, pp. 2430–2444,
Oct. 2020.
[30] S. Fan, H. Zhang, Y. Zeng, and W. Cai, “Hybrid blockchain-based
resource trading system for federated learning in edge computing,” IEEE
Internet Things J., vol. 8, no. 4, pp. 2252–2264, Feb. 2021.
[31] R. Zeng, S. Zhang, J. Wang, and X. Chu, “FMore: An incentive
scheme of multi-dimensional auction for federated learning in MEC,”
in Proc. IEEE 40th Int. Conf. Distrib. Comput. Syst., Singapore, 2020,
pp. 278–288.
[32] L. Dong and Y. Zhang, “Federated learning service market: A game the-
oretic analysis,” in Proc. Int. Conf. Wireless Commun. Signal Process.,
Nanjing, China, 2020, pp. 227–232.
[33] Y. Zhan, P. Li, Z. Qu, D. Zeng, and S. Guo, “A learning-based incentive
mechanism for federated learning,” IEEE Internet Things J., vol. 7, no. 7,
pp. 6360–6368, Jul. 2020.
[34] M. Wu, D. Ye, J. Ding, Y. Guo, R. Yu, and M. Pan, “Incentivizing
differentially private federated learning: A multidimensional contract
approach,” IEEE Internet Things J., vol. 8, no. 13, pp. 10639–10651,
Jul. 2021.
[35] N. Ding, Z. Fang, and J. Huang, “Optimal contract design for efficient
federated learning with multi-dimensional private information,” IEEE J.
Sel. Areas Commun., vol. 39, no. 1, pp. 186–200, Jan. 2021.
[36] P. Sun et al., “Pain-FL: Personalized privacy-preserving incentive for
federated learning,” IEEE J. Sel. Areas Commun., vol. 39, no. 12,
pp. 3805–3820, Dec. 2021.
[37] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Commun. Lett., vol. 24, no. 6, pp. 1279–1283,
Jun. 2020.
[38] J. Kang et al., “Optimizing task assignment for reliable blockchain-
empowered federated edge learning,” IEEE Trans. Veh. Technol., vol. 70,
no. 2, pp. 1910–1923, Feb. 2021.
[39] Y. Qu, S. R. Pokhrel, S. Garg, L. Gao, and Y. Xiang, “A blockchained
federated learning framework for cognitive computing in industry 4.0
networks,” IEEE Trans. Ind. Informat., vol. 17, no. 4, pp. 2964–2973,
Apr. 2021.
[40] D. D. Wood. “Ethereum: A Secure Decentralised Generalised
Transaction Ledger.” 2014. [Online]. Available: https://ethereum.github.
io/yellowpaper/paper.pdf
[41] E. Androulaki et al., “Hyperledger fabric: A distributed operating system
for permissioned blockchains,” in Proc. 13th EuroSys Conf., New York,
NY, USA, 2018, pp. 1–15.
Jiahao Qi received the B.Eng. degree from the
College of Mathematics and Computer Science,
Zhejiang Normal University, Jinhua, China, in 2019,
where he is currently pursuing the M.Eng. degree.
His research interests include federated learning
and blockchain technology.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
http://dx.doi.org/10.1109/TETC.2021.3063517
http://dx.doi.org/10.1109/JIOT.2021.3079104
http://dx.doi.org/10.1109/JIOT.2021.3103635
http://dx.doi.org/10.1109/JIOT.2020.3033806
http://dx.doi.org/10.1109/TITS.2021.3081560
http://dx.doi.org/10.1109/TII.2020.3036166
http://dx.doi.org/10.1109/TII.2021.3095506
QI et al.: HIGH-QUALITY MODEL AGGREGATION FOR BLOCKCHAIN-BASED FEDERATED LEARNING 18391
Feilong Lin (Member, IEEE) received the B.Eng.
and M.Eng. degrees in electronic information engi-
neering from Xidian University, Xi’an, China,
in 2004 and 2007, respectively, and the Ph.D.
degree in control science and engineering from
Shanghai Jiao Tong University, Shanghai, China, in
2016.
He joined the School of Mathematics and
Computer Science, Zhejiang Normal University,
Jinhua, China, in 2016, where he is currently an
Associate Professor and the Associate Director of
the Department of Computer Science and Engineering. He is also the Deputy
Director of the Blockchain Lab, Zhejiang Normal University. His research
interests include blockchain, edge computing, and federated learning, and
their applications in industrial networks, medical big data, and new energy
networks.
Zhongyu Chen received the Ph.D. degree from
the College of Computer, Shanghai University,
Shanghai, China, in 2011.
He is currently a Full Professor with the
Department of Computer Science and Engineering,
Zhejiang Normal University, Jinhua, China. He is the
Chief Director of the Blockchain Lab and the Center
of Software R&D, Zhejiang Normal University
and the Vice Chairman of Zhejiang Blockchain
Technology Application Association. His research
interests include software engineering, big data, and
blockchain technology and applications.
Changbing Tang (Member, IEEE) received the B.S.
and M.S. degrees in mathematics and applied math-
ematics from Zhejiang Normal University, Jinhua,
China, in 2004 and 2007, respectively, and the
Ph.D. degree from the Department of Electronic
Engineering, Fudan University, Shanghai, China, in
2014.
He is currently an Associate Professor with the
College of Physics and Electronics Information
Engineering, Zhejiang Normal University. His
current research interests include game theory,
blockchain and its applications, networks, and distributed optimization.
Dr. Tang was a recipient of the Academic New Artist Doctoral Post
Graduate from the Ministry of Education of China in 2012.
Riheng Jia received the B.E. degree in electron-
ics and information engineering from Huazhong
University of Science and Technology, Wuhan,
China, in 2012, and the Ph.D. degree in computer
science and technology from Shanghai Jiao Tong
University, Shanghai, China, in 2018.
He is currently an Associate Professor with the
Department of Computer Science and Engineering,
Zhejiang Normal University, Jinhua, China. His cur-
rent research interests include wireless networks,
energy harvesting networks, and smart IoT.
Minglu Li (Senior Member, IEEE) received the
Ph.D. degree in computer software from Shanghai
Jiao Tong University, Shanghai, China, in 1996.
He is a Full Professor and the Director of Artificial
Intelligence Internet of Things Center, Zhejiang
Normal University, Jinhua, China. He is also hold-
ing the Director of the Network Computing Center,
Shanghai Jiao Tong University. He has published
more than 400 papers in academic journals and inter-
national conferences. His research interests include
vehicular networks, big data, cloud computing, and
wireless sensor networks.
Prof. Li was the Chairman of Technical Committee on Services Computing
from 2004 to 2016 and Technical Committee on Distributed Processing from
2005 to 2017 of IEEE Computer Society in Great China region. He served
as a General Co-Chair for IEEE SCC, IEEE CCGrid, IEEE ICPADS, and
IEEE IPDPS and a Vice Chair for IEEE INFOCOM. He also served as
a PC Member of more than 50 international conferences, including IEEE
INFOCOM 2009–2016 and IEEE CCGrid 2008.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:05:51 UTC from IEEE Xplore.  Restrictions apply. 
<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles false
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.4
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /LeaveColorUnchanged
  /DoThumbnails false
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo false
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts false
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
    /Arial-Black
    /Arial-BoldItalicMT
    /Arial-BoldMT
    /Arial-ItalicMT
    /ArialMT
    /ArialNarrow
    /ArialNarrow-Bold
    /ArialNarrow-BoldItalic
    /ArialNarrow-Italic
    /ArialUnicodeMS
    /BookAntiqua
    /BookAntiqua-Bold
    /BookAntiqua-BoldItalic
    /BookAntiqua-Italic
    /BookmanOldStyle
    /BookmanOldStyle-Bold
    /BookmanOldStyle-BoldItalic
    /BookmanOldStyle-Italic
    /BookshelfSymbolSeven
    /Century
    /CenturyGothic
    /CenturyGothic-Bold
    /CenturyGothic-BoldItalic
    /CenturyGothic-Italic
    /CenturySchoolbook
    /CenturySchoolbook-Bold
    /CenturySchoolbook-BoldItalic
    /CenturySchoolbook-Italic
    /ComicSansMS
    /ComicSansMS-Bold
    /CourierNewPS-BoldItalicMT
    /CourierNewPS-BoldMT
    /CourierNewPS-ItalicMT
    /CourierNewPSMT
    /EstrangeloEdessa
    /FranklinGothic-Medium
    /FranklinGothic-MediumItalic
    /Garamond
    /Garamond-Bold
    /Garamond-Italic
    /Gautami
    /Georgia
    /Georgia-Bold
    /Georgia-BoldItalic
    /Georgia-Italic
    /Haettenschweiler
    /Helvetica
    /Helvetica-Bold
    /HelveticaBolditalic-BoldOblique
    /Helvetica-BoldOblique
    /Impact
    /Kartika
    /Latha
    /LetterGothicMT
    /LetterGothicMT-Bold
    /LetterGothicMT-BoldOblique
    /LetterGothicMT-Oblique
    /LucidaConsole
    /LucidaSans
    /LucidaSans-Demi
    /LucidaSans-DemiItalic
    /LucidaSans-Italic
    /LucidaSansUnicode
    /Mangal-Regular
    /MicrosoftSansSerif
    /MonotypeCorsiva
    /MSReferenceSansSerif
    /MSReferenceSpecialty
    /MVBoli
    /PalatinoLinotype-Bold
    /PalatinoLinotype-BoldItalic
    /PalatinoLinotype-Italic
    /PalatinoLinotype-Roman
    /Raavi
    /Shruti
    /Sylfaen
    /SymbolMT
    /Tahoma
    /Tahoma-Bold
    /Times-Bold
    /Times-BoldItalic
    /Times-Italic
    /TimesNewRomanMT-ExtraBold
    /TimesNewRomanPS-BoldItalicMT
    /TimesNewRomanPS-BoldMT
    /TimesNewRomanPS-ItalicMT
    /TimesNewRomanPSMT
    /Times-Roman
    /Trebuchet-BoldItalic
    /TrebuchetMS
    /TrebuchetMS-Bold
    /TrebuchetMS-Italic
    /Tunga-Regular
    /Verdana
    /Verdana-Bold
    /Verdana-BoldItalic
    /Verdana-Italic
    /Vrinda
    /Webdings
    /Wingdings2
    /Wingdings3
    /Wingdings-Regular
    /ZapfChanceryITCbyBT-MediumItal
    /ZWAdobeF
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 200
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages false
  /ColorImageDownsampleType /Average
  /ColorImageResolution 300
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 200
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages false
  /GrayImageDownsampleType /Average
  /GrayImageResolution 300
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 400
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages false
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False
  /CreateJDFFile false
  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Recommended"  settings for PDF Specification 4.01)
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice