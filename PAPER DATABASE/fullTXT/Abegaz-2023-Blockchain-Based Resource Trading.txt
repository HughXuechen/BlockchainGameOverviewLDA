Blockchain-Based Resource Trading in Multi-UAV-Assisted Industrial IoT Networks: A Multi-Agent DRL Approach
166 IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, VOL. 20, NO. 1, MARCH 2023
Blockchain-Based Resource Trading in
Multi-UAV-Assisted Industrial IoT Networks:
A Multi-Agent DRL Approach
Abegaz Mohammed Seid , Member, IEEE, Hayla Nahom Abishu , Yasin Habtamu Yacob ,
Tewodros Alemu Ayall, Aiman Erbad , Senior Member, IEEE, and Mohsen Guizani , Fellow, IEEE
Abstract—With the Industrial Internet of Things (IIoT), mobile
devices (MDs) and their demands for low-latency data communi-
cation are increasing. Due to the limited resources of MDs, such
as energy, computation, storage, and bandwidth, IIoT systems
cannot meet MDs’ quality of service (QoS) and security require-
ments. Recently, UAVs have been deployed as aerial base stations
in the IIoT network to provide connectivity and share resources
with MDs. We consider a resource trading environment where
multiple resource providers compete to sell their resources to
MDs and maximize their profit by continually adjusting their
pricing strategies. Multiple MDs, on the other hand, interact
with the environment to make purchasing decisions based on the
prices set by resource providers to reduce costs and improve QoS.
We propose a novel intelligent resource trading framework that
integrates multi-agent deep reinforcement Learning (MADRL),
blockchain, and game theory to manage dynamic resource trad-
ing environments. A consortium blockchain with a smart contract
is deployed to ensure the security and privacy of the resource
transactions. We formulated the optimization problem using a
Stackelberg game. However, the formulated optimization problem
in the multi-agent IIoT environment is complex and dynamic,
making it difficult to solve directly. Thus, we transform it into
a stochastic game to solve the dynamics of the optimization
problem. We propose a dynamic pricing algorithm that combines
the Stackelberg game with the MADRL algorithm to solve the
formulated stochastic game. The simulation results show that our
proposed scheme outperforms others to improve resource trading
in UAV-assisted IIoT networks.
Index Terms—Blockchain, DRL, industrial IoT, resource trad-
ing, unmanned aerial vehicles.
Manuscript received 18 February 2022; revised 15 June 2022; accepted
27 July 2022. Date of publication 9 August 2022; date of current version
7 March 2023. This work was made possible by NPRP-Standard (NPRP-S)
Thirteen (13th) Cycle grant NPRP13S-0205-200265 from the Qatar National
Research Fund. The findings achieved herein are solely the responsibility
of the authors. The associate editor coordinating the review of this arti-
cle and approving it for publication was J. Zhang. (Corresponding author:
Aiman Erbad.)
Abegaz Mohammed Seid and Aiman Erbad are with the Division of
Information and Computing Technology, College of Science and Engineering,
Hamad Bin Khalifa University, Doha, Qatar (e-mail: mamsied2002@
gmail.com; aerbad@hbku.edu.qa).
Hayla Nahom Abishu and Yasin Habtamu Yacob are with the School of
Computer Science and Engineering, University of Electronic Science and
Technology of China, Chengdu 611731, China.
Tewodros Alemu Ayall is with the Department of Computer Science,
Zhejiang Normal University, Jinhua 321004, Zhejiang, China.
Mohsen Guizani is with the Machine Learning Department, Mohamed Bin
Zayed University of Artificial Intelligence, Abu Dhabi, UAE.
Digital Object Identifier 10.1109/TNSM.2022.3197309
I. INTRODUCTION
OVER the previous decade, the Internet of Things (IoT)
revolution has had a significant impact on manufactur-
ing, energy, agriculture, transportation, and other industrial
sectors [1]. The Industrial IoT (IIoT) is an industry-specific
variant of the IoT, which provides an impressive potential
for businesses via connected machines, sensors, and appli-
cations [2], [3]. It is one of the most exciting technologies
now reshaping industrial enterprises, prompting them to mod-
ernize their processes, system intelligence, and facilities in
order to cope with emerging disruptive technologies. The IIoT
improves manufacturing efficiency, safety, scalability, produc-
tion time, and profitability in the industrial sector [4]. In this
regard, beyond fifth-generation (B5G) networks are envisioned
to provide factory platforms capable of realizing mission-
critical use-cases in an industrial setting with low latency, high
reliability, and the capacity to accommodate a much larger
number of IIoT devices (IIoTD) [5]. It promises to improve
factory automation by supporting ultra-reliable low latency
communication (uRLLC), enhanced machine type commu-
nication (eMTC), and enhanced mobile broadband (eMBB).
Thus, current industry systems connect more sophisticated
mobile devices (MDs), such as sensitive and precise sen-
sors and location-aware technologies, to develop integrated
manufacturing and supply chain monitoring. These MDs run
mission-critical and ultra-low latency applications/services like
online gaming, augmented/virtual reality, and image/video
encoding to provide broader, advanced, and automated con-
trols in industries like aerospace, defense, healthcare, and
energy [6], [7], [8]. However, most of MDs connected to
the IIoT ecosystem (e.g., sensors, actuators, and so on)
have resource constraints such as computation, battery, spec-
trum, and storage in order to communicate efficiently in the
system [9], [10].
Recently, many researchers have attempted to propose solu-
tions for resource trading in IIoT networks by integrating
blockchain and B5G enabler technologies such as software-
defined industry automation networks (SDIAN) [11], network
function virtualization (NFV), and smart contract (SC) [12].
The integration of B5G systems, mobile edge computing
(MEC), and network slicing has been introduced to meet
MDs’ ultra-low latency response and quality of service (QoS)
requirements [13], [14]. Blockchain is a technology that uses
1932-4537 c© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
https://orcid.org/0000-0002-3672-6132
https://orcid.org/0000-0002-3243-7579
https://orcid.org/0000-0003-2113-8538
https://orcid.org/0000-0001-7565-5253
https://orcid.org/0000-0002-8972-8094
SEID et al.: BLOCKCHAIN-BASED RESOURCE TRADING IN MULTI-UAV-ASSISTED INDUSTRIAL IoT NETWORKS 167
distributed consensus and cryptographic hashing to keep dig-
ital assets secure and transparent [15]. It is being utilized in
various industries to guarantee the security and privacy of
transactions between untrusted nodes [16], [17]. Besides, the
MEC paradigm has been given great attention to solve the
computational and storage issues of MDs. The MEC oper-
ates as a new model to provide computing resources for
MDs. It allows MDs to offload their computation tasks to
a nearby edge nodes/MEC servers for processing. The MEC
servers then allocate resources, perform the requested compu-
tation tasks, and return the results to MDs in a significantly
shorter time frame. This could help MDs overcome their
computing resource restrictions and improve the user expe-
rience [18]. When there is a system failure, or MDs are
moved out of coverage, the execution time of each task can be
slowed, lowering the QoS. Thus, the B5G network has recently
deployed unmanned aerial vehicles (UAVs) as aerial base
station (ABS) equipped with MEC to provide computation
offloading [19], resource allocation, and enhanced coverage or
relaying services to MDs in wireless systems with limited or
no physical infrastructure coverage [20], [21], [22], [23], [24].
Besides, UAVs are actively used in mission-critical services
such as military, emergency communication, and health care
due to their low implementation cost, short-range line-of-sight
connection, and capacity to execute jobs like delivery services,
disaster relief, and agriculture applications that humans cannot
easily perform [25], [26], [27]. These UAVs provide flexible
short-distance wireless communication, allowing the collection
and dissemination of information at a minimal cost. Moreover,
the rapid deployment and relocating capabilities of UAVs also
enable them to automatically adapt to dynamic and emerging
communication requirements, improving fault tolerance and
resilience in IIoT systems.
Furthermore, several studies have proposed solutions for
resource trading in IIoT networks to address the resource
limitations of MDs. Traditional resource-sharing approaches
cannot achieve the desired performance due to the dynamic
nature of the IIoT environment and the resource limitations of
MDs. Moreover, the IIoT mandates that resource sharing poli-
cies be intelligent enough to make intelligent resource access
decisions [28], [29]. In this regard, machine learning (ML) is
one of the most powerful data-driven approaches for enabling
intelligent decision-making by using a massive amount of
data from multiple heterogeneous IIoT devices [30]. In recent
years, researchers have applied ML approaches to the problem
of dynamic resource allocation and sharing, such as single-
agent reinforcement learning (RL) and deep reinforcement
learning (DRL) [31], [32], [33], [34]. However, in a com-
plex and multi-agent system, a single agent RL/DRL approach
does not achieve various optimization problems [35]. Thus,
multi-agent reinforcement learning algorithms (MARL) have
been proposed for finding optimal decision-making policies
for the pricing and resource management problem between
cloud providers and miners [35]. Nevertheless, the existing
resource trading solutions are still insufficient to address the
ultra-reliable, low latency, and differentiated service require-
ments of MDs connected to the IIoT system, particularly in
addressing the spectrum and energy constraints.
We are motivated to address the issues with MDs in the
IIoT system mentioned above. In this work, we propose a DRL
approach combined with game theory for blockchain-enabled
resource trading scheme in which UAVs are deployed as ABS
to lease resources to MDs associated to the IIoT network.
Because of its ability to handle a wide range of complex
decision-making tasks that were previously unreachable for a
machine to address real-world problems with human-like intel-
ligence, the DRL is preferred over other ML approaches. UAVs
have recently been widely deployed as ABS in the B5G system
to assist MDs connected to IIoT networks in computing [36],
resource sharing, and expanding access to areas outside of
the physical infrastructure coverage [21], [22], [23], [24]. The
consortium blockchain is used in combination with SC to
ensure the security and privacy of resource transactions. A
two-stage Stackelberg game model is adopted and integrated
with the DRL method to find the optimal pricing policy for
buyers and sellers in a resource trading environment where
there are multiple buyers and sellers. In the trading process,
the multi-agent deep deterministic policy gradient (MADDPG)
is used for intelligent decision-making policy to maximize the
benefits for both MDs and UAVs. The UAVs affordably lease
spectrum and energy resources to the MDs via wireless com-
munication systems, and the MDs can then efficiently interact
in the network to improve the performance of the industrial
system. It can significantly improve the performance of the
IIoT communication system. The main contributions of this
paper are as follows:
• We propose a novel resource trading framework that
integrates multi-agent DRL (MADRL) with consortium
blockchain and the Stackelberg game. In this framework,
UAVs act as ABS to lease resources such as spectrum and
energy to the MDs deployed in the IIoT ecosystem. We
formulate utility maximization problem as a two stage
multi-leader-multi-followers (MLMF) Stackelberg game
to allow resource sellers/UAVs and resource buyers/MDs
to maximize the aggregate reward and improve resource
trading efficiency.
• We model the optimization problem as an extended
Markov decision process (MDP)/stochastic game to han-
dle the dynamic resource trading problems of multi-UAV-
assisted IIoT networks, in which each UAV and MD acts
as a learning agent and each resource trading solution
corresponds to a UAV and MD action.
• We adopt a dynamic pricing algorithm that combines
the Stackelberg game with MADDPG algorithm, namely,
Stackelberg MADDPG (SMADDPG) to solve the for-
mulated stochastic game of multi-UAV-assisted IIoT
networks. It allows UAVs to integrate spectrum and
energy strategic planning to increase their utilities while
meeting the QoS requirements of various MDs.
• Extensive simulations are conducted to demonstrate the
efficiency of our proposed framework. The numeri-
cal analysis of these simulation results proves that the
proposed model is better than the baseline schemes.
The rest of the paper is organized as follows: Section II
summarizes related studies. The system framework used in
resource trading is described in Section III, and Section IV
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
168 IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, VOL. 20, NO. 1, MARCH 2023
presents the optimization problem formulation. Further, we
present the MADRL approach to solve the optimization
problem in Section V. Section VI presents the simulation
results and analysis. Finally, in Section VII, we conclude our
work.
II. RELATED WORK
Recently, various studies have attempted to propose a solu-
tion for the resource constraints of MDs connected to the
IoT and IIoT networks. In the IoT era, the MEC is a viable
paradigm for resolving MDs’ computational and resource allo-
cation difficulties by proximate resources at the edge level.
The MEC provides the services required of high data rate
and high computation capability [37]. Many previous research
works proposed optimization methods for task offloading in
MEC systems [38], energy consumption optimization, devices
service response latency [39]. Naram et al. [40] proposed on
designing intelligent agents to provide services in blockchain-
enabled systems. The authors propose lightweight online
learning SCs that can optimize performance using DRL agents.
The authors in [41] proposed a blockchain-based peer-to-
peer computation resource trading for edge-cloud computing
IoT, where a broker manages the computing resource trad-
ing among the sellers and buyers. Yao et al. [35] presented a
decentralized self-organized trading platform for IIoT devices,
focusing on the resource management and pricing problem
between the cloud providers and miners. In this work, cloud
mining is used to help MDs offload massive computational
work to the cloud provider, and a multi-agent RL algorithm is
introduced for finding optimal decision-making policies. This
work tries to address MDs’ computational limitations while
also enhancing the IIoT system’s efficiency through resource
trading. In [42], the authors proposed a futures-based resource
trading strategy to address the risk of trading failure and
the unfair on-site negotiating process in wireless networks to
share resources. Based on historic resource supply and demand
facts, the resource requester and resource supplier agree on a
mutually advantageous forward contract in advance.
Moreover, the authors in [43] presented a contract-based
cooperative spectrum sharing system to solve the bandwidth
limitation of MDs. In addition, a cooperative relaying tech-
nique was devised that uses superposition coding at both
cellular and device-to-device (D2D) transmitters. This max-
imizes the profit of cellular links while maximizing transmis-
sion chances for D2D lines. The authors of [44] proposed
a bandwidth-auction game-based spectrum trading system in
which base stations (BSs) can sell or share spectrum with the
D2D pair, allowing the D2D pair to operate in orthogonal or
non-orthogonal sharing. The D2D pair can buy the appropri-
ate spectrum from a variety of service providers based on the
frequency spectrum of each mode. Qiu et al. [45] proposes
a secure blockchain-based spectrum trading system in which
UAVs purchase spectrum bands from service providers/owners
of spectrum resources. In [46] presented a novel distributed
spectrum trading protocol based on blockchain, which focuses
on efficiency, simplicity, safety, and energy-saving in IoT
networks. Besides, a blockchain-enabled secure power trading
method is proposed for the smart grid using wireless networks
in [47]. Lin et al. [48] proposed a novel wirelessly pow-
ered edge intelligence framework that uses energy harvesting
approaches to generate a stable, reliable, and sustainable edge
intelligence. Baig et al. [49] introduced an IoT-based energy
trading system linked to blockchain-based financial transac-
tions. The authors of [50] presented an optimal contract-based
electricity trading method that increases profit effectively.
Nguyen et al. [51] proposed an economic model to jointly
optimize revenues of energy service providers and IoT ser-
vice providers participating in a heterogeneous IoT wireless-
powered backscatter communication network.
The above-mentioned resource trading-related studies do not
adequately address the challenges of MDs’ energy and spec-
trum resource limitations, and do not take into account the
mobility of MDs in the IIoT networks. In this work, we pro-
pose an intelligent resource trading framework that integrates
blockchain technology, DRL, game theory, and SCs. In the
UAV-assisted IIoT system, the ABS can serve MDs moving
outside physical infrastructure coverage area and when the
physical infrastructure fails due to natural or human-made dis-
asters. Therefore, MDs can buy energy and spectrum resources
from UAVs while in motion to continue performing in the
IIoT system. This can improve the reliability and consistency
of IIoT systems. Despite the benefits of integrating the B5G
key enabler technologies, such as blockchain, AI, and game
models in IIoT networks, their inappropriate implementation
can lead to inefficient resource utilization, performance degra-
dation, and security breaches. These challenges require careful
consideration when integrating these technologies. In this
regard, we integrate these technologies carefully to improve
the performance of the IIoT network. We use consortium
blockchain for better security and efficient resource utilization
because it is a lightweight platform that allows only authorized
nodes to join the system. In addition, we applied a multi-agent
DRL algorithm combined with a stochastic game model to
handle the dynamics and complexity of resource trading and
optimization problems arising due to the heterogeneity and
diversified service requirements of devices. Furthermore, edge
nodes are in charge of the consensus process to assist devices
with limited resources.
III. SYSTEM MODEL
We consider UAV-assisted IIoT network which consists of
macro BSs (MBS), UAVs, MEC servers, and heterogeneous
MDs, as shown in Fig. 1. The MBSs are deployed to pro-
vide wireless connectivity for the UAVs and MDs in the
physical infrastructure coverage area. In addition, UAVs pro-
vide wireless connectivity to devices outside of the physical
network coverage and provide access to resources for MDs.
The MDs connected to the UAVs can request spectrum, energy,
or both resources to meet their needs. The MEC performs
resource-intensive and latency-sensitive tasks, including block
mining and storage to assist resource-limited nodes. Software-
defined networking (SDN) is a network architecture that allows
for the creation of programmable resource-trading connec-
tivity services, which can dynamically manage and direct
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
SEID et al.: BLOCKCHAIN-BASED RESOURCE TRADING IN MULTI-UAV-ASSISTED INDUSTRIAL IoT NETWORKS 169
TABLE I
LIST OF NOTATIONS
traffic flows for maximum performance gain [52]. The cen-
tral SDN controller has a global and aggregated view of the
trading network. Cloud computing utilizes virtual machines
with high-performance computing and storage capabilities to
solve complicated computation tasks. It consists of blockchain
and DRL applications, which communicate securely with an
SDN controller. The certificate authority (CA) is responsi-
ble for issuing identity certificates to entities participating in
the resource trading system. The CA manages the network
infrastructure and ensures its security and privacy.
It maintains the list of registered users with their associ-
ated SCs, and the SC is used to enforce compliance with
the agreements signed by the entities. Moreover, heteroge-
neous IIoT devices such as autonomous industrial machines,
mobile vehicles, robots, and management systems are con-
nected via ubiquitous connectivity to control and manage
industrial machinery in real-time. These devices can exchange
their sensed data/knowledge for efficient production.
A. Business Model
We present a business model that consists of MDs, UAV
operators (UAVOs), and an infrastructure provider/primary
resource provider (PRP) to promote resource trading. These
entities have the following business relationship; PRPs own the
infrastructure and lease their resources (spectrum and energy)
to UAVOs on a service level agreement (SLA) basis. Likewise,
depending on MD’s demand, the UAVOs sell these resources
to MDs at a reasonable price. In addition, the UAVs are
equipped with solar energy receivers, allowing them to har-
vest solar energy and serve as alternative energy sources [53].
Due to the broadcast nature of the wireless environment and
multiple untrusted resource sellers and purchasers, it is result-
ing in privacy breaches, double-spending attacks, and overall
system vulnerability in the real-time resource trading process.
Therefore, we used a consortium blockchain with SC to estab-
lish a decentralized, transparent, and trusted environment for
entities who seek to trade resources. In this work, the trad-
ing entities are the UAVOs and MDs, where UAVOs act as
a resource provider and MDs act as resource requester. The
resource provider/seller and resource requester/buyer need to
contract SLA to build a harmonious environment for resource
trading. In this regard, the buyers and sellers deploy SCs that
will self-execute when the parties achieve all contractual con-
ditions, ensuring real-time renegotiation of agreements when
network traffic changes rapidly. The SC can easily manage the
agreements between UAVOs and MDs involved in the resource
trading.
B. Network Model
In this paper, we consider the IIoT network with MDs that
need a variety of services and connected to ABS. Multiple
UAVs are deployed as ABS to provide resources and network
coverage for MDs outside of the physical infrastructure cov-
erage or the terrestrial cellular network is out of service due
to natural or human-made disaster, ensuring network stabil-
ity. A group of UAVs can act as ABS to relay between
the central network controller (CNC) and IIoT networks. Let
i ∈ U = {1, 2, . . . ,U } and j ∈ D = {1, 2, . . . ,D} denotes
the set of UAVs and MDs connected to the IIoT network,
respectively. The MDs in the network are distributed at ran-
dom and organized into Z clusters. We use i UAVs that are
cellularly-connected to the core network to assist Z clusters of
MDs in the IIoT network coverage area. Both UAVs and MDs
are equipped with antennas to establish a wireless connection
between them [54]. We consider three data transmission links
at any given time slot t: UAV to MD (U2M), UAV to UAV
(U2U) in a single cluster, and UAV to core (U2C). The multi-
UAV network and the terrestrial IIoT network are managed by
a single CNC. Each cluster in the network coverage can serve
a finite number of MDs with the set of Dz = {1, . . . ,Dz },
where z ∈ Z = {1, ..,Z}. The MD (z, j) denotes the jth MD
in cluster z. The BS, ABS, and MDs are all assumed to be in
3D space, and their locations coordinate can be expressed as
(x0, y0,H0), (xi ,z , yi ,z ,Hi ,z ), ∀i ∈ U , and (xj , yj ), ∀j ∈ D,
respectively, where (xi ,z , yi ,z ) denotes location of ith ABS in
the z cluster. The antenna heights of the BS and the UAV are
denoted as H0 and Hi ,z , respectively. The distance between
the BS and the ith UAV is then expressed as [55]:
L0,z =
√(
xi ,z − x0
)2
+
(
yi ,z − y0
)2
+
(
Hi ,z − H0
)2
. (1)
In the same way, the distance between the UAV i and the MD
j in its cluster is defined by:
Li ,j =
√
α2i ,j + H 2
i ,z , j ∈ Dz , (2)
where αi ,j =
√
(xi ,z − xd )2 + (yi ,z − yj )2 is the Euclidean
distance between the UAV i and the MD j. When positioning
the ABS, considering interference from the other co-channels
is very important. We consider the impact of cross channel
interference (CCI) in the ABS placement and MD associa-
tion problems [56]. The signal-to-interference-plus-noise ratio
(SINR) of jth MD associated with the ith ABS is given as:
λ(i , j ) =
SPR(i , j )
IEAgg (i) +N0
, (3)
where SPR(ij) denotes the signal power received at the jth
MD from the ith ABS, IEAgg (i) is the aggregate interference
encountered by the jth MD, and N0 refers the power spec-
tral density of the Gaussian noise. The signal power received,
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
170 IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, VOL. 20, NO. 1, MARCH 2023
Fig. 1. System architecture.
SPR(ij) can be expressed as:
SPR(i , j ) = pi
t
[
P
(
LoS , θji
)
hL(i , j ) +
(
1− P
(
LoS , θji
))
hN (i , j )
]
,
(4)
where pit is the transmit power of ith ABS and LoS is the line
of site. Beside the aggregate CCI, IEAgg (i) is given as:
IEAgg (i) =
U∑
l=1,l �=j
SPR(i , j ), (5)
where U is the number of UAVs. The LoS and non-LoS
(NLoS) propagation from interfering ABS is included.
C. Communication Model
In this multi-UAV assisted IIoT network scenario, MDs
are connected to UAV network through wireless networks.
MDs and UAVs use the orthogonal frequency division multiple
access for their resource trading activities, in which different
MDs occupy different sub-channels for their communications.
The U2M communication channel experiences both LoS and
NLoS propagation conditions depending on the altitude of the
ABSs. Both LoS and NLoS links should be considered to eval-
uate the realistic performance of the system. The power gain
of the U2M channel is determined primarily by the free space
path loss model [57], which is given below.
Πn
i ,j = φ
(
α0/α
n
i ,j
)2
, (6)
where φ is a constant that varies with antenna physiognomies
and frequency, dignified at the reference distance α0 = 1m
and αnj ,i is the square of the Euclidean distance between UAV
i and MD j. Let m ∈ C = {1, 2, . . . ,C} denotes the set
of sub-channels available to each UAV during the restoration
process. These sub-channels will be subdivided further and
assigned to the MDs associated with each UAV. Thus, each
UAV i transmits to each MD j at a transmit power of pnj ,i ,m
per sub-channel. If UAV i is not assigned to sub-channel m,
then pnj ,i ,m equals zero. As a result, the SINR between UAV i,
and MD j per sub-channel m during time block n is as follows:
N n
j ,i ,m =
pnj ,i ,mΠn
j ,i∑
j∈D ,i �=j
∑
i∈U pni ,j ,mΠn
j ,i + ε2
, (7)
where ε2 denotes the power of the Additive White Gaussian
Noise at the receiver. Thus, the achievable per sub-channel
downlink rate from UAV i to MD j can be calculated as:
γnj ,i ,m = log2
(
1 +N n
j ,i ,m
)
. (8)
D. Blockchain Model
We employ a consortium blockchain integrated with SC,
DRL model, and game theory to ensure the security and pri-
vacy of the resource trading transactions between the UAVOs
and MDs while allowing entities to make intelligent deci-
sions to increase their utilities. A consortium blockchain is
a lightweight blockchain that is preferable than the permis-
sionless blockchain for designing a secure and distributed
resource trading framework that optimally allocates resources
from UAVOs to MDs in the multi-UAV-assisted IIoT system.
The consortium blockchain is fast, energy-efficient, and more
suitable for resource-constrained devices. SC is used to effi-
ciently manage and ensure that the parties’ agreed-upon terms
and conditions for satisfying service quality and quantity
are met. The operation of the consortium blockchain in our
proposed resource trading framework is presented in five
phases as in Algorithm 1. The detailed procedures are as
follows:
1) System Initialization and Entity Registration: Nodes that
wish to engage in trading create an account in the trading
system and receive a unique identification that proves they
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
SEID et al.: BLOCKCHAIN-BASED RESOURCE TRADING IN MULTI-UAV-ASSISTED INDUSTRIAL IoT NETWORKS 171
are a legitimate entity to participate as a seller, buyer, or
miner/consensus node (lines 1-2). We use an elliptic curve
digital signature algorithm [58] and asymmetric cryptography
for system initialization in order to guarantee the unforge-
ability and transparency of data [59]. After registering to the
trading system, authorized entities would be issued a certifi-
cate by the trusted authority. In the proposed resource trading
blockchain, every entity joins the resource trading network
with its certificate and obtains public parameters such as public
key (PK)/private key (SK) and wallet address Wadd . To ensure
the security and integrity of message transmission between the
senders and receivers, we utilize an asymmetric cryptography
scheme, which is expressed as:
DPKi
(
SigSKi
(H (m))
)
= H (m), (9)
where SigSKi
() is the digital signature of sender i with its
private key, DPKi
() is the decryption function with sender
i’s public key, and H(m) is the hash digest of message m.
Once the system is initialized, sellers and buyers interested
in trading join the system by presenting their unique identi-
ties. The system verifies the participants’ identities, sets up the
Stackelberg game, and runs SC for trading (lines 4-12).
2) Block Mining: The main process of block mining in IIoT
resource trading is adding transaction data to the resource
trade blockchain. Edge nodes/consensus nodes are responsi-
ble for achieving consensus on resource trading transactions.
Upon the initialization of the resource trading system, resource
demanding nodes begin sending requests to the nearest edge
node. The edge node then collects the requests and uses the
collected data to train the DRL model. The resource providers
then observe the environment state and audit their supplies,
set unit prices, and respond to the edge servers. Based on
consumer demand and available supplier resources, the edge
server will match demand-supply trade pairings. After that,
both consumers and suppliers can complete their transactions.
3) Block Generation: In the consortium blockchain, only
some selected nodes can participate in the block generation
and verification process. Thus, we use the hybrid of practical
byzantine fault tolerant and proof of reputation (PPoR) con-
sensus scheme, which selects miners based on their reputation
value. More than three-fourths of nodes with high reputation
value would be selected as candidate consensus nodes. And a
node with the highest reputation value is taken as the leader
of the miners. The leader node collects the transaction from
the transaction pool, creates a block, sings it, and broadcasts
to the other consensus nodes [60].
4) Consensus Process: After the leader has built and broad-
cast the block, the miners must audit the newly created block
using a consensus mechanism before adding it to the chain.
The consensus nodes check the legitimacy of the new block
broadcasted by the leader. If it is legitimate, they will start
auditing the transactions in the block and broadcast their audit
results to other consensus nodes for mutual supervision. They
compare their (lines 15-18).
5) Blockchain Update: Finally, the leader collects the
consensus nodes’ block verification reports. If more than three-
quarters of the consensus nodes agree that the block is valid,
Algorithm 1 Blockchain-Enabled Resource Trading
1: Initialize: Sellers, Buyers
2: Registration and certification of sellers, buyers, consensus
nodes
3: for time =1 to T do
4: for all i, j do
5: Verify the certificates
6: if the certificates of sellers, buyers are verified
7: set-up Stackelberg game as Eqns. (12) and (13)
8: Run SC for trading and create a block
9: else
10: Go back to initialization step
11: end if
12: end for
13: for N consensus nodes do
14: Leader l broadcast block blk to all consensus nodes
15: Each node receive the block and check its legality;
16: if blk is legal then
17: Audit and send the result to each other
18: Compare its work with others and sends to the
leader
19: The leader analyze the responses of nodes
20: if majority of nodes agree then
21: Append the block to the global chain and
broadcast updates to all nodes
22: else
23: Discard blk
24: end if
25: else
26: Ignore
27: end if
28: end for
29: end for
the leader sings on it, appends the block to the chain, and
broadcasts the update to all nodes (lines 19-24).
IV. PROBLEM FORMULATION
In this section, we formulate the utility optimization
problem of buyers and sellers as a two-stage MLMF
Stackelberg game, a strategic game in which leaders and
followers compete for resources, allowing honest nodes to
maximize their utility by actively participating in the resource
trading [61]. The resource sellers/UAVOs act as leaders,
while the buyers/MDs act as followers [62]. The resource
seller UAVOs compete with one another to sell their idle
resources by continuously observing the environment, set-
ting their resource price, and estimating QoS levels in a
non-cooperative manner. The customers/MDs also constantly
observe the state of the environment and make a matching
decision based on the costs and expected QoS of resource
providers. Each leader must determine an appropriate price
β in order to maximize utility within the constraints of their
resources. Likewise, each follower decides how to maximize
their utility (reduce their costs) while achieving a desired QoS.
In our scenario, both UAVOs and MDs are the agents who
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
172 IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, VOL. 20, NO. 1, MARCH 2023
make intelligent decisions to optimize their utilities. The ideal
response of each agent is determined using Stackelberg-based
MADDPG, a variant of the MADRL algorithm in which each
agent interacts with its environment to learn an optimal policy
that maximizes its long-term reward without prior knowledge
of the actions of others. The Stackelberg leader–follower game
is characterized as follows:
1) Players: The UAVOs and MDs are the players/agents of
the game, with the UAVOs act as the leader and MDs act as
the followers.
2) Strategy: The strategy for each UAVO is to determine the
decision strategy Λ and the price of a unit quantity of resource
β, where βe , βs ∈ β = {βe1 , βs1 , βe2 , βs2 , . . . , βeN , βsN } denote
the energy and spectrum prices. The strategy for the MDs is
to decide how much resource to buy from each UAVO. Let
θe , θs ∈ θ = {θe1 , θs1, θe2 , θs2, . . . , θeN , θsN } denote the energy
and spectrum demand of MDs. For all MDs j ∈ D, we define
resource request as rrj (t) ∈ {−1, 0, 1}, where rrj (t) = −1
denotes the MD j request spectrum, rrj (t) = 0 denotes the
MD j request energy, and rrj (t) = 1 denotes the MD j request
both spectrum and energy resources.
3) Reward: For each UAVO i ∈ U and MD j ∈ D,
the payoff functions are shown in equations (10) and (11),
respectively. The reward of each UAVO is given as:
Ri = Λ
D∑
j=1
(
βej θ
e
j + βsj θ
s
j
)
−
D∑
j=1
C
(
θej + θsj
)
, (10)
where βej , β
s
j , θ
e
j , and θsj denote energy price, spectrum price,
energy purchased, and spectrum purchased by MD j, respec-
tively. The overhead costs incurred by UAVOs during mainte-
nance, electricity, hardware loss, and operation are denoted by
C. On the other hand, the MDs wish to maximize their utility
by minimizing the cost of resources. The reward of each MD
is calculated as:
Rj = ωj log2
(
1 +
θej
μej
+
θsj
μsj
)
−
(
βej θ
e
j + βsj θ
s
j
)
, (11)
where ωj log2(1 +
θej
μe
j
+
θsj
μs
j
) is the obtainment gain from the
purchased energy and spectrum resources, ωj is a positive
coefficient that is used to convert the obtainment revenues into
monetary reward. μej and μsj are energy and spectrum demand
of MD j, respectively. To attain optimal utility for both lead-
ers and followers, the optimization problems are formulated
as the MLMF Stackelberg game below:
Stage 1: Leaders’ pricing
max
β
Ri (β, θ,Λ)
s.t. β ≥ 0,
D∑
j=1
θej ≤ χ,
D∑
j=1
θsj ≤ χ, (12)
where χ is the total available idle resource of the UAVO.
Stage 2: Followers’ purchasing
max
θ
Rj (θ, μ,Λ)
s.t. θe , θs ,Rj ≥ 0. (13)
This game aims to find the Nash equilibrium (NE) point(s)
where both the leader and the followers benefit the most.
Equations (12) and (13) form the Stackelberg game. We define
the NE as follows:
Definition 1: Let β∗,Λ∗, and θ∗ are optimal price, decision
strategy of UAVOs, and resource demand of MDs, respectively.
The point (β∗,Λ∗, θ∗) is the NE if it satisfies the following
conditions.
Ri (β
∗,Λ∗, θ∗) ≥ Ri (β,Λ
∗, θ∗). (14)
Rj (β
∗,Λ∗, θ∗) ≥ Rj (β
∗,Λ∗, θ). (15)
Since the resource trading in a multi-agent IIoT environment
is complex, finding the optimal decision policy is challeng-
ing. Therefore, we transform the optimization problem into the
stochastic game problem and can be solved by the MADRL
techniques.
A. Nash Equilibrium Analysis
To obtain and demonstrate the existence and uniqueness of
NE in the game, we must calculate the best response strategy
for each trader and derive the derivatives of equations (10)
and (11) with respect to β and θ, respectively.
∂Ri
∂β
=
∂
∂β
⎛
⎝Λ
D∑
j=1
(
βe
j θ
e
j + βs
j θ
s
j
)−
D∑
j=1
C
(
θej + θsj
)
⎞
⎠. (16)
∂Rj
∂θ
=
∂
∂θ
(
ωj log2
(
1 +
θej
μe
j
+
θsj
μs
j
)
− (βe
j θ
e
j + βs
j θ
s
j )
)
.
(17)
The utility functions of UAVOs and MDs are strictly concave
in terms of β and θ, respectively, because the second-order
derivatives functions in equations (16) and (17) are less than
zero. The first and second-order derivative of equations (16)
and (17) are presented in Appendix 1.
V. MADRL FOR RESOURCE TRADING IN IIOT NETWORK
DRL is a promising approach to handle complex
optimization problems in the modern wireless networks for
finding optimal policies that enable agents to make appro-
priate decisions [63]. Without prior knowledge of the system
model, the agents may easily assess and pick the optimal
action among alternative actions. It was initially designed for
a single-agent environment; however, it has recently been used
in a more complex setting where multiple agents compete or
coordinate with each other in a dynamic environment. The
MADRL method focuses on optimizing the reward of all trad-
ing system agents while taking other agents’ behaviors into
account, whereas the single-agent approach can learn a policy
mapping from state to action by interacting with the underlying
environment in order to maximize the cumulative reward. This
paper considers a resource trading environment where multiple
resource providers compete to sell their resources to MDs and
maximize their profit by continually adjusting their pricing
strategies. Multiple MDs, on the other hand, interact with the
environment to make purchasing decisions based on the prices
set by resource providers to reduce costs and improve QoS.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
SEID et al.: BLOCKCHAIN-BASED RESOURCE TRADING IN MULTI-UAV-ASSISTED INDUSTRIAL IoT NETWORKS 173
Fig. 2. MADRL Framework for resource trading in IIoT.
The optimization problems defined in equations (12) and (13)
are complex and continuous action space problems that need
to be formulated as a stochastic game model to handle the
complexity and dynamics in the system.
A. Stackelberg Game-Based MADRL
In this subsection, the complex optimization problems for-
mulated in the MLMF Stackelberg game are modeled as a
stochastic Markov game to handle the agents’ dynamic inter-
actions with the environment that change in response to player
behavior. The formulated stochastic optimization issues are
then solved using dynamic programming and MADRL tech-
niques. The MADRL is used to build a joint strategy for
resource trading. The stochastic Markov game extends MDPs
to the multi-agent case and repeated games to multiple state
cases. In the MADRL approach, the rewards and transitions
in the environment are determined by the actions of all agents
in the system, as shown in Fig. 2. Therefore, the agents must
learn the environment in a joint action space A. However, in
the traditional MDP, finding the best solution is challenging
because each agent has different profit-maximizing objectives.
This can significantly increase the learning complexity of the
agents in the resource trading system. These issues can be
addressed by the formulated stochastic Markov game. The
extended MDP/stochastic game can be defined as the follow-
ing tuples (S ,M ,A,P ,R, γ) to handle the multiple agents
interaction in the environment, where S = {1, 2, 3, . . . ,S}
represents a finite set of states for agent m, ∀m ∈ M =
{1, 2, . . . ,M } is the set of agents/players. The finite set of the
joint actions denoted by A and Am is the action set of agent
m. Further, P defines a state transition probability, R describes
a reward function, and γ is the discount factor γ ∈ [0, 1].
The extended MDP involves multiple agents that continually
observe the current state st of the controlled system and then
take action at among the available actions allowed in that
state. Each agent acts in the environment in accordance with a
specific policy π(s , a), which represents the probabilities that
govern the agent’s decision to perform an action in response
to the current state st of the environment. The agents’ goal is
to maximize their long-term reward r by iteratively changing
their policy in response to the reward Rt they receive from the
environment after taking action at . The agent will then trans-
fer to a new state st+1 and receive a reward rt , all within a
time slot t. Hence, these functions can be expressed as:
P
(
s , s ′, a
)
= P
[
St+1 = s ′|St = s ,At = a
]
,
R
(
s , s ′, a
)
= E[Rt+1|St = s ,At = a],
π(s , a) = P[At = a|St = s ], (18)
where St represents an agent’s state at a learning step t of
an episode, At depicts the action the agent takes at learning
step x, and Rt+1 denotes the reward received by the agent
corresponding to the state-action pair. Therefore, the long-term
reward of agent m is given as:
r tm =
+∞∑
ρ=0
γρRm (t + ρ+ 1). (19)
The values of γ here reflect the effect of future rewards on
optimal decisions: if γ is close to 0, the decision emphasizes
the short-term gain; if γ is close to 1, the decision empha-
sizes the long-term gain in which the future benefits are given
more weight, and the decisions are said to be farsighted. Each
agent m in the formulated stochastic game aims to maximize
its expected payoff over time. Therefore, for agents in a
joint strategy π, each agent m has a strategy πm , and the
optimization objective in equation (19) can be reformulated
as follows:
r tm = E
⎧⎨
⎩
+∞∑
ρ=0
γρr t+ρ+1
m |st = s , π
⎫⎬
⎭, (20)
where r
t+ρ+1
m is the immediate reward received by agent m at
time t + ρ+1, and E{.} represents the expectation operations
in which the expectation is taken over the probabilistic state
transitions under strategy π from state s.
The MDP model of the resource trading in the IIoT system
can be shown as follows. Let βt ∈ Ai and μt ∈ Aj denote
the unit price set by UAVO and the resource demand action of
MD, respectively, where Ai indicates the action space of the
UAVO/resource provider and Aj represents the action space
of MD. The state s ∈ S is a tuple of environment features
relevant to the problem at hand, and it defines the agent’s
relationship with its environment. At every time step t, the
agent observes the state of its environment st ∈ S and then
takes action at ∈ A in accordance with a policy π. The policy
π(s , a) serves as the basis for individuals to determine their
probability of taking action on the current state s. The policy
function must satisfy
∑
a∈A π(s , a) = 1. The environment of
the agent transitions from the current state st to the next state
st+1 once the agent takes action at , and the agent then receives
a reward rt+1 which reveals the benefit of doing action at at
state st . This strategy forms an experience e at time t + 1,
which describes an interaction of agents with the environment
and is expressed as et+1 = (st , at , rt+1, st+1) [64].
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
174 IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, VOL. 20, NO. 1, MARCH 2023
State space: All agents in the system observe the state
of the environment to gain experience and make the
optimal decision. The state space is given as: sm,t =
{βm,t−1, μm,t , χm,t , ψm,t} ∈ S , where βm,t−1 =
{βem,t−1, β
s
m,t−1} denoting prior unit pricing of energy
and spectrum supplied by other agents, while μm,t =
{μem,t , μ
s
m,t} denotes the energy and spectrum resource
demand of MDs. Moreover, χm,t = {χem,t , χ
s
m,t} denotes
the aggregated available energy and spectrum resource, and
ψm,t ∈ [−1, 0, 1] denotes the type of resource requested by
the MDs. Here, ψm,t = −1 indicates that MD requests energy,
ψm,t = 0 implies that MD requests spectrum, and ψm,t = 1
means that the MD wants both energy and spectrum.
Action space: The agent acts in accordance with a pol-
icy π, which is a mapping from the state space S to
action space A, expressed as π : st ∈ S → at ∈ A.
The action can be from UAVOs (leaders) and MDs (fol-
lowers). The action space contains a vector of transmitted
energy θe = [θe1 , θ
e
2 , . . . , θ
e
N ], a vector of allocated spec-
trum θs = [θs1, θ
s
2, . . . , θ
s
N ], and a vector of decision strategy
Λ = [Λ
e,s
1 ,Λ
e,s
2 , . . . ,Λ
e,s
N ]. The action space is given as
a(t) = [Λe,s
1 , θe1 , θ
s
1,Λ
e,s
2 , θe2 , θ
s
2, . . . ,Λ
e,s
N , θeN , θ
s
N ]. The pol-
icy π can be determined using the space-action function called
Q-function Q(st , at ), which can be approximated by deep
learning [65].
Reward function: The agents consider to maximize the long
term reward rm(t) by selecting optimal decision from the
available actions at the time slot t. The reward of the fol-
lowers is rj (t) = Rj (11) and the reward of UAVOs is given
by ri (t) = Ri (10). The system reward is given by:
r(t) =
N∑
j=1
U∑
i=1
(
rj (t) + ri (t)
)
. (21)
The players in the formulated stochastic game have individual
expected rewards based on the joint strategy rather than the
players’ individual strategies. In this non-cooperative game,
players learn their best strategies through repeated interactions
with the stochastic environment, and NE points are obtained
where the optimal expected reward can be realized.
Definition 2: NE is a collection of strategies, one for each
player in the environment, so that each individual strategy is
the best response to the others, where π∗ = {π∗1 , . . . , π∗M }.
For each agent m, the strategy π∗m is expressed as:
rm(π∗m , π−m ) ≥ rm
(
π′m , π−m
)
, ∀π′m , (22)
where π′m ∈ [0, 1] represents all possible strategies taken by
agent m.
B. Stackelberg-Based MADDPG
For our resource trading optimization problem, we adopt
the MADDPG, which extends a DDPG-based actors-critics
algorithm to handle multi-agent scenarios in a non-stationary
environment and performs better in a multi-agent environment.
It contains M agents with a set of deterministic policies for all
agents. Agents can learn a policy function and a value function
simultaneously using actor-critic approaches. The actor-critic
has two parts: a policy model called actor and a value function
named critic. The policy function acts as an actor, whereas
the value function acts as a critic [66]. The output of the
actor network is action value whereas the output of the critic
network is Q-value. The Q-value represents an approximation
of the value of the action selected by the actor network.
1) Actor Network: The actor network is a function that
maps the resource trading environment state st to an action
at in order to find the best price policy. Agents choose their
actions based on the parameters and the trade state in each
decision time slot t, which is expressed as:
atm = πm
(
S t
m |Φπ
m
)
. (23)
2) Critic Network: The critic network is primarily used
to evaluate the value of the actions that have been cho-
sen by the agents. All states (s1, s2, . . . , sM ) and agents’
actions (a1, a2, . . . , aM ) are inputs for critic network. The
critic network calculates the temporal difference (TD)-errors,
ε as:
εm =
1
Γ
∑
ι
(
yιm −Qm
(
sι1, . . . , s
ι
M , aιm |ΦQ
m
))
, (24)
where yιm = r tm + γQ ′
m (sι+1
1 , . . . , sι+1
M , πm (sι+1
m |ΦQ ′
m )), γ
is the discount factor of the long term cumulative reward with
the range of [0, 1], Γ denotes the size of the minibatch, and
ΦQ
m denotes the parameters of the critic network. The TD-
errors can be taken as the evaluation results and use them to
update the critic network. The actor network is updated by the
policy gradient as:
∇Φπ
mJ = ∇Φπ
m log πm(sm , am )εm . (25)
Let π is the set of all agents’ deterministic policies, which
is denoted as π = {π1, π2, . . . , πM } and parameterized by
Φ = {Φ1,Φ2, . . . ,ΦM }. The gradient of the expected reward
for agent m, JΦm
= E[Rm ] expressed as:
∇Φm
JΦm
= Es,a∼B[∇Φm
log πm (om )∇amQπ
m
× (κ, a1, . . . , aM )|am = πm (om )], (26)
where Qπ
m (κ, a1, . . . , aM ) is a centralized action-value func-
tion that takes all agents’ actions, a1, . . . , aM , and state
information κ as input and outputs the Q-value for agent
m. The observations of all agents κ is expressed as κ =
{o1, . . . , oM }. The experience replay buffer is denoted by B,
which is made up of tuples (κ, κ′, a1, . . . , aM , r1, . . . , rM ),
where κ′ is the next state from κ after taking action
a1, . . . , aM . Furthermore, the centralized action-value func-
tion Qπ
m is updated by minimizing the loss function, which is
denoted as:
XΦm
= Eκ,a,r ,κ′
[
(Qπ
m(κ, a1, . . . , aM )− y)2
]
,
y = rm + γQπ′
m
(
κ′, a ′1, . . . , a ′M
)|a ′m = π′m(om ), (27)
where π′ = {πΦ′
1
, . . . , πΦ′
M
} is the set of target policies with
delayed parameters Φ′
m .
1) Stackelberg-Based Actor-Critic Update: Let Fj ⊆ M
represent the set of followers, and Li ⊆ M represents the
set of leaders for agent m. The actions of the leaders, ai =
πi (oi ), ∀i ∈ L(m), are computed using private observation oi
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
SEID et al.: BLOCKCHAIN-BASED RESOURCE TRADING IN MULTI-UAV-ASSISTED INDUSTRIAL IoT NETWORKS 175
and are unlikely to be affected by agent m’s action because
the leaders have already committed to their actions. The set
of leader actions is denoted by aL = {ai1 , . . . , aiZ }, where
Z = |L(m)| < M . The follower responses, aj = πj (oj , am ),
∀j ∈ F (m), are computed using personal observations oj and
agent m ′s action am . Each follower j ∈ F (m) may have
other leaders actions aL, where am in aL are part of the input
to πj , but these actions are omitted to minimize notation and
considered to be part of oj . The set of follower actions aF =
{aj1 , . . . , ajG}, where G ∈ |F (m)| < M .
The objective of Stackelberg-based learning is to maximize
the gradient of the expected reward JΦm
, maxΦm
JΦm
, where
JΦm
= Es∼σπ [Rm ]
= Eκ0∼σ
⎡
⎢⎢⎢⎣
Qπ
S ,m
(
κ0, a0L, a
0
m , a
0
F
)|
a0m = πm
(
o0m
)
,
a0L = πi
(
o0i
)∀i ∈ L(m),
a0F = πj
(
o0j , a
0
m
)
∀j ∈ F (m)
⎤
⎥⎥⎥⎦. (28)
Here σπ represents the discounted state observation distribu-
tion of deterministic policy πm , which is parametrized by
Φm and the follower responses a0F at time slot 0 are con-
fined with a0m . The updated centralized action-value function
Qπ
S ,m (κ, aL, am , aF ) in (28) can be re-structured as:
Qπ
S ,m (κ, aL, am , aF ) = rm (κ, aL, am , aF )
+ γEκ′
⎡
⎢⎢⎢⎣
Qπ
S ,m
(
κ′, a ′L, a
′
m , a
′
F
)|
a ′m = πm
(
o′m
)
,
a ′L = πi
(
o′i
)∀i ∈ L(m),
a ′F = πj
(
o′j , a ′m
)
∀j ∈ F (m)
⎤
⎥⎥⎥⎦, (29)
where the Q-function allows both leaders and followers to rely
on the current global state κ and actions a to determine their
conditions. The Q-value produced from this function repre-
sents the current reward ri , as well as the discounted future
return starting from state κ′, and discounted by γ.
In an IIoT environment with multiple hierarchies, an agent
m can have multiple leaders with actions aL and multiple
followers with actions aF . Each action has an effect on the
gradient ∇Φm
J (Φm ) used to update the deterministic policy
πm of agent m. We consider two agent systems to estimate
the impacts of each agent, with A1,A2 representing leaders
and followers actions, respectively. The Stackelberg objective
for the A1 (leaders) is as follows:
J (Φ1) = Eκ∼σ
⎡
⎣
Qπ
S ,1(κ, a1, a2)|
a1 = π1(o1),
a2 = π2(o2, a1)
⎤
⎦. (30)
In addition, the gradient ∇Φ1
J (Φ1) for A1, considering the
impacts of A2’s response is given as:
∇Φ1
J (Φ1) = Eκ,a∼B
⎡
⎢⎢⎣
∇Φ1
π1(o1)∇a1Q
π
S ,1(κ, a1, a2)
+∇Φ1
π1(o1)∇a2Q
π
S ,1(κ, a1, a2)
∇a1π2(o2, a1)|a1 = π1(o1),
a2 = π2(o2, a1)
⎤
⎥⎥⎦,
(31)
where ∇Φ1
π1(o1)∇a2Q
π
S ,1(κ, a1, a2) denotes the impact of
follower A2 to the gradient ∇Φ1
J (Φ1), which is a result
of the chain rule, and ∇Φ1
π1(o1) denotes the updates to
Φ1 that modify the output policy π1 in order to maximize
the expected reward. Additionally, since the follower reaction
depends on the leaders’ actions, modifying the output policy
π1 impacts the follower response. Furthermore,∇a1π2(o2, a1)
denotes the impact of changes in a1 on the follower response,
and ∇a1Q
π
S ,1(κ, a1, a2) shows how changes to the follower
response affect the modified action-value Qπ
S ,1.
The Q-function updates similarly to the MADDPG update,
but with the additional constraint of leaders committing to their
actions before followers, and followers observe and respond to
this commitment. The Q-function is updated to accommodate
these constraints by minimizing the TD-error.
X (v1) = Eκ,a,r ,κ′∼B
[(
Qπ
S ,1(κ, a1, . . . , aM )− y
)2]
,
y = rm + γQπ′
S ,m
(
κ′, a ′L, a
′
m , a
′
F
)
a ′m = π′m (om ),
a ′L = π′i (oi ), ∀i ∈ L(m)
a ′F = π′j (oj , a ′m ), ∀j ∈ F (m), (32)
where π′ = {πΦ′
1
, . . . , πΦ′
M
} is the set of target policies with
delayed parameters Φ′
m and Qπ′
S = {Qπ′
S ,1, . . . ,Q
π′
S ,M } is the
set of target critics with delayed parameters v ′. Lastly, the
target network of agent m can update by soft update as:
Φπ′
m ← Φπ
m + (1− τ)Φπ
m
′
Φ
Q
m ′ ← ΦQ
m + (1− τ)ΦQ
m
′. (33)
Algorithm 2 provides a full description of the learning pro-
cess in the SMADDPG. First, initialize the available resources
of sellers and buyers, the sellers’ resources (i.e., bandwidth and
energy), the actor and critic network parameters with random
weight Φπ
m , and the replay memory buffer Bm (line 1). Each
agent observes the state of the environment and performs the
action, receives the reward, and creates a new state (lines 3-7).
In the training phase (lines 9-15), we calculate the reward and
store the experience in the replay memory buffer; we use pol-
icy training, which involves mini-batch sampling from a replay
memory buffer. An actor and critic networks are then updated
based on a randomly selected sample.
2) Computational Complexity Analysis: We investigate the
computational complexity of our proposed SMADDRL algo-
rithm using Big O notation. Its computational complexity is of
order O(M.T.W), where M is the total number of agents, T is
the number of episodes and W is the learning steps. Therefore,
increasing the heterogeneity and number of agents has no
significant effect on each agent’s computational complexity.
VI. PERFORMANCE EVALUATION
This section presents the performance evaluation of our
proposed DRL-based resource trading scheme (SMADDPG)
with the benchmark algorithms (DDPG [67], DQN [34], and
MADDPG [68]) in terms of various metrics such as transaction
processing delay, profit, cost, and convergence rate.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
176 IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, VOL. 20, NO. 1, MARCH 2023
Algorithm 2 SMADDPG Algorithm for Resource Trading
1: Initialize: The actor network πm (sm |Φπ
m ) with weights
Φπ
m ; the critic network Qm (s1, . . . , sM , am |ΦQ
m ) with
weights Φ
Q
m ;
2: Initialize: Actor and critic target networks with weights
Φπ′ ← Φπ and ΦQ ′ ← ΦQ
3: Initialize: The replay buffer Bm
4: for each episode = 1 to V do
5: Configure simulation environment
6: Receive initial state S0
7: for t = 1 to T do
8: Each agent observes the trading state stm
9: Execute Algorithm 1
10: For agent m, selects action am = πm(sm |Φπ
m )
11: Execute atm
12: Observe next state st+1
m
13: for each agent m = 1 to M do
14: Calculate the reward r tm for m
15: Store (stm , a
t
m , r
t
m , s
t+1
m ) in Bm
16: Sample a random mini-batch of Γ transactions
(stm , a
t
m , r
t
m , s
t+1
m ) in Bm
17: Set y tm = g tm + γQ ′
m (st , . . . , stm , a
t
m |ΦQ
m )
18: Update the critic network Qm by minimizing the
loss as in (27)
19: Each agent updates the actor network as in (25)
20: end for
21: Update target network parameters of each agent m
by (33)
22: end for
23: end for
A. Simulation Environment
In this paper, we set up a multi-UAV-assisted IIoT network
simulation environment based on the configuration and param-
eters utilized in [68], [69]. We consider a group of multi-UAV
connected in D2D link called UAV cluster deployed in a small
cell area with radius ru = 800m. The MDs are randomly and
uniformly distributed in small cells, where UAVs fly at a fixed
altitude of 100m. The sub-channel bandwidth is set to 80KHz,
and the QoS threshold (SINRmin ) is 3.5dB. The simulations
are run in a Python 3.6 environment on a machine with a Core
i7 processor running at 2.4GHz and 16GB memory. In our
simulation, we set the maximum number of training episodes
as 3000 and the maximum episode step as 25 for both our
algorithm and baseline algorithms. The path-loss exponent is
set to -90 dBm and the sub-channel bandwidth is 80 kHz. In
this proposed multi-agent strategy, we use a fully connected
neural network (NN) with critic-network and actor-network.
In both the actor and critic NNs, we deploy two hidden layers
for each agent, with the first hidden layer set to 256 and the
second hidden layer set to 128. We use Rectified Linear Unit
(ReLU) as an activation function for the hidden layers, and
the sigmoid function is employed at the output layer. We used
a replay buffer size of 107 and a mini-batch size of 256. We
set the probability of agent action selection δ ∈ (0, 1). We use
Adam optimizer function, which measures the degree to which
TABLE II
SIMULATION PARAMETERS
Fig. 3. System reward (δ = 0.5).
newly acquired information overrides old information, result-
ing in a faster learning rate. The learning rate for an actor is set
to 0.0001 and for a critic is 0.001. The discount factor is set
as 0.95, which determines the importance of future rewards.
B. Convergence Analysis
We first evaluate the convergence of our algorithm with
respect to various contexts. Fig. 3 compares the convergence
on the average system rewards of SMADDPG to baseline
schemes in terms of the agent’s action selection probability
and training step. The performance of all algorithms is initially
unsatisfactory due to random action selection during the explo-
ration phase, but after gaining some experience by interacting
with the wireless environment, SMADDPG has a higher learn-
ing performance than the other algorithms. The agents in the
SMADDPG algorithm learn and update their policies more
quickly, early converge than the baseline algorithms and after
about 400 episodes, they begin to converge. The proposed
SMADDPG and MADDPG algorithms have more cooperative
and distributive sharing policies and experiences than DDPG
and DQN algorithms. Generally, this figure shows that when
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
SEID et al.: BLOCKCHAIN-BASED RESOURCE TRADING IN MULTI-UAV-ASSISTED INDUSTRIAL IoT NETWORKS 177
Fig. 4. The effect of increasing buyers and sellers with on cost of buyers.
the reward value is high, the leaders and followers achieve
higher utilities with optimal prices and minimum costs.
C. Performance Analysis
We compare the performance of the proposed algorithm
with the benchmark algorithms in terms of buyer cost as
the number of buyers increases. As shown in Fig. 4(a), the
costs of buyers increase as the number of buyers increase
in the system. The increasing number of buyers indicates
the number of MDs that require more resources to achieve
their QoS and perform given tasks within the time frame.
Due to this, the sellers/UAVOs also change the resource
prices dynamically depending on the number of buyers. Then,
the overall costs from the resource seller’s perspective can
increase as the number of buyers increases parallelly. The
proposed algorithm increases the cost more slowly than bench-
mark algorithms. The results show that SMADDPG scored
16.38%, 17.81%, and 19.31% lower than MADDPG, DDPG,
and DQN, respectively. Therefore, the proposed SMADDPG
algorithm achieved better performance than benchmark algo-
rithms, i.e., it has optimal prices and minimum computation
costs. Furthermore, we assessed the cost of buyers in rela-
tion to the increase in the number of resource providers. The
simulation results in Fig. 4(b) show that the cost of buy-
ers decreases slightly as the number of resource providers
increases because resource providers compete with each other
to sell more resources by setting an optimal price. When the
number of resource providers/sellers in the system increases,
the proposed SMADDPG algorithm reduces buyers’ costs by
21.66%, 25.15%, and 28.13% compared to MADDPG, DDPG,
and DQN, respectively.
We also compare the utility of resource providers as the
number of sellers and buyers increases. According to the
results shown in Fig. 5(a), the utility/revenue of resource
providers increases as the number of buyers increases. The
utility of resource provider increase as the number of buyers
increase in SMADDPG by 28.62%, 46.36%, and 92.41% com-
pared to MADDPG, DDPG, and DQN, respectively. Therefore,
the proposed SMADDPG algorithm obtained better profits
when the number of resource buyers increased than other
benchmark algorithms.
On the other hand, Fig. 5(b) shows that the profit of the
sellers decrease when the number of sellers increases. It
shows that the sellers compete with each other to provide
resources at optimal prices. The sellers’ prices decline slowly
Fig. 5. The effect of increasing buyers and sellers number on the profit of
UAVOs.
Fig. 6. System cost with respect to number of MDs.
when the number of sellers increases. Due to this, sellers’
profit decreases as the number of sellers increases. Hence,
the proposed SMADDPG algorithm outperforms MADDPG,
DDPG, and DQN in terms of profit, with 19.32%, 45.35%,
and 147.36%, respectively.
In our simulation, the overall system cost is analyzed by
considering the number of MDs requesting energy and spec-
trum resources, consensus nodes, and resource sellers in the
proposed scenario. As shown in Fig. 6, the overall cost of
the system rises as the number of resources requesting MDs
increases, and other blockchain entities increases. Compared
with MADDPG, DDPG, and DQN, the SMADDPG reduces
system cost by 53.09%, 67.34%, and 75.71%, respectively.
D. Network Performance Analysis
To show the coupling of blockchain and the IIoT network,
we investigate network performance in terms of average
blockchain transaction throughput and latency with varying
UAVOs (sellers) and MDs (connected devices, such as buyers
and other devices interacting each other via UAV networks).
As illustrated in Fig. 7(a), the average throughput of the
network is evaluated with 5 UAVOs serving as resource sellers
and increasing the number of MDs connected to the systems
from 20-70. The figure shows that the average throughput
increases with the increasing number of MDs connected to
the network. This is because when the number of the MD
increase their service demand also increases. Compared to
other schemes, SMADDPG has the highest throughput, with
15.13%, 32.43%, and 61.5% for MADDPG, DDPG, and DQN,
respectively. The average transaction processing latency also
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
178 IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, VOL. 20, NO. 1, MARCH 2023
Fig. 7. Blockchain performance analysis in terms of UAVOs and MDs.
TABLE III
PERFORMANCE COMPARISON OF THE PROPOSED SCHEME
WITH STATE-OF-THE-ART APPROACHES
increases as the number of MDs in the network increases. This
is because when the number of transaction request increases,
the consensus time required also increase. However, accord-
ing to Fig. 7(b), the average transaction processing latency
of SMADDPG is 24.85%, 47.89%, and 56.01% lower than
that of MADDPG, DDPG, and DQN, respectively. This is
because our proposed algorithm allows both UAVOs and
MDs to reach at optimal strategy in shorter time than other
approaches. Moreover, we analyze the throughput and latency
of the blockchain to measure the performance of the proposed
scheme with increased number of UAVOs to evaluate the
impact of the increasing number of sellers in the blockchain
performance. As shown in Fig. 7(c), the throughput slightly
increases with the increasing number of the sellers (UAVOs)
and MDs (buyers). This is because the increase of both sellers
and buyers can allow demand and supply matching. However,
SMADDPG achieves a better performance than the benchmark
schemes and makes it easier for buyers and sellers to reach
optimal policies. SMADDPG allows buyers and sellers to
reach NE faster, significantly increasing the system’s through-
put. From the simulation results, we observed that SMADDPG
achieves a throughput of 13.87%, 29.55%, and 53.03%, higher
than MADDPG, DDPG, and DQN, respectively. Similarly,
the latency increases with an increasing number of UAVOs
and MDs, as shown in Fig. 7(d). But the SMADDPG scheme
reduced the latency by 22.5%, 44.38%, and 52.27% over the
MADDPG, DDPG, and DQN schemes, respectively.
We compared the performance of our proposed scheme
with other state-of-the-art schemes in Table III. We assess the
performance of the proposed scheme based on the increas-
ing number of UAVs, MDs, and consensus nodes in the
blockchain-enabled IIoT network. The SMADDPG has a
lower latency as it uses the actors-critics algorithm to han-
dle multi-agent settings and dynamic environments where the
agents’ actions vary over time. With an increasing number of
MDs and agents in the system, transactions/resource requests
also increase. Further, the designed dynamic pricing algorithm
promotes efficient trading among agents, increasing overall
system performance, including throughput, latency, etc. This
reduces system costs and improves the utility of traders (both
UAVs and MDs).
VII. CONCLUSION
In this paper, we proposed a novel resource trading frame-
work that integrates MADRL with blockchain and game theory
to achieve secure and efficient resource sharing between var-
ious types of MDs in the UAV-assisted IIoT networks. A
consortium blockchain with SC is deployed to ensure the secu-
rity and privacy of the resource trading system. Furthermore,
the optimization problems are modeled using the MLMF
Stackelberg game. We converted the formulated optimization
problems into a stochastic game and solved them using the
proposed SMADDPG algorithm to deal with the complexity
and dynamics of the IIoT network. According to the simulation
results, our proposed scheme outperforms others in terms of
improving the efficiency of resource trading in UAV-assisted
IIoT networks. In the future, we will investigate slicing and
virtualization for intelligent and efficient resource trading in
UAV-assisted IIoT networks.
APPENDIX
THE NE ANALYSIS USING FIRST AND SECOND-ORDER
DERIVATIVES
To analyzes the NE, we use a Hessian of Ri and Rj as
follows:
Ri =
⎛
⎝Λ
D∑
j=1
(
βej θ
e
j + βsj θ
s
j
)
−
D∑
j=1
C
(
θej + θsj
)⎞⎠
So,
∂Ri
∂β
=
∂
∂β
⎛
⎝Λ
D∑
j=1
(
βej θ
e
j + βsj θ
s
j
)
−
D∑
j=1
C
(
θej + θsj
)⎞⎠
=
[
∂Ri
∂β1
,
∂Ri
∂β2
, . . . ,
∂Ri
∂βD
]
Let Δj =
∂Ri
∂β
,where, j = 1, 2, . . . ,D
⇒ Δj = Λ
[
eβe−1
j θej + sβs−1
j θsj
]
⇒ ∂Ri
∂β
= [Δ1,Δ2, . . . ,ΔD ]
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
SEID et al.: BLOCKCHAIN-BASED RESOURCE TRADING IN MULTI-UAV-ASSISTED INDUSTRIAL IoT NETWORKS 179
Then, Ri is calculated as follows using Hessian matrix
Ri =
⎡
⎢⎢⎣
∂2Ri
∂β2
1
. . . ∂2Ri
∂β1∂βd
. . . . . . . . .
∂2Ri
∂βdβ1
. . . ∂2Ri
∂β2
d
⎤
⎥⎥⎦
∂Ri
∂θ
=
⎡
⎢⎣
∂Δ1
∂β1
. . . ∂Δd
∂β1
. . . . . . . . .
∂Δ1
∂βd
. . . ∂Δd
∂βd
⎤
⎥⎦
If rj =
∂2Ri
∂β2 = Λ(e(e − 1)θej β
e−2
j + s(s − 1)θsj β
s−2
j , where
j = 1, 2, . . . ,D .
H (Ri ) =
⎡
⎢⎢⎣
r1 0 . . . 0
0 r2 . . . 0
. . . . . . . . . . . .
0 0 0 rD
⎤
⎥⎥⎦
Therefore, rj < 0, then H (Ri ) is strictly concave.
Rj =
(
ωj log2
(
1 +
θej
μe
j
+
θsj
μs
j
)
− (βe
j θ
e
j + βs
j θ
s
j
)
)
∂Rj
∂θ
=
[
∂Rj
∂θ1
, . . . ,
∂Rj
∂θD
]
Let αj =
∂Rj
∂θj
, j = 1, . . . ,D
αj =
∂Rj
∂θj
= ωj
∂
∂θj
(
log2
(
1 +
θej
μe
j
+
θsj
μs
j
)
− C
(
Be
j θ
e
j + βs
j θ
s
j
)
)
= ωj
∂
∂θj
(
ln
(
1 +
θej
μe
j
+
θsj
μs
j
)
− C
(
βj
e0j
e + θsj θ
s
j
)
)
=
ωj
ln2
⎛
⎜⎜⎝
eθe−1
j
μe
j
+
sθs−1
j
μs
j(
1 +
θej
μe
j
+
θsj
μs
j
)
⎞
⎟⎟⎠− C
(
eβe
j θ
e−1
j + sβs
j θ
s−1
j
)
Then,
∂Rj
∂θ
=
⎡
⎢⎢⎣
α1 0 . . . 0
0 α2 . . . 0
. . . . . . . . . . . .
0 0 0 αD
⎤
⎥⎥⎦
Let ζj =
∂2Rj
∂θ2
=
∂αj
∂θj
=
ωj
lnj
(a∗b−c∗d
b2
)− q where
a = e(e − 1)
θe−2
e
μej
+ s(s − 1)
θs−2
s
μsj
b = 1 +
θej
μej
+
θsj
μsj
c = d =
eθe−1
j
μej
+
sθsj
μsj
q = C
(
e(e − 1)βej θ
e
j + s(s − 1)βsj θ
s−2
j
)
H (Rj ) =
⎡
⎢⎢⎣
ζ1 0 . . . 0
0 ζ2 . . . 0
. . . . . . . . . . . .
0 0 0 ζD
⎤
⎥⎥⎦ (34)
Then ζj < 0, H (Rj ) also strictly concave function.
REFERENCES
[1] D. O’Halloran and E. Kvochko, “Industrial Internet of Things:
Unleashing the potential of connected products and services, collab-
oration with Accenture,” Cologny, Switzerland, World Econ. Forum,
White Paper, 2015, p. 34. [Online]. Available: http://reports.weforum.
org/industrial-internet-of-things
[2] Z. Shi, X. Xie, H. Lu, H. Yang, M. Kadoch, and M. Cheriet,
“Deep-reinforcement-learning-based spectrum resource management for
Industrial Internet of Things,” IEEE Internet Things J., vol. 8, no. 5,
pp. 3476–3489, Mar. 2021.
[3] L. D. Xu, W. He, and S. Li, “Internet of Things in industries: A survey,”
IEEE Trans. Ind. Informat., vol. 10, no. 4, pp. 2233–2243, Nov. 2014.
[4] S. Iqbal, R. M. Noor, A. W. Malik, and A. U. Rahman, “Blockchain-
enabled adaptive learning-based resource sharing framework for IIoT
environment,” IEEE Internet Things J., vol. 8, no. 19, pp. 14746–14755,
Oct. 2021.
[5] S. Messaoud, A. Bradai, O. B. Ahmed, P. T. A. Quang, M. Atri,
and M. S. Hossain, “Deep federated Q-learning-based network slic-
ing for industrial IoT,” IEEE Trans. Ind. Informat., vol. 17, no. 8,
pp. 5572–5582, Aug. 2021.
[6] Y. Miao, Q. Tong, K.-K. R. Choo, X. Liu, R. H. Deng, and
H. Li, “Secure online/offline data sharing framework for cloud-assisted
Industrial Internet of Things,” IEEE Internet Things J., vol. 6, no. 5,
pp. 8681–8691, Oct. 2019.
[7] C. Paniagua and J. Delsing, “Industrial frameworks for Internet of
Things: A survey,” IEEE Syst. J., vol. 15, no. 1, pp. 1149–1159,
Mar. 2021.
[8] K. Tange, M. De Donno, X. Fafoutis, and N. Dragoni, “A systematic
survey of Industrial Internet of Things security: Requirements and fog
computing opportunities,” IEEE Commun. Surveys Tuts., vol. 22, no. 4,
pp. 2489–2520, 4th Quart., 2020.
[9] W. Sun, J. Liu, Y. Yue, and Y. Jiang, “Social-aware incentive mecha-
nisms for D2D resource sharing in IIoT,” IEEE Trans. Ind. Informat.,
vol. 16, no. 8, pp. 5517–5526, Aug. 2020.
[10] W. Mao, Z. Zhao, Z. Chang, G. Min, and W. Gao, “Energy efficient
Industrial Internet of Things: Overview and open issues,” IEEE Trans.
Ind. Informat., vol. 17, no. 11, pp. 7225–7237, Nov. 2021.
[11] C. Qiu, F. R. Yu, H. Yao, C. Jiang, F. Xu, and C. Zhao, “Blockchain-
based software-defined Industrial Internet of Things: A dueling deep
Q -learning approach,” IEEE Internet Things J., vol. 6, no. 3,
pp. 4627–4639, Jun. 2019.
[12] J. Wan et al., “Toward dynamic resources management for IoT-based
manufacturing,” IEEE Commun. Mag., vol. 56, no. 2, pp. 52–59,
Feb. 2018.
[13] L. Yang, M. Li, Y. Zhang, P. Si, Z. Wang, and R. Yang, “Resource man-
agement for energy-efficient and blockchain-enabled industrial IoT: A
DRL approach,” in Proc. IEEE 6th Int. Conf. Comput. Commun. (ICCC),
2020, pp. 910–915.
[14] B. Yang, X. Cao, X. Li, Q. Zhang, and L. Qian, “Mobile-edge-
computing-based hierarchical machine learning tasks distribution for
IIoT,” IEEE Internet Things J., vol. 7, no. 3, pp. 2169–2180, Mar. 2020.
[15] Z. Xiong, Y. Zhang, N. C. Luong, D. Niyato, P. Wang, and N. Guizani,
“The best of both worlds: A general architecture for data management
in blockchain-enabled Internet-of-Things,” IEEE Netw., vol. 34, no. 1,
pp. 166–173, Jan./Feb. 2020.
[16] M. B. Mollah et al., “Blockchain for the Internet of Vehicles towards
intelligent transportation systems: A survey,” IEEE Internet Things J.,
vol. 8, no. 6, pp. 4157–4185, Mar. 2021.
[17] A. H. Khan et al., “Blockchain and 6G: The future of secure and
ubiquitous communication,” IEEE Wireless Commun., vol. 29, no. 1,
pp. 194–201, Feb. 2022.
[18] L. Tang and H. Hu, “Computation offloading and resource allocation
for the Internet of Things in energy-constrained MEC-enabled HetNets,”
IEEE Access, vol. 8, pp. 47509–47521, 2020.
[19] Z. Jia, Q. Wu, C. Dong, C. Yuen, and Z. Han, “Hierarchical
aerial computing for Internet of Things via cooperation of HAPs
and UAVs,” IEEE Internet Things J., early access, Feb. 16, 2022,
doi: 10.1109/JIOT.2022.3151639
[20] S. Jeong, O. Simeone, and J. Kang, “Mobile edge computing via a UAV-
mounted cloudlet: Optimization of bit allocation and path planning,”
IEEE Trans. Veh. Technol., vol. 67, no. 3, pp. 2049–2063, Mar. 2018.
[21] W. Feng, J. Wang, Y. Chen, X. Wang, N. Ge, and J. Lu, “UAV-
aided MIMO communications for 5G Internet of Things,” IEEE Internet
Things J., vol. 6, no. 2, pp. 1731–1740, Apr. 2019.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
http://dx.doi.org/10.1109/JIOT.2022.3151639
180 IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, VOL. 20, NO. 1, MARCH 2023
[22] H. Ke, H. Wang, W. Sun, and H. Sun, “Adaptive computation offload-
ing policy for multi-access edge computing in heterogeneous wire-
less networks,” IEEE Trans. Netw. Service Manag., vol. 19, no. 1,
pp. 289–305, Mar. 2022.
[23] N. H. Motlagh, T. Taleb, and O. Arouk, “Low-altitude unmanned aerial
vehicles-based Internet of Things services: Comprehensive survey and
future perspectives,” IEEE Internet Things J., vol. 3, no. 6, pp. 899–922,
Dec. 2016.
[24] Z. Zhao et al., “Predictive UAV base station deployment and service
offloading with distributed edge learning,” IEEE Trans. Netw. Service
Manag., vol. 18, no. 4, pp. 3955–3972, Dec. 2021.
[25] M. Mozaffari, W. Saad, M. Bennis, Y.-H. Nam, and M. Debbah, “A tuto-
rial on UAVs for wireless networks: Applications, challenges, and open
problems,” IEEE Commun. Surveys Tuts., vol. 21, no. 3, pp. 2334–2360,
3rd Quart., 2019.
[26] T. Yuan, C. E. Rothenberg, K. Obraczka, C. Barakat, and T. Turletti,
“Harnessing UAVs for fair 5G bandwidth allocation in vehicular com-
munication via deep reinforcement learning,” IEEE Trans. Netw. Service
Manag., vol. 18, no. 4, pp. 4063–4074, Dec. 2021.
[27] S. H. Alsamhi et al., “Green Internet of Things using UAVs in B5G
networks: A review of applications and strategies,” Ad Hoc Netw.,
vol. 117, Jun. 2021, Art. no. 102505.
[28] W. Zhang et al., “Deep reinforcement learning based resource manage-
ment for DNN inference in IIoT,” in Proc. IEEE Global Commun. Conf.,
2020, pp. 1–6.
[29] Y. Chen, Z. Liu, Y. Zhang, Y. Wu, X. Chen, and L. Zhao, “Deep
reinforcement learning-based dynamic resource management for mobile
edge computing in Industrial Internet of Things,” IEEE Trans. Ind.
Informat., vol. 17, no. 7, pp. 4925–4934, Jul. 2021.
[30] H. Zhou, C. She, Y. Deng, M. Dohler, and A. Nallanathan,
“Machine learning for massive Industrial Internet of Things,” 2021,
arXiv:2103.08308.
[31] A. Mohammed, H. Nahom, A. Tewodros, Y. Habtamu, and G. Hayelom,
“Deep reinforcement learning for computation offloading and resource
allocation in blockchain-based multi-UAV-enabled mobile edge comput-
ing,” in Proc. 17th Int. Comput. Conf. Wavelet Act. Media Technol. Inf.
Process. (ICCWAMTIP), 2020, pp. 295–299.
[32] P. Yu et al., “Intelligent-driven green resource allocation for Industrial
Internet of Things in 5G heterogeneous networks,” IEEE Trans. Ind.
Informat., vol. 18, no. 1, pp. 520–530, Jan. 2022.
[33] T. Qiu, J. Chi, X. Zhou, Z. Ning, M. Atiquzzaman, and D. O. Wu,
“Edge computing in Industrial Internet of Things: Architecture, advances
and challenges,” IEEE Commun. Surveys Tuts., vol. 22, no. 4,
pp. 2462–2488, 4th Quart., 2020.
[34] A. M. Seid, G. O. Boateng, S. Anokye, T. Kwantwi, G. Sun, and G. Liu,
“Collaborative computation offloading and resource allocation in multi-
UAV assisted IoT networks: A deep reinforcement learning approach,”
IEEE Internet Things J., vol. 8, no. 15, pp. 12203–12218, Aug. 2021.
[35] H. Yao, T. Mai, J. Wang, Z. Ji, C. Jiang, and Y. Qian, “Resource trading
in blockchain-based Industrial Internet of Things,” IEEE Trans. Ind.
Informat., vol. 15, no. 6, pp. 3602–3609, Jun. 2019.
[36] Y. Xu, Z. Liu, C. Huang, and C. Yuen, “Robust resource allocation
algorithm for energy-harvesting-based D2D communication underlay-
ing UAV-assisted networks,” IEEE Internet Things J., vol. 8, no. 23,
pp. 17161–17171, Dec. 2021.
[37] X. Lin, J. Wu, S. Mumtaz, S. Garg, J. Li, and M. Guizani, “Blockchain-
based on-demand computing resource trading in IoV-assisted smart city,”
IEEE Trans. Emerg. Topics Comput., vol. 9, no. 3, pp. 1373–1385,
Jul.–Sep. 2021.
[38] M. Chen and Y. Hao, “Task offloading for mobile edge computing in
software defined ultra-dense network,” IEEE J. Sel. Areas Commun.,
vol. 36, no. 3, pp. 587–597, Mar. 2018.
[39] Y. Dai, D. Xu, S. Maharjan, and Y. Zhang, “Joint computation offloading
and user association in multi-task mobile edge computing,” IEEE Trans.
Veh. Technol., vol. 67, no. 12, pp. 12313–12325, Dec. 2018.
[40] N. Mhaisen, M. S. Allahham, A. Mohamed, A. Erbad, and M. Guizani,
“On designing smart agents for service provisioning in blockchain-
powered systems,” IEEE Trans. Netw. Sci. Eng., vol. 9, no. 2,
pp. 401–415, Mar./Apr. 2022.
[41] Z. Li, J. Kang, R. Yu, D. Ye, Q. Deng, and Y. Zhang, “Consortium
blockchain for secure energy trading in Industrial Internet of
Things,” IEEE Trans. Ind. Informat., vol. 14, no. 8, pp. 3690–3700,
Aug. 2018.
[42] S. Sheng, R. Chen, P. Chen, X. Wang, and L. Wu, “Futures-
based resource trading and fair pricing in real-time IoT networks,”
IEEE Wireless Commun. Lett., vol. 9, no. 1, pp. 125–128,
Jan. 2020.
[43] C. Ma et al., “Cooperative spectrum sharing in D2D-enabled cellu-
lar networks,” IEEE Trans. Commun., vol. 64, no. 10, pp. 4394–4408,
Oct. 2016.
[44] M. K. Farshbafan, M. H. Bahonar, and F. Khaiehraveni, “Spectrum
trading for device-to-device communication in cellular networks using
incomplete information bandwidth-auction game,” in Proc. 27th Iran.
Conf. Elect. Eng. (ICEE), 2019, pp. 1441–1447.
[45] J. Qiu, D. Grace, G. Ding, J. Yao, and Q. Wu, “Blockchain-based
secure spectrum trading for unmanned-aerial-vehicle-assisted cellular
networks: An operator’s perspective,” IEEE Internet Things J., vol. 7,
no. 1, pp. 451–466, Jan. 2020.
[46] L. Xue, W. Yang, W. Chen, and L. Huang, “STBC: A novel blockchain-
based spectrum trading solution,” IEEE Trans. Cogn. Commun. Netw.,
vol. 8, no. 1, pp. 13–30, Mar. 2022.
[47] Z. Liu, D. Wang, J. Wang, X. Wang, and H. Li, “A blockchain-enabled
secure power trading mechanism for smart grid employing wireless
networks,” IEEE Access, vol. 8, pp. 177745–177756, 2020.
[48] X. Lin, J. Wu, A. K. Bashir, J. Li, W. Yang, and J. Piran, “Blockchain-
based incentive energy-knowledge trading in IoT: Joint power transfer
and AI design,” IEEE Internet Things J., early access, Sep. 15, 2020,
doi: 10.1109/JIOT.2020.3024246.
[49] M. J. A. Baig, M. T. Iqbal, M. Jamil, and J. Khan, “IoT and blockchain
based peer to peer energy trading pilot platform,” in Proc. 11th IEEE
Annu. Inf. Technol. Electron. Mobile Commun. Conf. (IEMCON), 2020,
pp. 402–406.
[50] K. Zhang et al., “Incentive-driven energy trading in the smart grid,”
IEEE Access, vol. 4, pp. 1243–1257, 2016.
[51] N.-T. Nguyen et al., “Energy trading and time scheduling for energy-
efficient heterogeneous low-power IoT networks,” in Proc. IEEE Global
Commun. Conf., 2020, pp. 1–6.
[52] D. Zhang, F. R. Yu, and R. Yang, “Blockchain-based distributed
software-defined vehicular networks: A dueling deep Q-learning
approach,” IEEE Trans. Cogn. Commun. Netw., vol. 5, no. 4,
pp. 1086–1100, Dec. 2019.
[53] K. R. B. Sri, P. Aneesh, K. Bhanu, and M. Natarajan, “Design analysis
of solar-powered unmanned aerial vehicle,” J. Aerosp. Technol. Manage.,
vol. 8, no. 4, hboxpp. 397–407, 2016.
[54] L. D. Nguyen, K. K. Nguyen, A. Kortun, and T. Q. Duong, “Real-
time deployment and resource allocation for distributed UAV systems
in disaster relief,” in Proc. IEEE 20th Int. Workshop Signal Process.
Adv. Wireless Commun. (SPAWC), 2019, pp. 1–5.
[55] B. Wang, Y. Sun, Z. Sun, L. D. Nguyen, and T. Q. Duong, “UAV-assisted
emergency communications in social IoT: A dynamic hypergraph col-
oring approach,” IEEE Internet Things J., vol. 7, no. 8, pp. 7663–7677,
Aug. 2020.
[56] H. Hydher, D. N. K. Jayakody, K. T. Hemachandra, and
T. Samarasinghe, “Intelligent UAV deployment for a disaster-resilient
wireless network,” Sensors, vol. 20, no. 21, p. 6140, 2020.
[57] M. Y. Selim and A. E. Kamal, “Post-disaster 4G/5G network rehabilita-
tion using drones: Solving battery and backhaul issues,” in Proc. IEEE
Globecom Workshops (GC Wkshps), 2018, pp. 1–6.
[58] N. Malik and B. Joshi, “ECDSA approach for reliable data sharing and
document verification using two level QR code,” in Proc. 2nd Int. Conf.
I-SMAC (IoT Social, Mobile, Analytics Cloud) (I-SMAC)I-SMAC (IoT
Social, Mobile, Analytics Cloud) (I-SMAC), 2018, pp. 434–437.
[59] Z. Su, Y. Wang, Q. Xu, M. Fei, Y. Tian, and N. Zhang, “A secure
charging scheme for electric vehicles with smart communities in energy
blockchain,” IEEE Internet Things J., vol. 6, no. 3, pp. 4601–4613,
Jul. 2019.
[60] H. N. Abishu, A. M. Seid, Y. H. Yacob, T. Ayall, G. Sun, and
G. Liu, “Consensus mechanism for blockchain-enabled vehicle-to-
vehicle energy trading in the Internet of Electric Vehicles,” IEEE Trans.
Veh. Technol., vol. 71, no. 1, pp. 946–960, Jan. 2022.
[61] Z. Xiong, S. Feng, W. Wang, D. Niyato, P. Wang, and Z. Han,
“Cloud/fog computing resource management and pricing for blockchain
networks,” IEEE Internet Things J., vol. 6, no. 3, pp. 4585–4600,
Jun. 2019.
[62] M. Ferrara, M. Khademi, M. Salimi, and S. Sharifi, “A dynamic
Stackelberg game of supply chain for a corporate social responsibility,”
Discr. Dyn. Nat. Soc., vol. 2017, Feb. 2017, Art. no. 8656174.
[63] Z. Xiong, Y. Zhang, D. Niyato, R. Deng, P. Wang, and L.-C. Wang,
“Deep reinforcement learning for mobile 5G and beyond: Fundamentals,
applications, and challenges,” IEEE Veh. Technol. Mag., vol. 14, no. 2,
pp. 44–52, Jun. 2019.
[64] Y. S. Nasir and D. Guo, “Multi-agent deep reinforcement learning for
dynamic power allocation in wireless networks,” IEEE J. Sel. Areas
Commun., vol. 37, no. 10, pp. 2239–2250, Oct. 2019.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
http://dx.doi.org/10.1109/JIOT.2020.3024246
SEID et al.: BLOCKCHAIN-BASED RESOURCE TRADING IN MULTI-UAV-ASSISTED INDUSTRIAL IoT NETWORKS 181
[65] Z. Li and C. Guo, “Multi-agent deep reinforcement learning based spec-
trum allocation for D2D underlay communications,” IEEE Trans. Veh.
Technol., vol. 69, no. 2, pp. 1828–1840, Feb. 2020.
[66] T. Yuan, W. D. R. Neto, C. E. Rothenberg, K. Obraczka, C. Barakat, and
T. Turletti, “Dynamic controller assignment in software defined Internet
of Vehicles through multi-agent deep reinforcement learning,” IEEE
Trans. Netw. Service Manag., vol. 18, no. 1, pp. 585–596, Mar. 2021.
[67] T. P. Lillicrap et al., “Continuous control with deep reinforcement
learning,” 2015, arXiv:1509.02971.
[68] A. M. Seid, G. O. Boateng, B. Mareri, G. Sun, and W. Jiang,
“Multi-agent DRL for task offloading and resource allocation in multi-
UAV enabled IoT edge network,” IEEE Trans. Netw. Service Manag.,
vol. 18, no. 4, pp. 4531–4547, Dec. 2021.
[69] Q. Wu, Y. Zeng, and R. Zhang, “Joint trajectory and communication
design for multi-UAV enabled wireless networks,” IEEE Trans. Wireless
Commun., vol. 17, no. 3, pp. 2109–2121, Mar. 2018.
Abegaz Mohammed Seid (Member, IEEE) received
the B.Sc. degree in computer science from Ambo
University in 2010, the M.Sc. degree in computer
science from Addis Ababa University, Ethiopia, in
2015, and the Ph.D. degree in computer science
and technology from the University of Electronic
Science and Technology of China in 2021. He is
currently a Postdoctoral Fellow with the College
of Science and Engineering, Hamad Bin Khalifa
University, Doha, Qatar. He served as a Graduate
Assistant and a Lecturer, as well as a member of
the academic committee, and an Associate Registrar with Dilla University,
Ethiopia, from 2010 to 2016. He has published more than seven scientific con-
ferences and journal papers. His research interests include a wireless network,
mobile edge computing, blockchain, machine learning, vehicular network, IoT,
machine learning, UAV network, IoT, and 5G/6G wireless network.
Hayla Nahom Abishu received the B.Sc. degree
in computer science and information technology
from Haramaya University in 2007, and the M.Sc.
degree in computer science and networking from
Dilla University in 2017, Ethiopia. He is currently
pursuing the Ph.D. degree in computer science and
technology with the University of Electronic Science
and Technology of China, where he is also a mem-
ber with the Mobile Cloud-Network Research Team.
His research interests include mobile computing,
wireless network, blockchain, UAV network, IoT,
network security, and machine learning.
Yasin Habtamu Yacob received the B.Sc. degree
in information technology from Addis Ababa
University, Addis Ababa, Ethiopia, in 2005, and the
M.Sc. degree in computer science and networking
from Dilla University, Dilla, Ethiopia, in 2017. He
is currently pursuing the Ph.D. degree with the
Department of Computer Science and Technology,
University of Electronic Science and Technology
of China. He was a Senior Instructor of Cisco
Networking Academy for more than five years. His
current research interests include blockchain, mobile
edge computing, wireless networks, IoT, machine learning and network secu-
rity. He won the Cisco Advanced Level Instructors Award of 2015 and
2016.
Tewodros Alemu Ayall received the B.Sc. degree
in computer science from University of Gondar,
Ethiopia, in 2010, the M.Sc. degree in computer sci-
ence from Andhra University, India, in 2015, and
the Ph.D. degree in computer science and tech-
nology from the University of Electronic Science
and Technology of China, China, in 2021. He is
engaged in research of distributed graph processing,
distributed graph database, big data processing, big
graph partitioning, and blockchain research.
Aiman Erbad (Senior Member, IEEE) received
the B.Sc. degree in computer engineering from
the University of Washington, Seattle, in 2004, the
Master of Computer Science degree in embedded
systems and robotics from the University of Essex,
U.K. in 2005, and the Ph.D. degree in computer
science from the University of British Columbia,
Canada, in 2012. He is an Associate Professor and
ICT Division Head with the College of Science and
Engineering, Hamad Bin Khalifa University. Prior
to this, he was an Associate Professor with the
Computer Science and Engineering Department and the Director of Research
Planning and Development, Qatar University until May 2020. His research
received funding from the Qatar National Research Fund, and his research out-
comes were published in respected international conferences and journals. His
research interests span cloud computing, edge intelligence, Internet of Things
(IoT), private and secure networks, and multimedia systems. He received the
Platinum award from H.H. The Emir Sheikh Tamim bin Hamad Al Thani at the
Education Excellence Day 2013 (Ph.D. category). He also received the 2020
Best Research Paper Award from Computer Communications, the IWCMC
2019 Best Paper Award, and the IEEE CCWC 2017 Best Paper Award. He
also served as the Director of Research Support responsible for all grants
and contracts from 2016 to 2018 and as the Computer Engineering Program
Coordinator from 2014 to 2016. He is an Editor for KSII Transactions on
Internet and Information Systems and the International Journal of Sensor
Networks and a Guest Editor for IEEE NETWORK. He also served as
the Program Chair of the International Wireless Communications Mobile
Computing Conference (IWCMC 2019), as the Publicity chair of the ACM
MoVid Workshop 2015, as the Local Arrangement Chair of NOSSDAV 2011,
and as the Technical Program Committee Member in various IEEE and ACM
international conferences (GlobeCom, NOSSDAV, MMSys, ACMMM, IC2E,
and ICNC). He is a senior member of ACM.
Mohsen Guizani (Fellow, IEEE) received the B.S.
degree (with distinction) and M.S. degrees in elec-
trical engineering and the M.S. and Ph.D. degrees
in computer engineering from Syracuse University,
Syracuse, NY, USA, in 1984, 1986, 1987, and
1990, respectively. He is currently a Professor as
appointed as an Associate Provost for Faculty Affairs
and Institutional Advancement with Mohamed Bin
Zayed University of Artificial Intelligence, United
Arab Emirates. Previously, he served in differ-
ent academic and administrative positions with the
University of Idaho, Western Michigan University, University of West Florida,
University of Missouri-Kansas City, University of Colorado-Boulder, and
Syracuse University. He is the author of nine books and more than 600
publications in refereed journals and conferences. His research interests
include wireless communications and mobile computing, computer networks,
mobile cloud computing, security, and smart grid. Throughout his career, he
received three teaching awards and four research awards. He is the recipient
of the 2017 IEEE Communications Society Wireless Technical Committee
Recognition Award, the 2018 AdHoc Technical Committee Recognition
Award for his contribution to outstanding research in wireless communica-
tions and Ad-Hoc Sensor networks, and the 2019 IEEE Communications and
Information Security Technical Recognition Award for outstanding contribu-
tions to the technological advancement of security. He served as the IEEE
Computer Society Distinguished Speaker and is currently the IEEE ComSoc
Distinguished Lecturer. He also served as a member, the Chair, and the
General Chair of a number of international conferences. He was the Chair
of the IEEE Communications Society Wireless Technical Committee and the
Chair of the TAOS Technical Committee. He is currently the Editor-in-Chief
of the IEEE Network Magazine, serves on the editorial boards of several
international technical journals and the Founder and the Editor-in-Chief of
Wireless Communications and Mobile Computing journal (Wiley). He guest
edited a number of special issues in IEEE journals and magazines. He is
Senior Member of ACM.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 06:17:10 UTC from IEEE Xplore.  Restrictions apply. 
<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles false
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.4
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /LeaveColorUnchanged
  /DoThumbnails false
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo false
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts false
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
    /Arial-Black
    /Arial-BoldItalicMT
    /Arial-BoldMT
    /Arial-ItalicMT
    /ArialMT
    /ArialNarrow
    /ArialNarrow-Bold
    /ArialNarrow-BoldItalic
    /ArialNarrow-Italic
    /ArialUnicodeMS
    /BookAntiqua
    /BookAntiqua-Bold
    /BookAntiqua-BoldItalic
    /BookAntiqua-Italic
    /BookmanOldStyle
    /BookmanOldStyle-Bold
    /BookmanOldStyle-BoldItalic
    /BookmanOldStyle-Italic
    /BookshelfSymbolSeven
    /Century
    /CenturyGothic
    /CenturyGothic-Bold
    /CenturyGothic-BoldItalic
    /CenturyGothic-Italic
    /CenturySchoolbook
    /CenturySchoolbook-Bold
    /CenturySchoolbook-BoldItalic
    /CenturySchoolbook-Italic
    /ComicSansMS
    /ComicSansMS-Bold
    /CourierNewPS-BoldItalicMT
    /CourierNewPS-BoldMT
    /CourierNewPS-ItalicMT
    /CourierNewPSMT
    /EstrangeloEdessa
    /FranklinGothic-Medium
    /FranklinGothic-MediumItalic
    /Garamond
    /Garamond-Bold
    /Garamond-Italic
    /Gautami
    /Georgia
    /Georgia-Bold
    /Georgia-BoldItalic
    /Georgia-Italic
    /Haettenschweiler
    /Helvetica
    /Helvetica-Bold
    /HelveticaBolditalic-BoldOblique
    /Helvetica-BoldOblique
    /Helvetica-Condensed-Bold
    /Helvetica-LightOblique
    /HelveticaNeue-Bold
    /HelveticaNeue-BoldItalic
    /HelveticaNeue-Condensed
    /HelveticaNeue-CondensedObl
    /HelveticaNeue-Italic
    /HelveticaNeueLightcon-LightCond
    /HelveticaNeue-MediumCond
    /HelveticaNeue-MediumCondObl
    /HelveticaNeue-Roman
    /HelveticaNeue-ThinCond
    /Helvetica-Oblique
    /HelvetisADF-Bold
    /HelvetisADF-BoldItalic
    /HelvetisADFCd-Bold
    /HelvetisADFCd-BoldItalic
    /HelvetisADFCd-Italic
    /HelvetisADFCd-Regular
    /HelvetisADFEx-Bold
    /HelvetisADFEx-BoldItalic
    /HelvetisADFEx-Italic
    /HelvetisADFEx-Regular
    /HelvetisADF-Italic
    /HelvetisADF-Regular
    /Impact
    /Kartika
    /Latha
    /LetterGothicMT
    /LetterGothicMT-Bold
    /LetterGothicMT-BoldOblique
    /LetterGothicMT-Oblique
    /LucidaConsole
    /LucidaSans
    /LucidaSans-Demi
    /LucidaSans-DemiItalic
    /LucidaSans-Italic
    /LucidaSansUnicode
    /Mangal-Regular
    /MicrosoftSansSerif
    /MonotypeCorsiva
    /MSReferenceSansSerif
    /MSReferenceSpecialty
    /MVBoli
    /PalatinoLinotype-Bold
    /PalatinoLinotype-BoldItalic
    /PalatinoLinotype-Italic
    /PalatinoLinotype-Roman
    /Raavi
    /Shruti
    /Sylfaen
    /SymbolMT
    /Tahoma
    /Tahoma-Bold
    /Times-Bold
    /Times-BoldItalic
    /Times-Italic
    /TimesNewRomanMT-ExtraBold
    /TimesNewRomanPS-BoldItalicMT
    /TimesNewRomanPS-BoldMT
    /TimesNewRomanPS-ItalicMT
    /TimesNewRomanPSMT
    /Times-Roman
    /Trebuchet-BoldItalic
    /TrebuchetMS
    /TrebuchetMS-Bold
    /TrebuchetMS-Italic
    /Tunga-Regular
    /Verdana
    /Verdana-Bold
    /Verdana-BoldItalic
    /Verdana-Italic
    /Vrinda
    /Webdings
    /Wingdings2
    /Wingdings3
    /Wingdings-Regular
    /ZapfChanceryITCbyBT-MediumItal
    /ZWAdobeF
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 200
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages false
  /ColorImageDownsampleType /Average
  /ColorImageResolution 300
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 200
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages false
  /GrayImageDownsampleType /Average
  /GrayImageResolution 300
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 400
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages false
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False
  /CreateJDFFile false
  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Recommended"  settings for PDF Specification 4.01)
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice