Online Deep Reinforcement Learning for Computation Offloading in Blockchain-Empowered Mobile Edge Computing
8050 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 68, NO. 8, AUGUST 2019
Online Deep Reinforcement Learning for
Computation Offloading in Blockchain-Empowered
Mobile Edge Computing
Xiaoyu Qiu , Luobin Liu, Wuhui Chen , Member, IEEE, Zicong Hong ,
and Zibin Zheng , Senior Member, IEEE
Abstract—Offloading computation-intensive tasks (e.g.,
blockchain consensus processes and data processing tasks) to
the edge/cloud is a promising solution for blockchain-empowered
mobile edge computing. However, the traditional offloading ap-
proaches (e.g., auction-based and game-theory approaches) fail
to adjust the policy according to the changing environment and
cannot achieve long-term performance. Moreover, the existing
deep reinforcement learning-based offloading approaches suffer
from the slow convergence caused by high-dimensional action
space. In this paper, we propose a new model-free deep reinforce-
ment learning-based online computation offloading approach for
blockchain-empowered mobile edge computing in which both
mining tasks and data processing tasks are considered. First, we
formulate the online offloading problem as a Markov decision
process by considering both the blockchain mining tasks and data
processing tasks. Then, to maximize long-term offloading perfor-
mance, we leverage deep reinforcement learning to accommodate
highly dynamic environments and address the computational com-
plexity. Furthermore, we introduce an adaptive genetic algorithm
into the exploration of deep reinforcement learning to effectively
avoid useless exploration and speed up the convergence without
reducing performance. Finally, our experimental results demon-
strate that our algorithm can converge quickly and outperform
three benchmark policies.
Index Terms—Online computation offloading, blockchain,
mobile edge computing, deep reinforcement learning.
I. INTRODUCTION
MOBILE edge computing (MEC) is a promising solution
that allows mobile devices to run the highly demand-
ing applications by providing computational resources. How-
ever, building trust among multiple parties (e.g., different mobile
users and edge/cloud providers) in MEC is a challenge because
Manuscript received May 18, 2019; revised June 12, 2019; accepted June
15, 2019. Date of publication June 20, 2019; date of current version August 13,
2019. This work was supported in part by the National Key Research and Devel-
opment Plan under Grant 2018YFB1003803, in part by the National Natural Sci-
ence Foundation of China under Grants 61802450 and 61722214, in part by the
Natural Science Foundation of Guangdong under Grant 2018A030313005, and
in part by Program for Guangdong Introducing Innovative and Entrepreneurial
Teams under Grant 2017ZT07X355. The review of this paper was coordinated
by Prof. G. Gui. (Corresponding author: Wuhui Chen.)
The authors are with the School of Data and Computer Science, Sun Yat-sen
University, Guangzhou 510245, China, and also with the National Engineering
Research Center of Digital Life, Sun Yat-sen University, Guangzhou 510006,
China (e-mail: qiuxy23@mail2.sysu.edu.cn; kelvin.liu1221@gmail.com;
chenwuh@mail.sysu.edu.cn; hongzc@mail2.sysu.edu.cn; zhzibin@mail.sysu.
edu.cn).
Digital Object Identifier 10.1109/TVT.2019.2924015
these parties usually have conflicting interests. Fortunately,
blockchain technologies taking advantage of decentralization,
anonymity, and trust have begun to exert a significant influence
on MEC [1], [2]. Nonetheless, running blockchain mining pro-
cesses (e.g., performing Proof of Stake (PoS)) while support-
ing increasingly intelligent applications (processing/analyzing
tasks) can require vast computing and storage resources [3].
Thus, the limited computing and storage capacities of mobile
devices are restricting real-world blockchain-empowered mo-
bile applications [4]. Therefore, it is vital to develop a com-
putation offloading solution that can extend the capacities of
mobile devices by offloading computation-intensive tasks (e.g.,
blockchain consensus processes) to the edge/cloud servers.
To develop the computation offloading solution for
blockchain-empowered MEC, social welfare maximization
auction-based approaches [5] (e.g., sealed-bid auction, com-
binatorial auction, forward, reverse, and double auction) and
game-theory approaches [6] (e.g., noncooperative game, stack-
elberg game [7], [8], and bargaining game) have been proposed,
in which the blockchain mining tasks can be offloaded to the
edge/cloud servers. These methods often require many itera-
tions for an algorithm to reach a satisfying optimum [9]. In ad-
dition, the above approaches are mostly built in consideration
of one-shot optimization, they may not apply well to maximize
the long-term performance of computation offloading. Further,
these algorithms usually assume specific models for the compu-
tation offloading solutions, which may not accurately character-
ize the realistic environment.
Deep reinforcement learning (DRL) is emerging as an effec-
tive approach to obtain the optimal decision-making policy and
maximize the long-term rewards [10]. First, the combination of
reinforcement learning and deep learning [11] has been exten-
sively researched to create strategies for computation offload-
ing. Kawamoto et al. [12] utilized Q-learning to construct an
unmanned aircrafts communication management system. How-
ever, the performance of the algorithm is greatly affected by
the convergence and accuracy of the deep Q-network (DQN).
Second, improving the performance of a DQN for computation
offloading has attracted much attention recently. Zhang et al.
[13] proposed a modified DQN by using the stacked autoen-
coder, which achieved an outstanding resource management in
cloud servers. However, the dimension of action space would
expand exponentially when considering multi-hop multi-user
0018-9545 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
https://orcid.org/0000-0002-9322-9060
https://orcid.org/0000-0003-4430-7904
https://orcid.org/0000-0001-5689-382X
https://orcid.org/0000-0001-7872-7718
mailto:qiuxy23@mail2.sysu.edu.cn
mailto:kelvin.liu1221@gmail.com
mailto:chenwuh@mail.sysu.edu.cn
mailto:hongzc@mail2.sysu.edu.cn
mailto:zhzibin@mail.sysu.edu.cn
QIU et al.: ONLINE DEEP REINFORCEMENT LEARNING FOR COMPUTATION OFFLOADING 8051
MEC systems. Third, some recent studies have attempted to
address the high complexity problem. Mnih et al. [10] com-
bined a value-based method with a policy-based method and pro-
posed asynchronous methods for deep reinforcement learning.
Nonetheless, to our knowledge, the slow convergence caused by
high-dimensional action space remains an urgent and challeng-
ing problem.
In this paper, we study the computation offloading problem
for both mining tasks and data processing tasks in multi-hop
multi-user blockchain-empowered MEC. To solve the problem,
we propose a new model-free DRL-based online computation
offloading algorithm, namely deep reinforcement learning com-
bined with genetic algorithm (DRGO). The DRGO algorithm
aims to maximize long-term rewards and accommodate highly
dynamic environments via deep reinforcement learning. In par-
ticular, to speed up the convergence caused by high-dimensional
action space, the DRGO algorithm adopts adaptive genetic al-
gorithm (AGA) [14] during exploration in deep reinforcement
learning, which effectively avoids useless exploration to a great
extent and reduces the computational complexity of deep re-
inforcement learning. To our knowledge, this is the first time
that AGA has been introduced to solve the large action space
problem of DRL.
Our contributions can be mainly summarized as follows.
1) We are the first to study the online computation of-
floading need for both the blockchain mining tasks and
data processing tasks in multi-hop multi-user blockchain-
empowered MEC. We then formulate the online offloading
problem as a Markov decision process (MDP) to consider
the dynamic environments.
2) To adopt the dynamic environments and achieve long-term
offloading performance, we leverage the deep reinforce-
ment learning to obtain the optimal offloading policy based
on the past offloading experience. In this context, a con-
siderable number of iterations for reaching a satisfying
optimum can be avoided.
3) To speed up the convergence, we introduce adaptive ge-
netic algorithm into the exploration of deep reinforcement
learning. In this way, we can make the most of the evaluat-
ing role of critic network in exploration, avoiding useless
exploration to a great extent without reducing the perfor-
mance.
The remainder of the paper is structured as follows. In
Section II, we review the related work. In Section III, we present
a system model and Section IV describes the problem formu-
lation. In Section V, we introduce the proposed DRL-based
method for computation offloading, and Section VI presents the
numerical simulations. Section VII concludes.
II. RELATED WORKS
Many studies have examined the computation offloading need
for edge computing or cloud computing [15]. Liu et al. [16]
proposed a multiplier-based computation offloading algorithm,
where the blockchain mining tasks can be offloaded to nearby
edge computing nodes. Chen et al. [17] transformed the re-
sources allocation problem of computation offloading into a
mixed-integer linear programming problem to improve quality-
of-service and reduce execution times. Chatzopoulos et al. [18]
proposed a hidden market design-based offloading approach that
allows users to specify the amount of resources they are willing
to share. However, all these algorithms are constrained by the
trade-off between efficiency and optimality. Moreover, they con-
sider immediate rewards and assume specific models that may
be incompatible.
Recently, artificial intelligence approaches such as deep learn-
ing [19], [20] and reinforcement learning [21] have emerged as
effective measures to solve computation offloading problems. A
resource allocation algorithm based on deep Q-learning is pro-
posed in [22] to optimize the performance of computation of-
floading in MEC. Liu et al. [23] proposed a deep learning-based
resource allocation algorithm to enable high energy efficiency
and low power consumption. Qi et al. [24] explored DRL to
obtain the optimal offloading decision for the Internet of Ve-
hicles. However, to apply DRL to computation offloading, the
major challenge lies in the convergence and accuracy of the deep
neural network (DNN).
To accelerate the learning process, transfer learning integrat-
ing with DRL is proposed in [25] to reduce the random explo-
ration at the initial learning process. Min et al. [26] compressed
the state space by utilizing a deep convolutional neural network
(CNN) and achieved a near-optimal offloading policy. In con-
trast, to avoid overestimation and reduce bias, Wang et al. [9]
devised two double DQN-based algorithms to address the cost-
minimization problem of D2D offloading. However, when con-
sidering the multi-hop multi-user environment, the exponential
explosions of the state space and action space are inevitable and
prevent the above approaches from working.
A few recent studies have attempted to solve the exponential
explosion of the action space. Wei et al. [27] proposed a pol-
icy gradient-based actor-critic algorithm for the computation of-
floading and content caching problems with continuous-valued
state and action variables. Zhu et al. [28] proposed a hybrid
actor-critic method, which asynchronously trained the network
parameters and decreased the long-term offloading cost. In addi-
tion, although methods such as asynchronous advantage actor-
critic (A3C) [29] or deep deterministic policy gradient (DDPG)
[30] can solve the problem of exponential explosion, they also
lead to slow convergence. Because of the high-dimensional ac-
tion space, traditional exploration methods such as the ε-greedy
policy are obviously inefficient and inadequate. Therefore, we
develop a DRL-based online offloading algorithm to address
the challenge. In particular, to speed up the convergence, we
adopt AGA during exploration, which can also avoid useless
exploration to a great extent and reduce the computational com-
plexity.
III. SYSTEM MODEL
In this work, we study the computation offloading need for
blockchain-empowered MEC, where nearby IoT devices are
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
8052 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 68, NO. 8, AUGUST 2019
Fig. 1. Multi-hop blockchain-empowered mobile edge computing.
grouped as communities such as smart home and smart indus-
trial. Because these IoT devices belong to the same communities,
it is reasonable to model the system as an optimization problem
about long-term total reward. Fig. 1 shows the architecture of our
proposed system, which consists of the following components.
A. Blockchain Network
To provide security and privacy for IoT devices, the
blockchain technology is introduced to MEC, which is known
for its security and immutability. A blockchain works as a de-
centralized database and stores data as blocks after validation.
Some nodes, known as miners, collect transactions from the
whole blockchain network and attempt to perform consensus
processes such as Proof-of-Work (PoW) or PoS. In this way, the
first miner who reaches consensus is rewarded and a new block
is connected to the whole blockchain after validation. There-
fore, the data contained in the block cannot be tampered with or
forged.
B. Computation Offloading for Data Processing Tasks
In the system model, end devices are connected through a
multi-hop ad-hoc network that supports data transmission and
computation offloading. Assuming there are N end devices
(EDs), we let N = {1, 2, 3, . . . , N} denote the set of the EDs.
Each ED i(i ∈ N ) is uniquely identified using an account in
the blockchain network. We denote bi (tokens) as the balance of
the account associated with ED i, that can be used to purchase
computational resources for data processing tasks, such as 3-D
sensing and augmented reality. Because of the limited computa-
tional power of EDs, executing the data processing tasks locally
could cause a long delay. Therefore, the tasks can be scheduled
to be executed locally or offloaded to edge/cloud servers. In this
work, we introduce the concept of task queue, which is a data
structure that contains a list of tasks to be performed in turn. If the
computational resources are occupied at the current time epoch,
tasks can be queued at the task queue until the resources are
available. The brief information of the generated tasks is trans-
mitted to a management hub, which is responsible for making
offloading decisions for EDs to achieve higher performance.
C. Computation Offloading for Blockchain Mining Tasks
As part of the blockchain network, EDs can participate in the
mining process to obtain rewards. However, the mining process
is not performed locally because of resource constraints. In gen-
eral, EDs can entrust proxy miners to purchase computational
resources from edge/cloud servers and perform the consensus
processes in the edge/cloud servers [31].
D. Resource Purchase Scheme in Edge/Cloud Servers
We assume that a virtual machine (VM) is the smallest unit
to perform the tasks [32] and can only execute one task at the
same time. In the servers, VMs are divided into different levels
to meet the requirements of various types of offloading requests
[32]. In this paper, we assume that VMs for data processing tasks
in the servers are divided into k levels, which are denoted by
Lc = {1, 2, . . . , k}. Similarly, VMs for mining tasks are divided
into j levels that are denoted by Lm = {1, 2, . . . , j}. In general,
the edge servers provide the resources for computation. In the
case that they cannot serve the EDs, they offload the tasks to the
cloud servers and charge the communication and computation
cost to the EDs.
E. Trade-Off Between Two Tasks
As the offloading of both tasks needs to purchase resources
from servers, it is important to consider the trade-off between
both tasks. For example, if an ED spends most of its tokens in
blockchain mining at the current time epoch, it may fail to exe-
cute the incoming data processing tasks because the mining cost
is paid immediately, while the reward is obtained until reaching
consensus. On the other hand, the reward obtained after reach-
ing consensus can be used to improve the performance of data
processing tasks. Therefore, it is critical to jointly consider both
tasks. We integrate the management hub for data processing
tasks with the proxy miner for mining tasks as a joint manage-
ment module (JM).
F. Multi-Hop Ad-Hoc Network
In the ad-hoc network, some EDs are directly connected to
the JM, but some may not connect because of distance or device
limitations. However, some nodes can communicate with their
neighboring peers and help their neighboring peers transfer data
to the JM. Therefore, similar to [33], we divide EDs into dif-
ferent hierarchical levels based on their network topology. We
assume EDs that directly connect to JM are in level 1. In the same
manner, EDs that can only send data to the JM via neighboring
EDs in level 1 are in level 2. Subsequent levels are formatted
similarly. To avoid data transmission congestion and adapt to the
dynamic environment, each ED can dynamically select nearby
devices as its next hop to transmit data.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
QIU et al.: ONLINE DEEP REINFORCEMENT LEARNING FOR COMPUTATION OFFLOADING 8053
TABLE I
NOTATION DEFINITIONS
IV. PROBLEM FORMULATION
A. State and Action
At each decision epoch t, the state of the system can be char-
acterized by St = (Sm,M c,G, P c, F c, Pm, Fm, Qs, H). The
meaning of each variable is as follows.
� Sm = {Si = (qi, fi, ui, bi)|i ∈ {1, 2, . . . , N}} denotes
the collected states of all EDs, including the tasks queue
length qi, the CPU frequency of local device fi, the unique
network ID ui and the balance bi of the associated account.
� M c = {M c
i = (li, di, vi, s
u
i , s
d
i )|i ∈ {1, 2, . . . , N}} de-
notes the brief information of all data processing tasks of
EDs, including CPU cycles li, the deadline of the task
di, completion reward vi, uploading data size sui , and
downloading data size sdi .
� G is the current network state, including the topology and
data transmission rates between each ED. If ED i cannot
communicate with ED j, the data transmission rate be-
tween ED i and ED j is set as 0.
� P c = {pc1, pc2, . . . , pck} and F c = {f c
1 , f
c
2 , . . . , f
c
k} denote
the prices and CPU frequencies of different VM levels
for data processing tasks, respectively. Similarly, Pm =
{pm1 , pm2 , . . . , pmj } and Fm = {fm
1 , fm
2 , . . . , fm
j } are for
mining VMs.
� Qs denotes the length of the task queue in the servers.
� Lastly, H is the estimation of the hash power (also called
hash rate) of the blockchain network, representing the
computational power that the blockchain network is con-
suming. H determines how many hash operations can be
performed per second and can be estimated based on the
compact status reports issued by the miners [34].
Based on the system state St at decision epoch t, JM makes
a task offloading action At = {ai|i ∈ {1, 2, . . . , N}} and sends
ai to the corresponding ED i. Each action ai can be represented
as (ci,mi, ni). ci ∈ {0, 1, 2, . . . , k} denotes the offloading de-
cision of data processing tasks. If ci = 0, ED i executes the tasks
locally. Otherwise, ED i purchases a corresponding level of VM
and offloads tasks to servers. Similarly, mi ∈ {0, 1, 2, . . . , j}
denotes the offloading decision of data mining tasks. If mi = 0,
ED i does not decide to mine. In addition, to offload the tasks,
the required data and program code need to be transmitted to the
servers. Considering the multi-hop scenario that we discussed,
ni denotes the network ID of the next hop in the upper level. In
this work, the action is coded in binary. For example, if ni = 3,
we convert it to ‘11’.
B. Cost of Data Processing Task
1) Local Execution Model: If a data processing task is ex-
ecuted locally at ED, that is, ci = 0, ED i need to consume a
large amount of energy to compute its tasks. In our proposed
blockchain-empowered system, we convert the cost of energy
consumption into tokens to unify units. Let σ denote the unit
energy consumption of CPU cycles (joules per cycle), li denote
the required CPU cycles and pe denote the unit price of energy
(tokens per joule). Thus, the local execution cost of ED is as
follows.
C local
i = σlip
e · 1{ci=0}, (1)
where 1{Ω} is the indicator function, that is, 1{Ω} equals to 1 if
the condition Ω is satisfied. Otherwise, 1{Ω} equals to 0.
2) Offloading Computation Model: Considering the limited
computational power of EDs, they can offload their tasks to the
servers to achieve better performance. If ED i decides to offload
its tasks to servers, the corresponding price for VM of level ci
must be paid, which is as follows.
XC
i = lip
c
ci
· 1{ci∈[1,k]}, (2)
where pcci is the price of the purchased VM (token per Gcycle).
In addition, to offload tasks, EDs need to upload the required
data and program code to the servers and download the return
data back with the assistance of JM or their neighboring peers.
We denote the maximal achievable transmission rate between
ED i and ED j as K<i,j>. It is worth noting that ED i may
receive transmission requests from other EDs and we assume the
bandwidth allocated to each ED is equal. Therefore, the expected
rate of transmitting data from ED i to ED j can be written as
follows:
ri =
K<i,j>
Ni
, (3)
where Ni is the number of EDs that transmit data via ED i.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
8054 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 68, NO. 8, AUGUST 2019
To offload tasks to servers, EDs should pay for the energy
consumption during data transmission. We denote Ui as the set
of EDs that transmit data via ED i (including itself), thus the
cost of data transmission in our offloading computation model
is as follows:
C trans
i =
∑
j∈Ui(s
u
j + sdj )
ri
· Wip
e · 1{ci∈[1,k]}, (4)
where Wi is the transmit power of ED i. suj and sdj are the
uploading and downloading data sizes of ED j. And pe denotes
the unit price of energy (token per joule).
To sum up, the total cost for offloading task is as follows:
Coffload
i =
(
XC
i +
∑
j∈Ui(s
u
j + sdj )
ri
· Wip
e
)
· 1{ci∈[1,k]}.
(5)
C. Reward of Mining Task
1) Reward of Blockchain Mining: Ifmi �= 0, that is, ED i de-
cides to perform the mining task in servers, it needs to compete
for accounting rights with other miners by calculating the nu-
merical solutions of a random hash. In a blockchain network, the
hash power determines how many hash operations can be per-
formed per second. According to [35], the relative hash power
of ED i to the blockchain network is as follows:
δi =
fm
mi
H
, (6)
where fm
mi
is the hash power of the VMs that ED i purchase,
and H is the estimation of the hash power of the blockchain
network, which is based on the compact status reports issued by
miners [34]. In general, as δi increases, the possibility to reach
a consensus increases, and the expected reward increases.
In the blockchain network, if a well-formed block is no longer
part of the longest and well-formed blockchain, called an orphan
block, the miner who mined that block does not actually get the
reward (or the transaction fees). According to [4], the possi-
bility of a block becoming an orphan block is approximately
1− e−λv(sm), where −λ denotes a constant rate, sm is the size
of the block to be mined and v(sm) denotes a function of block
size. We set R as the rewards of the first miner that reaches
consensus. Therefore, the expected rewards of ED i are:
Ri = e−λv(sm)δi · R · 1{mi∈[1,j]}. (7)
2) Cost of Blockchain Mining: Based on our proposed
blockchain-empowered MEC, a payment is required for the al-
located computational resources when ED i decides to perform
the mining tasks in the servers, which is as follows:
XM
i = pmmi
· 1{mi∈[1,j]}. (8)
Overall, the total reward of mining tasks can be represented
as follows:
Rmine
i =
(
e−λv(sm)δi · R −XM
i
)
· 1{mi∈[1,j]}. (9)
D. Task Failure Cost and Completion Reward
1) Local Execution Failure: In this part, we consider the fail-
ure cost of the data processing tasks. In our model, the computa-
tion resources of local devices are not infinite. We assume tasks
performed locally are queued at the task queue based on the
first-in-first-out principle. If the task queue is full, the generated
task will be rejected and EDs will receive a penalty Θ. We de-
note the task queue of ED i at the current time epoch as qi and
the maximum task queue length as qmax
i . In addition, we assume
that the queuing delay is the estimated execution time of all the
prior tasks in the queue and is denoted as dqueue
i . Thus, the task
delay for local execution is determined by the queuing delay and
execution delay, which can be represented as:
dlocal
i =
(
dqueue
i +
li
fi
)
· 1{ci=0}, (10)
where li is required CPU cycles and fi is the CPU frequency
of ED i. If dlocal
i > di, that is, the task delay is larger than the
deadline, the execution of the task fails and ED i should pay
a penalty. Therefore, the failure penalty for local execution is
defined as:
Θlocal
i = Θ{{qi=qmax
i }∨{dlocal
i >di}}, (11)
where ∨ means “logic OR”.
2) Offload Execution Failure: Similarly, the task delay for
offloading is determined by the transmission delay, queue delay
and execution delay. First, we denote Li as the complete routing
path from ED i to servers, which contains the network IDs of all
passing EDs and JM. Therefore, the transmission delay of ED i
is as follows:
dtrans
i =
∑
j∈Li
(sui + sdi ) ·Nj
rj
, (12)
where sui and sdi are the uploading data size and downloading
data size respectively.Nj is the number of EDs that transmit data
via ED j (j �= 0) or JM (j = 0) and rj is the data transmission
rate of ED j (j �= 0) or JM (j = 0).
Second, we denote the task queue at the servers as qs, the
maximum task queue length as qmax
s , and the estimated queuing
time as dqueue
s . Therefore, the sum of the queue delay and the
execution delay of the offloaded tasks is as follows:
dexecute
i =
li
pcci
+ dqueue
s , (13)
where li is the required CPU cycles and pcci is the CPU frequency
of the purchased VM.
By using Equation (12) and (13), the task delay can be repre-
sented as:
doffload
i =
∑
j∈Li
(sui + sdi ) ·Nj
rj
+
li
pcci
+ dqueue
s . (14)
In addition, similar to local execution, if the task delay exceeds
the deadline di, ED i receives a penalty that can be represented
as:
Θoffload
i = Θ{{doffload
i >di}∨{qs=qmax
s }}. (15)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
QIU et al.: ONLINE DEEP REINFORCEMENT LEARNING FOR COMPUTATION OFFLOADING 8055
3) Completion Reward: If a data processing task is success-
fully executed, ED i receives a reward vi. Thus, the completion
reward of ED i is as follows:
Vi = vi · 1{Θlocal
i =0}∧{Θoffload
i =0}, (16)
where ∧ means “logic AND”.
E. Cost of System
To sum up, using Equations (1), (5), (9), (11), (15) and (16),
the cost of ED i is:
Ci = C local
i + Coffload
i −Rmine
i +Θlocal
i +Θoffload
i − Vi. (17)
From the perspective of system, the instantaneous system cost
can be represented as:
Ctotal =
∑
i∈N
Ci, (18)
where N = {1, 2, 3, . . . , N} is the set of EDs.
F. MDP-Based Optimization Problem
In our performance optimization problem, it is important to
note that costs such as energy consumption and purchasing VMs
should be paid immediately. However, the task completion re-
wards are not given until the tasks are successfully executed.
Similarly, the mining rewards are not given until reaching con-
sensuses. If an ED spends most of its tokens on blockchain min-
ing or the current data processing tasks, it may obtain many
rewards after achieving consensuses or finishing the tasks. How-
ever, it may not have enough tokens for the incoming data pro-
cessing tasks and thus suffers a penalty. Therefore, it is important
to consider the computation offloading problem in the long run.
In this paper, we formulate the computation offloading prob-
lem as a discrete time MDP, which is widely used to address
sequential stochastic decision-making problems. In this con-
text, the considerations we discussed above are automatically
considered in an MDP model, which can be defined as a tuple
< S,A, P,R(·),T >:
� S: the state space
� A: the action space
� P : the transition probability from taking action At under
state St to next state St+1
� R(·): the immediate reward function that takes the state–
action pair as input
� T: a sequence of time
In our scenarios, the immediate reward function R(·) of the
MDP equals to −Ctotal(·), which is the system cost function
in Equation (18). Therefore, the objective of this research is to
find an optimal policy π∗ that makes decisions to maximize the
long-term reward, which can be defined as:
maxEπ,S
[
T∑
t=1
R(St+1|St, π(At))
]
. (19)
V. ALGORITHM DESIGN
Deep reinforcement learning (DRL) is emerging as an effec-
tive approach to obtain the optimal decision-making policy and
maximize the long-term rewards [10]. In traditional deep rein-
forcement learning, a DNN is used to approximate the mapping
from the current system state St to the expected rewards or se-
lected probabilities of all possible actions. Because it requires
exhaustive search through all actions, it is unsuitable for prob-
lems with high-dimensional action spaces, which leads to slow
convergence [10]. To support problems with high-dimensional
action spaces, approaches such as A3C [29] or DDPG [30] are
proposed, which combine policy gradient with value function.
These algorithms use two kind of neural networks: actor net-
work and critic network. The actor network makes action based
on the state, while the critic network evaluates the action taken.
However, they often require specific design for the exploration
mechanism to explore the complex action space. Otherwise, it
inevitably leads to a largely unexplored action space.
To address the challenge, we propose an online computation
offloading framework combined with DRL and AGA. The AGA
is introduced in the exploration phase to avoid largely unex-
plored action space. The next section focuses on the details of
our proposed algorithm and its update strategy.
A. Overview of the DRGO Algorithm
Fig. 2 shows that the DRGO algorithm consists of two im-
portant components: an actor network and a critic network. In
the actor network, rather than outputting the expected rewards
or possibilities of all possible actions, it works as a policy π
that takes system states St as inputs and outputs an action At at
decision epoch t. For any given system state St, the offloading
policy π can be defined as a mapping:
π : St �→ A. (20)
And for any actionA, the critic network is used to evaluate the
expected long-term reward, which can be defined as a mapping.
Q : (St, A) �→ R. (21)
In the system, JM receives requests from EDs periodically
and combines them to system state St. The DRGO algorithm
is triggered regularly or when JM receives a certain amount of
requests from EDs. In this context, the decision epoch is change-
able in practice, which is more realistic for actual situations. At
decision epoch t, the actor network takes system stateSt as input
and produces an action used in exploration or exploitation. We
denote the action after exploration or exploitation as At. Next,
JM transmits At to EDs to obtain the next state St+1 and re-
ward Rt. Then, we store (St, At, Rt, St+1) in replay memory.
At each training epoch, we select samples from the replay mem-
ory to update the parameters in the actor network and the critic
network toward maximizing the long-term reward.
B. Using AGA to Generate the Candidate Solution Set
The actor network works as a policy π that takes system state
St as the inputs and produces a action A (Algorithm 1: line 6).
To maximize the expected long-term reward of the system, we
need to train the actor network toward outputting the optimal
action A∗. In traditional reinforcement learning method such as
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
8056 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 68, NO. 8, AUGUST 2019
Fig. 2. DRL combined with genetic algorithm for computation offloading in blockchain-empowered MEC.
Fig. 3. High-efficiency DRL exploration using AGA.
DDPG, an entropy bonus is added to ensure sufficient explo-
ration. However, to fully explore all actions, it consumes a great
amount of time and leads to low data utilization.
Therefore, in this work, two methods are used to guarantee
both efficiency and sufficiency of the exploration process. As
mentioned, the critic network can be used to evaluate the ex-
pected long-term reward for each state-action pair on condition
that it is well-trained. And the loss value is used to measure the
inconsistency between the predicted value and the actual value.
Therefore, if the loss value of the critic network is larger than
the preset value ϕ, that is, the critic network cannot evaluate
candidate actions very well, we use the random mechanism to
generate a new action after obtaining the output of the actor net-
work for sufficiency. Otherwise, we use the AGA for efficiency
(Algorithm 1: line 7). The preset value ϕ can be set as the loss
value when the critic network converges, which can be obtained
by pretraining the critic network.
The usage of AGA in the exploration process is illustrated
in Fig 3. First, it combines the output of the actor network
A with m− 1 randomly generated solutions, where m is the
size of the population. Then it uses genetic operators such as
crossover and mutation to generate the candidate solution set
A = {A1
t , A
2
t , . . . , A
m
t } as the next and subsequent generations.
For each generation of AGA, we use the critic network to mea-
sure the performance of each action and use selection operators
to select candidates that contribute to the population of the next
generation. AGA terminates when the number of generations
has reached a preset maximum number Kmax.
Here, we introduce two important terms, the probability of
crossover (pc), which indicates the converge characteristic, and
the probability of mutation (pm), which indicates the charac-
teristic of breaking through the limitation of the current search
space. If the population tends to stay at the local optimum, we
increase the values of pc and pm. Otherwise, we decrease the
values of pc and pm. One way to measure the convergence abil-
ity is R−Rmin, where R is the average reward and Rmin is
the minimum reward. It is obvious that R−Rmin is likely to be
lower when a population converges to a local optimum (or global
optimal) solution. In this way, pc and pm can be expressed as:
pc =
k1
R−Rmin
, (22)
pm =
k2
R−Rmin
. (23)
Increasing pc and pm may cause disruptions of the near-
optimal solutions when a population reaches a global optimal
solution. To solve this problem, pc should depend on the reward
of its parent R′, and pm should depend on the reward of itself
R. As Rmin −R′ increases, pc should also increase. Similarly,
as R−Rmin increases, pm should also increase. In addition, we
preset k3 and k4 as the recommended values when pc and pm
increase beyond 1. Therefore, the expressions for pc and pm are
pc =
⎧
⎨
⎩
k1(Rmin −R′)
R−Rmin
R′ ≤ Rmin
k3 R′ > Rmin
, (24)
pm =
⎧
⎨
⎩
k2(R−Rmin)
R−Rmin
R ≥ Rmin
k4 R < Rmin
, (25)
where k1, k2, k3, and k4 < 1.0.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
QIU et al.: ONLINE DEEP REINFORCEMENT LEARNING FOR COMPUTATION OFFLOADING 8057
C. Selecting From the Candidate Set in AGA
In the next step, we use the critic network to select from the
candidate set by estimating Q(St, A) for each state–action pair
in the candidate solution set A. Considering a stationary task
offloading policy π, the system follows the state transition prob-
ability, which can be expressed as:
Pr{St+1|St, π(St)} =
N∏
i=0
Pr{Si
t+1|Si
t , π(S
i
t)}, (26)
where Si
t is the state of ED i in decision epoch t.
The task offloading problem can be regarded as an MDP. Con-
sidering the offloading policy π, V (S, π) denotes the expected
reward for following policy π in state S, and Q(S,A) is the
expected reward for selecting action A in state S and then fol-
lowing policy π. V (S, π) and Q(S,A) are considered as the
state value and state–action value functions, respectively. Tak-
ing expectation with respect to system reward R at each epoch
over a time sequence {t|(t ∈ N+)}, the expected long-term cu-
mulative reward can be characterized as:
V(S, π) = Eπ
[
(1− γ) ·
∞∑
t=1
(γ)t−1 ·R(St, π(St))
]
, (27)
where St (t ∈ N+) is the system state at epoch t, (γ)t−1 is
the discount factor, and R(St, π(St)) denotes the total system
reward for taking action π(St) under state St. Decomposed into
the Bellman equation, the maximum state value achievable by
policy π for state S is:
V∗(S, π) = max
A
∑
S′
Pr{S ′|S,A} · (R(S,A)
+ γ ·V∗(S ′, π)) , (28)
where S ′ is the next state of S after choosing action A. In con-
trast, similar to Equation (28), the optimal state–action value
function can be defined as:
Q∗(S,A) =
∑
S′
Pr{S ′|S,A}
·
(
R(S,A) + γ ·max
A′
Q∗(S ′, A′)
)
, (29)
where S ′ is the next state of S after choosing action A, and A′ is
the offloading action performed under state S ′. Equations (28)
and (29) are the Bellman optimality equations. In Q-learning,
the policy π is updated using value iteration in a recursive way
based on the observation of system state S.
The prerequisite for using the critic network in the selection
of AGA is the convergence of the Q-function.
Theorem 1: The critic network of the DRGO algorithm con-
verges to the optimal Q-function.
Proof: Because the state space S and action space A
are finite, the state transition probabilities Pr{St+1|St, π(St)}
in Equation (26) are stationary, and all state–action pairs
{(St, At)|t ∈ N+} can be visited infinitely often. Given
(St, At, St+1, Rt), the update rule of the critic network in the
DRGO algorithm therefore is:
Q(St, At) = Q(St, At) + α(St, At)
×
[
Rt + γmax
b∈A
Q(St+1, b)−Q(St, At)
]
.
(30)
We subtract Q∗(St, At) from both sides, thus obtaining
Δ(St, At) = Q(St, At)−Q∗(St, At), (31)
which yields
Δ(St, At) = (1− α)Δ(St, At) + α(St, At)F (St, At), (32)
F (St, At) =
[
Rt + γmax
b∈A
Q(St+1, b)−Q∗(St, At)
]
. (33)
Because
∑∞
t=1 α is infinite and
∑∞
t=1 α
2 is finite, according to
[36], Δ(St, At) converges to zero w.p.1 if:
1) ‖E[F (St, At)|F ]‖∞ ≤ γ‖Δ(St, At)‖∞,with γ < 1.
2) var[F (St, At)|F ] ≤ C(1 + ‖Δ(St, At)‖2
∞),with C > 0
First, the following equation is derived.
‖E[F (St, At)|F ]‖∞ = Pr(St+1|St, At)F (St, At)
≤ γ‖Q(St, At)−Q∗(St, At)‖∞
= ‖Δ(St, At)‖∞.
(34)
Then, the following equation is obtained.
var[F (St, At)|F ] = var
[
Rt + γmax
b∈A
Q(St+1, b)|F
]
. (35)
Because Rt is bounded, the following is true.
var[F (St, At)|F ] ≤ C(1 + ‖Δ(St, At)‖2
∞). (36)
Here, C is a constant. Hence, Δ(St, At) converges to zero
w.p.1, which means the critic network of the DRGO algorithm
converges to the optimal Q-function Q∗(S,A). �
Therefore, we can use the critic network to estimate the per-
formance of each action in the candidate solution set. We denote
the optimal action A∗ as:
A∗ = arg max
Ai∈A
Q∗(St, Ai). (37)
D. Self-Adjusting K in AGA
In AGA, with a larger K, we can reach a better offloading
decision and achieve higher performance. However, a larger K
also leads to computational complexity and large time consump-
tion. Therefore, to balance the performance and complexity, we
propose an algorithm with self-adjusting K.
The rule of adjusting K is simple. After obtaining the optimal
action A∗ from the candidate solution set, we compare it with
the original output of the actor network A. If A is equal to A∗,
then subtract one from K; otherwise, we increase the value of
K based on the difference between A and A∗. The difference
can be measured by the L2-norm of A−A∗, which is denoted
as ‖A−A∗‖2.
In addition, to avoidK becoming too large, making AGA con-
sume too much time, we add a constraint K ≤ Kmax to limit the
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
8058 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 68, NO. 8, AUGUST 2019
Algorithm 1: Online Deep Reinforcement Learning Frame-
work with AGA in the Task Offloading System.
1: Initialize parameters of the online actor network and
critic network with weights θμ and θQ randomly.
2: Initialize parameters of the target actor network and
critic network with weights θμ
′ ← θμ and θQ
′ ← θQ.
3: Initialize the prioritized replay memoryM with the
size of x.
4: Set the max number of generations Kmax, the training
interval δ, the step count t, and ϕ used to measure the
loss value of the critic network.
5: Repeat
6: At the beginning of decision epoch t, JM takes system
state St as an input to the actor network and obtains
action At.
7: With probability 1− ε output action At. Otherwise, if
L(θQ
′
) ≤ ϕ, use AGA to replace At with the optimal
action A∗ of a generated solution set A; or replace At
with a random action if L(θQ
′
) > ϕ.
8: Execute action At, observe reward Rt, and observe
new state St+1
9: Store transition (St, At, Rt, St+1) in replay memory.
10: if t mod δ = 0 then
11: Select samples from replay memory and update θμ and
θQ using the Adam algorithm by minimizing the loss
function in Equation (40) and (41).
12: end if
13: Regularly update the target networks:
14:
θQ
′ ← τθQ + (1− τ)θQ
′
,
θμ
′ ← τθμ + (1− τ)θμ
′
.
15: Until A predefined stopping condition is satisfied.
maximum value ofK. Given a strictly monotonically increasing
functions φ, the updating rule of K is:
K =
{
max (0,K − 1) A∗ = A
min (K + φ(‖A−A∗‖2),Kmax) A∗ �= A.
(38)
E. Updating Policy With Prioritized Experience Replay
The DRGO algorithm regularly updates the parameters in its
deep neural networks based on the experience. It maintains a
replay memory with the finite size of x to store historical expe-
riences, which can be represented asM = {M1,M2, . . . ,Mx},
where Mi = (St, At, Rt, St+1) (Algorithm 1: line 9). Then, in
the training phase, we sample a mini-batch of transitions from
the replay memory to update parameters in the actor network and
the critic network toward maximizing the long-term reward.
Traditional deep reinforcement learning randomly selects a
batch of training samples from the replay memory, making it
difficult to learn valuable experience. Therefore, we organize
the replay memory with the SumTree structure. It arranges sam-
ples in the replay memory according to their priorities, which is
called prioritized experience replay [37]. As the loss value of a
sample increases, it becomes more likely to be used to update
the network. By selecting samples from replay memory with
higher priority, our model can learn more efficiently, and the
probability of the selecting sample i is:
P (i) =
pβi
∑
k p
β
k
, (39)
where pi > 0 is the priority of sample i, and the exponent β
denotes how much prioritization is used.
For the actor network, we denote samples selected from pri-
oritized replay memory as MT = {(St, At)|t ∈ T }, where T
represents the set of time indices. Using Adam optimizer, the pa-
rameters θμ of the actor network are updated in the direction of
minimizing the cost function L(θμ) (Algorithm 1: lines 13-14).
As mentioned, we represent actions in binary form. Therefore,
the loss function L(θμ) is defined by:
L(θμ) = EMT
[
At
T logμ(St) + (1−At)
T log(1− μ(St))
]
,
(40)
whereμ(St) denotes the output of the current actor network with
input St. The loss function L(θμ) is the averaged cross-entropy
loss of the selected memory set.
For the critic network, we denote samples selected from prior-
itized replay memory asM′
T = {(St, At, Rt, S
′
t)|t ∈ T }. Sim-
ilarly, we denote the loss function L(θQ) as follows.
L(θQ) = EM′
T
(
Rt + γmax
A′t
Q(S ′t, A
′
t)−Q(St, At)
)2
,
(41)
where γ is the discount value.
VI. SIMULATION
In this section, we evaluate the performance of our proposed
algorithm through numerical simulations.
A. Simulation Setup
We consider a scenario where EDs are randomly distributed
within an area of 500 m × 500 m. In addition, EDs are con-
nected through a multi-hop ad-hoc network based on the Wire-
lessHART protocol over IEEE 802.15.4 [38]. EDs can commu-
nicate with each other within the range of 100 m, so they are
divided into different levels according to their distances from
the JM. The channel bandwidths between EDs are 2.4 GHz.
The transmit powers of EDs are 0.5 W [39], while the additive
white Gaussian noise power is –120 dBm. The path loss ex-
ponent is −2. The maximum achievable rate (in bps) of ri for
downloading/uploading is set as 250 kbps [40]. For the interfer-
ence between EDs, we use the sum of the transmission powers
from all the interfering EDs as the interference strength [41]. In
addition, we study the surveillance and security systems for a
smart home in this simulation. Consequently, according to [38],
the task’s uploading data sizes follow the uniform distribution
on [100, 1000] KB, while the transmission of the downloading
data is ignored because the amount of returning data is usually
small.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
QIU et al.: ONLINE DEEP REINFORCEMENT LEARNING FOR COMPUTATION OFFLOADING 8059
For the task execution, the required CPU cycles of data pro-
cessing tasks li follow the uniform distribution on [20, 40]
Gcycles [3]. And the CPU frequencies of ED fi range from
10 GHz to 20 GHz. For simplicity, there are five different
VM level for data processing tasks in the servers, whose CPU
frequencies are 50 GHz, 100 GHz, 150 GHz, 200 GHz, and
250 GHz, respectively. Then, the prices for these five dif-
ferent VM levels are 1× 10−6 tokens/Gcycle, 2× 10−6 to-
kens/Gcycle, 3× 10−6 tokens/Gcycle, 4× 10−6 tokens/Gcycle,
and 5× 10−6 tokens/Gcycle, respectively. The arrival rate of
data processing tasks is 0.6. The task queue lengths of EDs and
the servers are 2 tasks and 10 tasks, respectively. The deadline
of each data processing task di follows the uniform distribution
on [T, 5T], where T is the decision period. To compute the trans-
mit cost, we set each energy unit corresponds to 2× 10−3 joules
[42]. To unitize the cost, we set that each joule costs 1× 10−4
tokens [43]. The rewards for successfully finishing tasks follow
the uniform distribution on [1× 10−6, 1× 10−5] tokens.
For the mining tasks, we set the parameters in the simula-
tions according to [31]. The data size of the block to be mined
is [5,10] Kb [16]. Similar to data processing tasks, there are
five different VM levels for mining tasks, whose hash pow-
ers are 20 MHash/s, 40 MHash/s, 60 MHash/s, 80 MHash/s,
and 100 MHash/s, respectively [34]. In addition, the prices are
2× 10−5 tokens, 4× 10−5 tokens, 6× 10−5 tokens, 8× 10−5
tokens, 1× 10−4 tokens respectively. The hash power of the
blockchain network follows a uniform distribution on [1× 103,
1× 105] GHash/s. In addition, the miner that first solves the
complex mathematical problem and achieves consensus is re-
warded with R = 30 tokens.
For the design of the DNN, we use two hidden layers consist-
ing of 200 neurons and 100 neurons. The activation functions
of hidden layers are Rectified Linear Unit (ReLU). In addition,
as the action uses binary encoding, the activation functions of
output layers in actor networks are Sigmoid functions. We set
the size of the replay memory as 10240, the batch size as 128,
and the training interval as 10.
For the parameters of AGA, the initial probability of crossover
pc is 0.8, and the initial mutation rate pm is 0.01. We assign k2
and k4 a value of 0.5 and assign k1 and k3 a value of 0.8 in
Equation (24) and (25). We set the initial value of K as 10 and
Kmax as 100. During each successive generation, AGA uses
the roulette-wheel selection as the fitness selection because of
its similarity [44]. However, it is worth noting that there is no
universal selection method for all problems. Thus, numerical
experiments are required to obtain the best selection methods.
B. Simulation Design
1) Evaluation Metrics:
� Average cost: This can be calculated by the ratio of system
cost to the number of EDs in the system.
� Task drop rate: This is the percentage of the failed data
processing tasks, which is used to evaluate the decision of
resource allocation.
� Average transmit time: This is the average time that spent
on uploading the required data and program code of the
Fig. 4. The convergence property of the DRGO algorithm.
processing tasks. We can use it to evaluate the degree of
the congestion in transmission links.
2) Baseline Algorithm:
� Greedy Algorithm: This algorithm randomly generates 107
actions and selects the best one.
� Genetic Algorithm: Genetic algorithms are commonly
used to search high-quality solutions for problems with
large search space. We set the size of the population as
200, the crossover probability as 0.7, the mutation proba-
bility as 0.05, and the maximum generation as 10000.
� DDPG: DDPG is a state-of-art DRL method that has suc-
cessfully solved many challenging problems across a vari-
ety of domains with large action spaces [30].
C. Convergence Performance
In this simulation, we evaluate the convergence property of the
DRGO algorithm using the above parameter settings. The con-
vergence of the DRGO algorithm is the crucial property to ob-
tain a policy which maps states to the optimal actions. Generally,
a neural network is considered to converge when the learning
curve becomes flat. Therefore, we plot the training loss L(θμ)
in each training episode under the scenario where the number
of EDs is 50. The results in Fig. 4 reveal the convergence be-
haviors of our algorithm, which show that the DRGO algorithm
converges after 105 episodes. Therefore, the DRGO algorithm
converges at an acceptable speed.
D. Performance Analysis
1) Performance Under Different Numbers of EDs: First, we
study the performance of our algorithm under different numbers
of EDs. Fig. 5(a) plots the average long-term cost under scenar-
ios where the numbers of EDs ranging from 5 to 50. The figure
shows that our algorithm outperforms the other three benchmark
policies. In particular, the average costs of the greedy algorithm
and the genetic algorithm increase relatively fast as the number
of EDs increases, while the average cost of the DRGO algorithm
shows no clear sign of increasing. This is because the increasing
number of EDs leads to a larger search space. And in Fig. 5(b),
we plot the task drop rate over different number of EDs, which
shows the same pattern as Fig. 5(a). This is because the increase
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
8060 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 68, NO. 8, AUGUST 2019
Fig. 5. Simulation results under: (a) and (b) different numbers of EDs; (c) and (d) different CPU cycles; and (e) and (f) different uploading data sizes.
of EDs inevitably increases the data transmitted in the transmis-
sion links. In this context, it takes more time on data transmis-
sion when ED chooses to offload tasks to the server, which may
cause execution failure. Even so, the DRGO algorithm has the
lowest task drop rate and shows no sign of increasing because
it can schedule the routing path to reduce (or even avoid) data
congestion.
2) Performance Under Different CPU Cycles: Next, we con-
sider the impact of different CPU cycles required to execute
the tasks. In this simulation, the required CPU cycles li of the
data processing tasks range from 20 Gcycles to 40 Gcycles. In
Fig. 5(c), we plot the average cost of all four algorithms. Because
larger CPU cycles often require more energy to finish tasks, the
average long-term cost of all four algorithms increases. How-
ever, the average cost of the DRGO algorithm increases rela-
tively slowly compared with three baseline algorithms, indicat-
ing that the DRGO algorithm can make intelligent offloading
decisions to reduce the execution cost. In addition, Fig. 5(d)
plots the task drop rate, which shows that the DRGO algorithm
achieves the lowest task drop rate. This is because the intelligent
decisions about tasks executions decrease the execution delay.
3) Performance Under Different Data Sizes: In this simula-
tion, we consider how the size of the required data during execu-
tion impacts the performance. We record the average cost of the
system for various data sizes ranging from 100 KB to 1000 KB.
In Fig. 5(e), we plot the average long-term cost under different
data sizes. The figure shows that the average long-term cost of the
DRGO algorithm increases much more slowly compared with
the greedy algorithm and the genetic algorithms. There are two
reasons for this difference. On the one hand, a larger data size re-
quires more energy on transmission. The DRGO algorithm can
arrange a data transmission path that avoids data congestion and
unnecessary cost. To prove it, we plot the average transmit time
under different data size in Fig. 5(f). We can see that the DRGO
algorithm has the shortest transmit time. In addition, it is worth
noting that the gap between the DRGO algorithm and the other
three algorithms grows steadily, indicating that the DRGO al-
gorithm brings greater performance gains under scenarios with
a larger size of transmission data. On the other hand, a larger
data size requires more space on memory during queuing. How-
ever, the memory space is not infinite in our considered scenario.
Therefore, a poor offloading policy faces a task-failure penalty
when if is not enough space for new tasks.
E. Scalability Analysis
In general, the dimension of a DNN input is an invariant value,
while the adding and exiting of EDs are common for MEC. To
support that, we regard the remaining ED states in the DNN as
empty states if the number of EDs is smaller than the preset
training value. For example, given the preset training value Vt
and the actual number of EDsVa (Va < Vt), we set the remaining
Vt − Va states to be 0 and drop the remaining Vt − Va actions in
the output of the DRGO algorithm. On the other hand, if Va >
Vt, we divide the collected EDs’ states into several parts and
input these parts to the actor network in turn. Therefore, in this
simulation, our goal is to validate the scalability of the DRGO
algorithm, which is the performance of the DRGO algorithm
under scenarios where the number of EDs is larger or smaller
than the preset value in training.
First, we apply the DRGO algorithm to environments with
fewer EDs than the preset value used in the training phase. And
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
QIU et al.: ONLINE DEEP REINFORCEMENT LEARNING FOR COMPUTATION OFFLOADING 8061
Fig. 6. Scalability Analysis.
we set the number of EDs in the environment as 20. As shown in
Fig. 6(a), the preset values of EDs in the DRGO algorithm and
DDPG algorithm range from 20 to 40 and the baseline algorithm
is the genetic algorithm, which achieves higher performance
than the greedy algorithm in the previous simulations. We can
see that although larger preset training values increase the cost,
the average costs of the DRGO algorithm are still lower than the
greedy algorithm.
Next, we apply the DRGO algorithm to environments with
more EDs than the preset value. In this simulation, we set the
number of EDs as 50 and plot the average cost in Fig. 6(b). The
figure shows that although the DDPG algorithm suffers from
higher cost when the preset value in training is lower than 36,
the DRGO algorithm still performs well and reaches a lower
cost than the genetic algorithm. In Fig. 6(c), we go one step
further and plot the task-drop rate of all three algorithms under
the same environment setting. The figure shows that the task-
drop rate of the DRGO algorithm remains the smallest. These
simulation results indicate that our proposed DRGO algorithm
can quickly adapt to environments where the number of EDs is
larger or smaller than the preset values in the training phase.
VII. CONCLUSION
In this paper, we propose a deep reinforcement learning-
based online computation offloading approach for blockchain-
empowered MEC, in which both mining tasks and data
processing tasks are considered. To achieve long-term offload-
ing performance, our DRGO algorithm uses model-free deep
reinforcement learning to adapt to highly dynamic environments
and maximize the long-term reward. In particular, to overcome
the slow convergence caused by the high-dimensional action
space, the DRGO algorithm takes advantage of the genetic algo-
rithms into deep reinforcement learning during the exploration
process, survives the curse of high-dimensional action space,
and converges at a acceptable speed. Simulation results have
shown that the DRGO algorithm achieves better performance
compared with three representative benchmark policies and
shows strong robustness under various environments. In future
work, we plan to study the exploration mechanism to further
improve the efficiency of the exploration, which is the key to
solve the convergence problem of deep reinforcement learning.
REFERENCES
[1] Z. Hong, H. Huang, S. Guo, W. Chen, and Z. Zheng, “QOS-aware coop-
erative computation offloading for robot swarms in cloud robotics,” IEEE
Trans. Veh. Technol., vol. 68, no. 4, pp. 4027–4041, Apr. 2019.
[2] J. Kang et al., “Blockchain for secure and efficient data sharing in vehicular
edge computing and networks,” IEEE Internet Things J., vol. 6, no. 3,
pp. 4660–4670, Jun. 2019.
[3] H. Guo, J. Liu, J. Zhang, W. Sun, and N. Kato, “Mobile-edge computation
offloading for ultradense IOT networks,” IEEE Internet Things J., vol. 5,
no. 6, pp. 4977–4988, Dec. 2018.
[4] Z. Li, Z. Yang, S. Xie, W. Chen, and K. Liu, “Credit-based payments
for fast computing resource trading in edge-assisted Internet of Things,”
IEEE Internet Things J., vol. 6, no. 4, pp. 6606–6617, Aug. 2019. doi:
10.1109/JIOT.2019.2908861.
[5] Z. Li, Z. Yang, and S. Xie, “Computing resource trading for edge-cloud-
assisted Internet of Things,” IEEE Trans. Ind. Inform., vol. 15, no. 6,
pp. 3661–3669, Jun. 2019.
[6] W. Chen et al., “Cooperative and distributed computation offloading
for blockchain empowered industrial Internet of Things,” IEEE Internet
Things J., to be published, doi: 10.1109/JIOT.2019.2918296.
[7] J. Kang, Z. Xiong, D. Niyato, P. Wang, D. Ye, and D. I. Kim, “Incentiviz-
ing consensus propagation in proof-of-stake based consortium blockchain
networks,” IEEE Wireless Commun. Lett., vol. 8, no. 1, pp. 157–160, Feb.
2019.
[8] Z. Xiong, S. Feng, W. Wang, D. Niyato, P. Wang, and Z. Han, “Cloud/fog
computing resource management and pricing for blockchain networks,”
IEEE Internet Things J., vol. 6, no. 3, pp. 4585–4600, Jun. 2019.
[9] Y. Wang, K. Wang, H. Huang, T. Miyazaki, and S. Guo, “Traffic and com-
putation co-offloading with reinforcement learning in fog computing for
industrial applications,” IEEE Trans. Ind. Inform., vol. 15, no. 2, pp. 976–
986, Feb. 2019.
[10] V. Mnih et al., “Human-level control through deep reinforcement learn-
ing,” Nature, vol. 518, no. 7540, pp. 529–533, Feb. 2015.
[11] H. Huang, J. Yang, H. Huang, Y. Song, and G. Gui, “Deep learning for
super-resolution channel estimation and DOA estimation based massive
MIMO system,” IEEE Trans. Veh. Technol., vol. 67, no. 9, pp. 8549–8560,
Sep. 2018.
[12] Y. Kawamoto, H. Takagi, H. Nishiyama, and N. Kato, “Efficient resource
allocation utilizing Q-learning in multiple UA communications,” IEEE
Trans. Netw. Sci. Eng., to be published, doi: 10.1109/TNSE.2018.2842246.
[13] Y. Zhang, J. Yao, and H. Guan, “Intelligent cloud resource management
with deep reinforcement learning,” IEEE Cloud Comput., vol. 4, no. 6,
pp. 60–69, Nov. 2017.
[14] M. Srinivas and L. M. Patnaik, “Adaptive probabilities of crossover and
mutation in genetic algorithms,” IEEE Trans. Syst., Man, Cybern., vol. 24,
no. 4, pp. 656–667, Apr. 1994.
[15] W. Chen, B. Liu, H. Huang, S. Guo, and Z. Zheng, “When UAV swarm
meets edge-cloud computing: The QOS perspective,” IEEE Netw., vol. 33,
no. 2, pp. 36–43, Mar. 2019.
[16] M. Liu, F. R. Yu, Y. Teng, V. C. M. Leung, and M. Song, “Computation of-
floading and content caching in wireless blockchain networks with mobile
edge computing,” IEEE Trans. Veh. Technol., vol. 67, no. 11, pp. 11008–
11021, Nov. 2018.
[17] W. Chen, Y. Yaguchi, K. Naruse, Y. Watanobe, and K. Nakamura,
“QOS-aware robotic streaming workflow allocation in cloud robotics
systems,” IEEE Trans. Services Comput., to be published, doi:
10.1109/TSC.2018.2803826.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
https://dx.doi.org/10.1109/JIOT.2019.2908861
https://dx.doi.org/10.1109/JIOT.2019.2918296
https://dx.doi.org/10.1109/TNSE.2018.2842246
https://dx.doi.org/10.1109/TSC.2018.2803826
8062 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 68, NO. 8, AUGUST 2019
[18] D. Chatzopoulos, M. Ahmadi, S. Kosta, and P. Hui, “Flopcoin: A cryp-
tocurrency for computation offloading,” IEEE Trans. Mobile Comput.,
vol. 17, no. 5, pp. 1062–1075, May 2018.
[19] Y. Wang, M. Liu, J. Yang, and G. Gui, “Data-driven deep learning for
automatic modulation recognition in cognitive radios,” IEEE Trans. Veh.
Technol., vol. 68, no. 4, pp. 4074–4077, Apr. 2019.
[20] G. Gui, H. Huang, Y. Song, and H. Sari, “Deep learning for an effec-
tive nonorthogonal multiple access scheme,” IEEE Trans. Veh. Technol.,
vol. 67, no. 9, pp. 8440–8450, Sep. 2018.
[21] M. Liu, R. Yu, Y. Teng, V. Leung, and M. Song, “Performance optimization
for blockchain-enabled industrial Internet of Things (IIOT) systems: A
deep reinforcement learning approach,” IEEE Trans. Ind. Inform., vol. 15,
no. 6, pp. 3559–3570, Jun. 2019.
[22] X. He, K. Wang, H. Huang, T. Miyazaki, Y. Wang, and S. Guo, “Green
resource allocation based on deep reinforcement learning in content-
centric IoT,” IEEE Trans. Emerg. Topics Comput., to be published, doi:
10.1109/TETC.2018.2805718.
[23] M. Liu, T. Song, and G. Gui, “Deep cognitive perspective: Resource al-
location for noma-based heterogeneous IoT with imperfect SIC,” IEEE
Internet Things J., vol. 6, no. 2, pp. 2885–2894, Apr. 2019.
[24] Q. Qi et al., “Knowledge-driven service offloading decision for vehicular
edge computing: A deep reinforcement learning approach,” IEEE Trans.
Veh. Technol., vol. 68, no. 5, pp. 4192–4203, May 2019.
[25] M. Min et al., “Learning-based privacy-aware offloading for healthcare IoT
with energy harvesting,” IEEE Internet Things J., vol. 6, no. 3, pp. 4307–
4316, Jun. 2019.
[26] M. Min, L. Xiao, Y. Chen, P. Cheng, D. Wu, and W. Zhuang, “Learning-
based computation offloading for IoT devices with energy harvesting,”
IEEE Trans. Veh. Technol., vol. 68, no. 2, pp. 1930–1941, Feb. 2019.
[27] Y. Wei, F. R. Yu, M. Song, and Z. Han, “User scheduling and resource
allocation in HetNets with hybrid energy supply: An actor-critic reinforce-
ment learning approach,” IEEE Trans. Wireless Commun., vol. 17, no. 1,
pp. 680–692, Jan. 2018.
[28] H. Zhu, Y. Cao, X. Wei, W. Wang, T. Jiang, and S. Jin, “Caching transient
data for Internet of Things: A deep reinforcement learning approach,”
IEEE Internet Things J., vol. 6, no. 2, pp. 2074–2083, Apr. 2019.
[29] V. Mnih et al., “Asynchronous methods for deep reinforcement learning,”
Jun. 2016. [Online]. Available: https://arxiv.org/abs/1602.01783
[30] T. P. Lillicrap et al., “Continuous control with deep reinforcement learn-
ing,” Feb. 2016. [Online]. Available: https://arxiv.org/abs/1509.02971
[31] S. Biswas, K. Sharif, F. Li, B. Nour, and Y. Wang, “A scalable blockchain
framework for secure transactions in IoT,” IEEE Internet Things J., vol. 6,
no. 3, pp. 4650–4659, Jun. 2019.
[32] H. Liu, S. Liu, and K. Zheng, “A reinforcement learning-based resource
allocation scheme for cloud robotics,” IEEE Access, vol. 6, pp. 17215–
17222, 2018.
[33] H. Nishiyama, M. Ito, and N. Kato, “Relay-by-smartphone: Realizing mul-
tihop device-to-device communications,” IEEE Commun. Mag., vol. 52,
no. 4, pp. 56–65, Apr. 2014.
[34] A. P. Ozisik, G. Bissias, and B. N. Levine, “Estimation of miner hash
rates and consensus on blockchains,” Jul. 2017. [Online]. Available:
https://arxiv.org/abs/1707.00082
[35] M. A. Ferrag, M. Derdour, M. Mukherjee, A. Derhab, L. Maglaras, and
H. Janicke, “Blockchain technologies for the Internet of Things: Research
issues and challenges,” IEEE Internet Things J., vol. 6, no. 2, pp. 2188–
2204, Apr. 2019.
[36] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Mach. Learn., vol. 8, no. 3,
pp. 279–292, May 1992.
[37] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
replay,” Feb. 2016. [Online]. Available: https://arxiv.org/abs/1511.05952
[38] T. Mendes, R. Godina, E. Rodrigues, J. Matias, and J. Catalão, “Smart
home communication technologies and applications: Wireless protocol
assessment for home area network resources,” Energies, vol. 8, no. 7,
pp. 7279–7311, Jul. 2015.
[39] J. Liu, N. Kato, J. Ma, and N. Kadowaki, “Device-to-device communica-
tion in lte-advanced networks: A survey,” IEEE Commun. Surveys Tut.,
vol. 17, no. 4, pp. 1923–1940, Oct.– Dec. 2015.
[40] J. Benoit, A. Yao, L. Saladis, and Y. Zheng, “Performance evaluations of
multi-hop wirelessHART network and 6LoWPAN using different topolo-
gies,” in Proc. Global Smart Industry Conf., Nov. 2018, pp. 1–5.
[41] C. Wang, F. R. Yu, C. Liang, Q. Chen, and L. Tang, “Joint computa-
tion offloading and interference management in wireless cellular networks
with mobile edge computing,” IEEE Trans. Veh. Technol., vol. 66, no. 8,
pp. 7432–7445, Aug. 2017.
[42] F. Tang, Z. M. Fadlullah, N. Kato, F. Ono, and R. Miura, “AC-POCA:
Anticoordination game based partially overlapping channels assignment
in combined UAV and D2D-based networks,” IEEE Trans. Veh. Technol.,
vol. 67, no. 2, pp. 1672–1683, Feb. 2018.
[43] J. Liu, N. Kato, H. Ujikawa, and K. Suzuki, “Device-to-device communi-
cation for mobile multimedia in emerging 5g networks,” ACM Trans. Mul-
timedia Comput. Commun. Appl., vol. 12, no. 5s, Sep. 2016, Art. no. 75.
[44] M. Madiafi and K. Jebari, “Selection methods for genetic algorithms,” Int.
J. Emerg. Sci., vol. 3, pp. 333–344, Dec. 2013.
Xiaoyu Qiu is currently pursuing the B.S. degree
with the School of Data and Computer Science, Sun
Yat-Sen University, Guangzhou, China. His research
interests include edge computing, cloud computing,
cloud robotics, and computation offloading, with em-
phasis on artificial intelligence in edge/cloud comput-
ing.
Luobin Liu is currently pursuing the B.Eng. degree
in information security with the School of Data and
Computer Science, Sun Yat-Sen University. His re-
search interests include game theory and edge com-
puting.
Wuhui Chen received the bachelor’s degree from
Northeast University, Shenyang, China, in 2008 and
the master’s and Ph.D. degrees from the Univer-
sity of Aizu, Aizu-Wakamatsu, Japan, in 2011 and
2014, respectively. From 2014 to 2016, he was a Re-
search Fellow with the Japan Society for the Pro-
motion of Science, Japan. From 2016 to 2017, he
was a Researcher with the University of Aizu. He
is currently an Associate Professor with Sun Yat-Sen
University, Guangzhou, China. His research interests
include edge/cloud computing, cloud robotics, and
blockchain.
Zicong Hong is currently pursuing the B.Eng. de-
gree in software engineering with the School of Data
and Computer Science, Sun Yat-sen University. His
research interest includes game theory, Internet of
Things, blockchain, and edge/cloud computing.
Zibin Zheng is currently a Professor with the School
of Data and Computer Science, Sun Yat-Sen Univer-
sity, Guangzhou, China. His research interests include
service computing and cloud computing. Prof. Zheng
was the recipient of the Outstanding Ph.D. Thesis
Award of the Chinese University of Hong Kong in
2012, the Association for Computing Machinerys
Special Interest Group on Software Engineering Dis-
tinguished Paper Award at the International Confer-
ence on Science and Engineering in 2010, the Best
Student Paper Award at the International Conference
on Web Services in 2010, and the IBM Ph.D. Fellowship Award in 2010. He
served as Program Committee (PC) member of the IEEE International Con-
ference on Cloud Computing, International Conference on Web Services, iIn-
ternational Conference on Service Computing (SCC), International Conference
on Service-Oriented Computing, International Symposium on Service-Oriented
System Engineering (SOSE), etc.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:20:05 UTC from IEEE Xplore.  Restrictions apply. 
https://dx.doi.org/10.1109/TETC.2018.2805718
<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles true
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.4
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /sRGB
  /DoThumbnails true
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo true
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts false
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
    /Algerian
    /Arial-Black
    /Arial-BlackItalic
    /Arial-BoldItalicMT
    /Arial-BoldMT
    /Arial-ItalicMT
    /ArialMT
    /ArialNarrow
    /ArialNarrow-Bold
    /ArialNarrow-BoldItalic
    /ArialNarrow-Italic
    /ArialUnicodeMS
    /BaskOldFace
    /Batang
    /Bauhaus93
    /BellMT
    /BellMTBold
    /BellMTItalic
    /BerlinSansFB-Bold
    /BerlinSansFBDemi-Bold
    /BerlinSansFB-Reg
    /BernardMT-Condensed
    /BodoniMTPosterCompressed
    /BookAntiqua
    /BookAntiqua-Bold
    /BookAntiqua-BoldItalic
    /BookAntiqua-Italic
    /BookmanOldStyle
    /BookmanOldStyle-Bold
    /BookmanOldStyle-BoldItalic
    /BookmanOldStyle-Italic
    /BookshelfSymbolSeven
    /BritannicBold
    /Broadway
    /BrushScriptMT
    /CalifornianFB-Bold
    /CalifornianFB-Italic
    /CalifornianFB-Reg
    /Centaur
    /Century
    /CenturyGothic
    /CenturyGothic-Bold
    /CenturyGothic-BoldItalic
    /CenturyGothic-Italic
    /CenturySchoolbook
    /CenturySchoolbook-Bold
    /CenturySchoolbook-BoldItalic
    /CenturySchoolbook-Italic
    /Chiller-Regular
    /ColonnaMT
    /ComicSansMS
    /ComicSansMS-Bold
    /CooperBlack
    /CourierNewPS-BoldItalicMT
    /CourierNewPS-BoldMT
    /CourierNewPS-ItalicMT
    /CourierNewPSMT
    /EstrangeloEdessa
    /FootlightMTLight
    /FreestyleScript-Regular
    /Garamond
    /Garamond-Bold
    /Garamond-Italic
    /Georgia
    /Georgia-Bold
    /Georgia-BoldItalic
    /Georgia-Italic
    /Haettenschweiler
    /HarlowSolid
    /Harrington
    /HighTowerText-Italic
    /HighTowerText-Reg
    /Impact
    /InformalRoman-Regular
    /Jokerman-Regular
    /JuiceITC-Regular
    /KristenITC-Regular
    /KuenstlerScript-Black
    /KuenstlerScript-Medium
    /KuenstlerScript-TwoBold
    /KunstlerScript
    /LatinWide
    /LetterGothicMT
    /LetterGothicMT-Bold
    /LetterGothicMT-BoldOblique
    /LetterGothicMT-Oblique
    /LucidaBright
    /LucidaBright-Demi
    /LucidaBright-DemiItalic
    /LucidaBright-Italic
    /LucidaCalligraphy-Italic
    /LucidaConsole
    /LucidaFax
    /LucidaFax-Demi
    /LucidaFax-DemiItalic
    /LucidaFax-Italic
    /LucidaHandwriting-Italic
    /LucidaSansUnicode
    /Magneto-Bold
    /MaturaMTScriptCapitals
    /MediciScriptLTStd
    /MicrosoftSansSerif
    /Mistral
    /Modern-Regular
    /MonotypeCorsiva
    /MS-Mincho
    /MSReferenceSansSerif
    /MSReferenceSpecialty
    /NiagaraEngraved-Reg
    /NiagaraSolid-Reg
    /NuptialScript
    /OldEnglishTextMT
    /Onyx
    /PalatinoLinotype-Bold
    /PalatinoLinotype-BoldItalic
    /PalatinoLinotype-Italic
    /PalatinoLinotype-Roman
    /Parchment-Regular
    /Playbill
    /PMingLiU
    /PoorRichard-Regular
    /Ravie
    /ShowcardGothic-Reg
    /SimSun
    /SnapITC-Regular
    /Stencil
    /SymbolMT
    /Tahoma
    /Tahoma-Bold
    /TempusSansITC
    /TimesNewRomanMT-ExtraBold
    /TimesNewRomanMTStd
    /TimesNewRomanMTStd-Bold
    /TimesNewRomanMTStd-BoldCond
    /TimesNewRomanMTStd-BoldIt
    /TimesNewRomanMTStd-Cond
    /TimesNewRomanMTStd-CondIt
    /TimesNewRomanMTStd-Italic
    /TimesNewRomanPS-BoldItalicMT
    /TimesNewRomanPS-BoldMT
    /TimesNewRomanPS-ItalicMT
    /TimesNewRomanPSMT
    /Times-Roman
    /Trebuchet-BoldItalic
    /TrebuchetMS
    /TrebuchetMS-Bold
    /TrebuchetMS-Italic
    /Verdana
    /Verdana-Bold
    /Verdana-BoldItalic
    /Verdana-Italic
    /VinerHandITC
    /Vivaldii
    /VladimirScript
    /Webdings
    /Wingdings2
    /Wingdings3
    /Wingdings-Regular
    /ZapfChanceryStd-Demi
    /ZWAdobeF
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 150
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages false
  /ColorImageDownsampleType /Bicubic
  /ColorImageResolution 900
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.00111
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.40
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 150
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages false
  /GrayImageDownsampleType /Bicubic
  /GrayImageResolution 1200
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.00083
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.40
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 1200
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages false
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 1600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.00063
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False
  /CreateJDFFile false
  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Suggested"  settings for PDF Specification 4.0)
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice