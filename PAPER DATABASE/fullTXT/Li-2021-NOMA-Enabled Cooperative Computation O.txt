NOMA-Enabled Cooperative Computation Offloading for Blockchain-Empowered Internet of Things: A Learning Approach
2364 IEEE INTERNET OF THINGS JOURNAL, VOL. 8, NO. 4, FEBRUARY 15, 2021
NOMA-Enabled Cooperative Computation
Offloading for Blockchain-Empowered Internet of
Things: A Learning Approach
Zhenni Li , Member, IEEE, Minrui Xu, Jiangtian Nie , Student Member, IEEE, Jiawen Kang,
Wuhui Chen , Member, IEEE, and Shengli Xie , Fellow, IEEE
Abstract—Blockchain technologies allow the Internet of Things
(IoT) to build trust among various interest parties. For the
resource-limited IoT devices, offloading computation-intensive
tasks (blockchain verification and mining tasks, and data pro-
cess tasks) to edge servers for execution is considered as a
promising solution in mobile-edge computing. However, conven-
tional methods (such as linear programming or game theory)
for the computation offloading problem cannot achieve long-
term performance while the existing deep reinforcement learning
(DRL)-based algorithms suffer from slow convergence, lack
of robustness, and unstable performance. In this article, we
propose a multiagent DRL framework to achieve long-term
performance for cooperative computation offloading, in which
a scatter network is adopted to improve its stability and league
learning is introduced for agents to explore the environment col-
laboratively for fast convergence and robustness. First, we study
the nonorthogonal multiple access-enabled cooperative compu-
tation offloading problem and formulate the joint problem as
a Markov decision process by considering both the blockchain
mining tasks and data processing tasks. Second, to avoid useless
exploration and unstable performance, we initially train an intel-
ligent agent represented by scatter networks using conventional
expert strategies. Third, in order to enhance the performance,
we subsequently establish a hierarchical league where agents col-
laborate with others to explore the environment. Finally, our
experimental results demonstrate that our algorithm could per-
form better in terms of reducing energy cost and delay cost, and
Manuscript received March 29, 2020; revised July 4, 2020; accepted
August 9, 2020. Date of publication August 14, 2020; date of current ver-
sion February 4, 2021. This work was supported in part by the National
Natural Science Foundation of China under Grant 61803096, Grant 61722304,
Grant 61703113, and Grant 61727810; in part by the Science and Technology
Plan Project of Guangzhou under Grant 202002030289; in part by the Key
Areas of Research and Development Plan Project of Guangdong under Grant
2019B010147001; and in part by the Major Research Project on Industry
Technology of Guangzhou under Grant 201902020014. (Corresponding
author: Shengli Xie.)
Zhenni Li is with the School of Automation and Guangdong Key Laboratory
of IoT Information Technology, Guangdong University of Technology,
Guangzhou 510006, China (e-mail: lizhenni2012@gmail.com).
Minrui Xu and Wuhui Chen are with the School of Data and Computer
Science, Sun Yat-sen University, Guangzhou 510006, China (e-mail:
xumr3@mail2.sysu.edu.cn; chenwuhui21@gmail.com).
Jiangtian Nie and Jiawen Kang are with the Energy Research
Institute @ NTU, Nanyang Technological University, Singapore (e-mail:
jnie001@e.ntu.edu.sg; kavinkang@ntu.edu.sg).
Shengli Xie is with the Guangdong–HongKong–Macao Joint Laboratory
for Smart Discrete Manufacturing and 111 Center for Intelligent Batch
Manufacturing Based on IoT Technology, Guangdong University of
Technology, Guangzhou 510006, China (e-mail: shlxie@gdut.edu.cn).
Digital Object Identifier 10.1109/JIOT.2020.3016644
shortening almost 60% of the training time compared with the
state-of-the-art approaches.
Index Terms—Blockchain, cooperative computation offloading,
deep reinforcement learning (DRL), edge computing.
I. INTRODUCTION
BLOCKCHAIN is an emerging decentralized and
autonomous technology that can assist the Internet of
Things (IoT) to build trust among multiple parties with
conflicting interests [1]–[4]. However, the consensus of
blockchain requires lots of computing resources, leading
to IoT devices’ lack of computing resources. To alleviate
the tension between resource-hungry mining tasks and
resource-constrained IoT devices, mobile-edge computing
(MEC) has been proposed as an enabler, which can provide
the computation resource to IoT devices with low latency [5].
MEC is to deploy the cloud-computing capabilities to the
edge of a pervasive radio network, so that IoT devices can
offload the computation via nonorthogonal multiple access
(NOMA) to the edge servers for capability augmentation
[6], [7]. With the objective of enabling multiple users to
share the same time–frequency resource, NOMA can enhance
MEC to serve a large number of users by supporting
massive connectivity. However, to make the offloading
decision in NOMA-enabled multihop cooperative computing
environments is a challenging issue.
To solve the computation offloading problem, conventional
methods, such as linear programming and game theory are
widely investigated [8]. For example, the multihop cooperative
computation offloading optimization problem was formulated
as a game for robot swarms [9]. However, there are some
drawbacks to these conventional methods. First, they often
presuppose that the system environment is determined, but
in reality, it is a turntable in dynamic balance. Second, the
solutions based on conventional methods ignore the essen-
tial that the perception of devices in the system is dynamic,
and moreover, the interaction between devices and the envi-
ronment is nondeterminism. Third, conventional methods are
often myopic that they seek the optimal solutions for one-slot
systems, however, a long-term return optimization is what the
systems really demanded.
2327-4662 c© 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
https://orcid.org/0000-0001-8098-0341
https://orcid.org/0000-0003-1414-0621
https://orcid.org/0000-0003-4430-7904
https://orcid.org/0000-0003-2041-5214
LI et al.: NOMA-ENABLED COOPERATIVE COMPUTATION OFFLOADING FOR BLOCKCHAIN-EMPOWERED IoT 2365
To overcome these challenges of the conventional methods,
deep reinforcement learning (DRL) is developed [10]. DRL
uses the Markov decision process (MDP) framework to define
the interaction between the intelligent agent and environment
in terms of states, actions, and rewards [11], where the intel-
ligent agent is a parameterized approximator that addresses
the classical “curse of dimensionality” in the control theory.
For example, a continuous action space-based DRL approach
named deep deterministic policy gradient (DDPG) is proposed
by Chen and Wang [12] to learn efficient computation offload-
ing policies independently at each mobile user. However,
there are still some urgent issues [13] that need to be paid
attention to.
1) The existing DRL-based algorithms take a long clock-
runtime to achieve convergence because they initialize
their agents with the random initialization, which leads
to lots of useless exploration of agents in the initial phase
of training. It reduces the availability of DRL-based
methods in practical applications since long training
time makes them unable to adjust to the changes in the
system in time.
2) The existing DRL-based works to address the MEC
problems are often based on the single-agent learning
algorithm. The final performances of their algorithms
are affected greatly by different superparameters such
as random seed, which is shown in the agent converges
in the local optimal solution.
3) The performances of the existing DRL-based algorithms
fluctuate sharply with the dynamic changes of the envi-
ronment. During their construction of intelligent agents’
neural networks, a tremendous difference in the numer-
ical value and meaning between the computation states
and communication states will affect the performances
of the existing DRL-based algorithm.
To address these issues, we propose a multiagent DRL
framework to minimize the long-term cost of cooperative com-
putation offloading, where the scatter network is adopted to
improve its stability, and moreover, league learning is intro-
duced for agents to explore the environment collaboratively
for fast convergence and robustness. To address the first issue,
we devote our effort to balance exploration and exploita-
tion. By using the existing expert strategies, an intelligent
agent avoids useless exploration arising from random initial-
ization at the initial phase of training. We initially train our
intelligent agent using the strategies designed for the OMA-
enabled system. To address the second issue, our intelligent
agent is empowered with a multiagent framework called league
learning, which allows hierarchical agents differing from their
desires to explore to interact with the environment collabo-
ratively. By training on the experiences from the league, the
intelligent agent can discover more novel strategies and has
robust under different super parameters. To address the third
issue, in order to achieve stable performance in spite of the
dynamics in the environment, we construct scatter networks
for an intelligent agent to distinguish information observed
from the environment and actions.
The main contributions of this article can be summarized
as follows.
1) We design an efficient multiagent DRL algorithm, which
allows hierarchical agents within a league to explore the
environment collaboratively and enhances the robustness
of our algorithm.
2) We propose an expert strategy-based initialization
method to avoid useless exploration at the initial phase
of the intelligent agent’s training, which greatly reduces
the convergent time.
3) To achieve stable performance in spite of the dynamics
of the environment, we construct scatter networks for the
intelligent agent to distinguish different observations and
actions.
4) Simulation results demonstrate the proposed algorithm
has a stable performance and reduces almost 60% of
training time compared with state-of-the-art approaches.
The remainder of this article is outlined as follows. We
discuss the related work in Section II. We present the system
model in Section III. We propose the multiagent reinforcement
learning framework in Section IV. We simulate our result in
Section V. Finally, we conclude this article in Section VI.
II. RELATED WORK
A. MEC for Blockchain-Empowered IoT
Many works in the rising blockchain-empowered IoT field
have been proposed to tackle challenges in systems, mod-
els, and applications [14]. For example, Xiong et al. [15]
have proposed a lightweight infrastructure of the proof-of-
work-backed blockchains using a game-theoretic approach.
Furthermore, a general architecture combining blockchain
and IoT systems has been presented by the work [16]. A
multileader–multifollower game-theoretic approach has been
proposed by Xiong et al. [17] that investigates the optimum
solution in interactions among the cloud/edge providers and
miners in the blockchain. Zhou et al. [18] have attempted to
cover the existing scaling solutions for blockchain and classify
them by level.
B. Conventional Methods for MEC
Extensive works focus on effective methods to improve
performance in computation offloading problems [19]. A
free-bound mechanism and a multihop cooperative messaging
mechanism have been studied by Hong et al. [20], by formu-
lating the joint problem as a potential game in which devices
determine their equilibrium. Ning et al. [21] have proposed a
hybrid computation offloading framework for real-time traffic
management, aiming at maximizing the link rate by enabling
various offloading schemes that empowering by the integra-
tion of NOMA and MEC technology. A potential game whose
devices can make their decisions autonomously has been
proposed by Chen et al. [22] to minimize the economic cost
of blockchain-empowered devices when they cannot directly
connect to the edge servers or cloud data centers. However, the
conventional algorithms ignored the dynamics and randomness
of the wireless environment and their solutions are designed
for the short term.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
2366 IEEE INTERNET OF THINGS JOURNAL, VOL. 8, NO. 4, FEBRUARY 15, 2021
C. DRL-Based Algorithms for MEC
With the advent of DRL [23], the above issues can be
addressed by formulating the computation offloading prob-
lems as the Markov decision processes and learning the
optimal policies by interacting with the environments [24].
Many existing works have studied the multiuser computa-
tion offloading using single-agent DRL algorithms, such as
Chen et al. [25] have been motivated by the additive struc-
ture of the utility function. They have proposed a Q-function
decomposition technique is combined with the double deep
Q-learning (DQN), which leads to a novel learning algorithm
for the solving of stochastic computation offloading. A duel-
ing DQN approach has been studied by Qiu et al. [13] to
address the joint optimization problem consists of agent min-
ing and cloud mining in blockchain-enabled IoT. However, the
above DRL-based approaches suffer from slow convergence
caused by high-dimensional action space. In order to improve
the slow convergence problems in DQN, Feng et al. [26] have
developed an A3C-based cooperation computation offloading
and resource allocation to maximize the computation rate of
MEC systems and the transaction throughput of blockchain
systems. To speed up the convergence, Qiu et al. [1]
have adopted an adaptive genetic algorithm [27] into the
exploration of DRL that effectively avoids useless explo-
ration and speeds up the convergence without decreasing the
performance. Asheralieva and Niyato [28] have modeled a
stochastic Stackelberg game under private information. They
have designed a hierarchical learning framework for this game
to converge to the stable states, in which miners’ actions are
the best responses to the optimal price assigned by the ser-
vice provides. However, using single-agent DRL algorithms to
address the computation offloading problems in the multiuser
case suffers from slow convergence and unstable performance.
That is because of the nonstationarity of the multiuser envi-
ronment that users constantly change their strategies during
the training phase.
Fortunately, the recent breakthrough of multiagent DRL pro-
vides alternative approaches [29] to address the nonstationarity
of the multiuser environment by balancing exploration and
exploitation. Zhan et al. [30] have applied the game theory to
the algorithm design and formulated the problem as a partially
observable MDP to solve the edge computing problem when
users refuse to expose their own information. Zhan et al. [31]
have investigated multiagent DRL-based techniques to address
the problem of assigning a satisfactory but profitable amount
of incentives to multileader and multifollower as a mobile
crowdsensing game. However, most studies have not taken into
account multiple agents explore the environment collabora-
tively. In this article, we propose a multiagent DRL framework
where multiple agents exploring the environment collabora-
tively for fast convergence and robustness while the scatter
network is adopted to guarantee its stability.
III. SYSTEM MODEL AND PROBLEM FORMULATION
The notations used in this paper are summarized in Table I.
We first introduce our system model. As shown in Fig. 1,
we consider a uplink NOMA-based cooperative computation
offloading for blockchain-empowered IoT, which consists of
TABLE I
NOTATIONS USED IN THIS ARTICLE
Fig. 1. Illustration of the NOMA-enabled cooperative computation offloading
framework for the blockchain-empowered IoT.
IoT devices and edge servers. We denote the set of IoT devices
as N = {1, . . . , n, . . . , N} and the set of edge servers as B =
{1, . . . , b, . . . , B}.
A. Communication Model
We first introduce the communication model for NOMA
in MEC. Here, the system consists of an edge server and N
IoT devices, and each IoT device has the ability to transmit
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
LI et al.: NOMA-ENABLED COOPERATIVE COMPUTATION OFFLOADING FOR BLOCKCHAIN-EMPOWERED IoT 2367
and receive data. Different from the conventional orthogo-
nal frequency-division multiple access (OFDMA), the same
subchannel in NOMA can be shared in multiple users to
gain multiplexing benefits. Multi-input–multi-output (MIMO)
[32] technology enables IoT devices to assist other devices
offloading computation to the edge servers, i.e., cooperative
computation offloading [20]. As illustrated in Fig. 1, the edge
servers can provide resource-rich computation capabilities for
IoT devices, and devices can offload their computation tasks
to the edge servers by making full use of the subchannel.
1) Direct Transmission Phase: In the direct transmission
phase, IoT devices can only transmit their hybrid tasks to edge
servers directly. The received signal yb at the edge server b is
given by
yb =
∑
n∈N
gnpnxn + vb (1)
where gn,b denotes the Rayleigh fading channel coefficient
from the device n to the edge server b and xn is the message
of device n. vb denotes the additive Gaussian noise at the edge
server b. The use of NOMA implies
∑
n∈N p2
n = 1, where pn
is the power allocation coefficient for the IoT device n.
Without loss of generality, assume that the IoT devices are
ordered based on their channel quality, i.e.,
|g1|2/δ2
1 < |g2|2/δ2
2 < · · · < |gn−1|2/δ2
n−1 < |gn|2/δ2
n . (2)
Successive interference cancelation (SIC) is a decoding
method in NOMA, where the users with strong channel condi-
tions are capable of removing the interference of weak users.
In uplink NOMA, the signal first decoded at the edge server
will experience interference from all users in the cluster with
relatively weaker channels. That is, the transmission of the user
with the highest channel gain experiences interference from all
users within its cluster, whereas the transmission of the user
with the lowest channel gain receives zero interference from
the users in its cluster. With the use of SIC, the signal-to-
interference-plus-noise ratio (SINR) [21] from the IoT device
n to edge server b is given by
SINRn,b = gnpn∑n−1
k=1 gkpk + δ2
b
(3)
where δ2
b denotes the variance of the additive white Gaussian
noise on the receiver b. However, it is weak when the user is
far away from the edge server.
2) Cooperative Transmission Phase: During this phase, the
IoT devices cooperate with each other using a short-range
communication channel. We denote that L(n) is the cluster
of relays assisting the IoT device n offloading its tasks to the
edge server. The relay devices within the cluster are denoted
by l ∈ L(n). With the help of relay devices, the edge server b
can observe the superposition as
yn,b =
∑
n∈N
∑
l∈L(n)
glplxn,l + vb (4)
where vb denotes the noise at edge server b. We then compute
the cooperative SINRco
n,b [26] by adding the source SINR and
the truncated cooperative SINR along the relays as
SINRco
n,b = SINRn,b +
∑
l∈L(n)
min
(
SINRl,b, SINRl−1,l
)
(5)
where (l−1, l) denotes the pair of relays in cooperative trans-
mission. Comparing (3) with (5), cooperative transmission
conspicuously increases the SINR received at edge servers
and boosts reception reliability. In our considered dense
networking scenario, according to the definition of SINRco
n,b
and the Shannon theory [33], the cooperatively achievable
transmit rate can be written as
rco
n,b = W · log
(
1+ SINRco
n,b
)
(6)
where W is the allocated channel bandwidth in the NCoCO
system. Moreover, this would positively affect the performance
of MEC if well controlling the interference among the coop-
erative system.
B. Computation Model
We then discuss the computation model. Here, every IoT
devices in blockchain-empowered IoT generates the hybrid
tasks that consist of mining tasks and computation tasks each
decision slot, and the IoT device n’s task Tn = (sn, cn) is
characterized by two parameters, i.e., sn denotes the size of the
task and cn denotes the required CPU cycles per data size. The
tasks arrive at IoT devices following a Poisson process. The
IoT devices within the coverage of edge servers can directly
offload their hybrid tasks to one of the edge servers via wire-
less access. The other IoT devices, offloading their tasks to
edge servers via relay devices, are not be covered by the edge
servers. We slice the time in the blockchain-empowered IoT
system into the decision slot t ∈ {1, . . . , T} [34], where a
decision slot is a much slower time scale than that of tasks
arrival and offloading. In NCoCO for blockchain-empowered
IoT, energy cost and delay cost are the main considerations in
system optimization, so our computation model consists of an
energy model and a delay model.
1) Energy Model: In NCoCO, the energy cost is divided
into three parts: the first one is the local computing energy
consumption of IoT devices, the second one is the computing
energy consumption of edge servers to accomplish tasks, and
the last one is the transmit energy consumption during data
transmission. Denoting hn as the consumed energy per CPU
cycle of the IoT device n, it can be obtained by the measure-
ment method in [35]. The n device’s consumed energy el
n(t)
in the local processing at decision slot t is given as
el
n(t) = sn(t)cn(t)hn(t). (7)
Edge servers are much more resource-rich than a single
IoT device, which prefer offloading their tasks to edge servers
for execution. We can then compute the offload processing
consumed energy as
eo
n(t) = sn(t)cn(t)hb(t)+ pn(t)
sn(t)
rco
n,b(t)
+
∑
l∈L(n)
Tsw
l (t) (8)
where hb is the consumed energy per CPU cycles at the edge
server b. For the IoT device n, its consumed energy at decision
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
2368 IEEE INTERNET OF THINGS JOURNAL, VOL. 8, NO. 4, FEBRUARY 15, 2021
slot t needs to be determined according to the decision profile
an(t) ∈ {0, 1}, and pn(t) is the transmit power that the device
n is allocated according to its remained energy. For the energy
cost en for the device n, we have that
en(t) = an(t)e
l
n(t)+ an(t)e
o
n(t) (9)
where sn is the transmit size of n’s tasks, and when an(t) = 1,
the hybrid tasks of the IoT device n are processed locally.
Otherwise, the hybrid tasks are offloaded to the edge servers
for execution.
2) Delay Model: We can then compute the local processing
consumed delay for the IoT device n as
dl
n(t) =
sn(t)cn(t)
fn(t)
(10)
where sn is the task size, cn is the required CPU cycles per
data size of the device n, and fn is the CPU frequency of the
device n.
The offload processing consumed delay for the IoT device
n is calculated as
do
n(t) =
cn(t)sn(t)
fb(t)
+ sn(t)
rco
n,b(t)
+
∑
l∈L(n)
Tsw
l (t) (11)
where fb is the computation capacity of the edge server b, and
rco
n,b is the transmit rate achieved by cooperative NOMA. In
the cooperative offloading phase, it cannot be ignored by the
switch delay Tsw for each relay l ∈ L(n).
So the delay for the IoT device n can be formulated as
dn(t) = an(t)d
l
n(t)+ an(t)d
o
n(t) (12)
where an is the decision profile for the device n deciding
locally executes its tasks or offloads them to the edge servers
for execution.
3) Overhead: Following (9) and (12), the overhead of the
blockchain-empowered IoT system can be expressed as
Ototal(t) =
∑
n∈N
(
αe
nen(t)+ αd
n dn(t)
)
(13)
where αe
n = βe
n/el
n and αd
n = βd
n /dl
n while 0 ≤ βe
n, β
d
n ≤ 1. We
set βd
n > βe
n to intensify the latency aspect when the device
is executing some latency-sensitive tasks. Oppositely, we can
set βe
n > βd
n that makes the devices more susceptive to energy
consumption, if the battery capacity of the device is petite.
C. MDP for NMCO
In the following section, we put forward the observa-
tion space, action space, state transition probability, and cost
function for the MDP in the NOMA-enabled cooperative com-
putation offloading system for blockchain-empowered IoT. In
the control theory, the MDP problem is defined by a tuple
< X ,U ,P, C >, where X is the state space of the system,
U is the action space of the system, P is the state transition
probabilities, and C is the cost space.
1) State Space: We define the joint state space at the current
decision slot t as x(t) = [xcmp(t), xcom(t)], which consists of
the computation state space xcmp(t) and the communication
state space xcom(t).
Computation State: We denote the computation state by
xcmp(t), as shown in the following:
xcmp(t) � [ID, T(t), F(t), E(t), R(t − 1)] (14)
where ID is the IoT devices’ identity number. sn(t) and
cn(t) in T(t) = [s1(t), c1(t), s2(t), c2(t), . . . , sn(t), cn(t)]
denote the task size and the required CPU cycles per
data size of the device n ∈ N , respectively. F(t) =
[f1(t), f2(t), . . . , fn(t), . . . , fb(t)] is the computation capac-
ities of devices and edge servers, and R(t − 1) =
[r1(t − 1), r2(t − 1), . . . , rn(t − 1)] is the feedback signal
from last decision slot t − 1 that devices observe from the
environment.
Communication State: The signal of the communication
state is always smaller than the computation state, and it is
witty to process them respectfully. We denote the communi-
cation state as
xcom(t) � [ID, G(t)] (15)
where G(t) = [g1(t), g2(t), . . . , gn(t)] denotes the channel
gains in the system. In order to make the observations better as
the input of the neural network, we need to apply some adjust-
ments to the raw input. We first use min–max normalization to
process the computation state xcmp(t) and the communication
state xcom(t), and then use Z-score normalization to process
the joint state x(t).
2) Action Space: The joint action space u(t) in the deci-
sion slot t, including the offloading decision a(t), the power
allocation decision P(t), and the constraint offloading distance
L(t), can be defined as u(t) = [a(t), P(t), L(t)]. By using the
offloading distance, we convert the next-hop choosing problem
into a distance decision problem.
Offloading Decision: We denote the joint decision profile as
a(t) = [a1(t), a2(t), . . . , aN(t)]. (16)
Power Allocation Decision: The increase in the transmit
energy will amplify the transmit signal and the degree of
interference between devices at the same time. However, when
the interference is too serious, the transmit rate does not
increase or even decrease. Constraining the transmit power is
a solution for this tension between transmit interference and
transmit signal. We denote the joint transmit power allocation
decision by P(t) as
P(t) = [
p1(t), p2(t), . . . , pN(t)
]
. (17)
Constraint Distance Decision: The transmit signal fades
with the increase of distance, thus affecting the transmit rate.
In the NOMA-enabled communication scenario, the increased
bandwidth makes the fading more significant. So we want to
make the communication efficient by limiting the distance of
the next-hop device. The constraint distance L(t) is shown as
follows:
L(t) = [l1(t), l2(t), . . . , lN(t)]. (18)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
LI et al.: NOMA-ENABLED COOPERATIVE COMPUTATION OFFLOADING FOR BLOCKCHAIN-EMPOWERED IoT 2369
In cooperative computation offloading, we note that the
choice of the next-hop devices will make the communica-
tion better. In other words, the distance between the next-hop
device to the edge server is closer than that from the source
device to the edge server. Moreover, when the distance
between the next-hop device and the source device is farther
than the distance from the source device to the edge server,
the source device is preferred to offload directly to the edge
server.
D. State Transition Probability
The probability of a particular state is 0 because the state
space is continuous. We assume that the current state and
action is x(t) = x and u(t) = u; then the probability that
the process will leave x(t) and transition into the next state
x(t + 1) = x′ after taking an action x(t) can be expressed as
Pr
(
x′|x, u
) =
∫
R
f
(
x′, c|x, u
)
dc (19)
where f : X × X × U × C → [0, 1] is the state transition
probability density function mapping current state, next state,
current action, and current cost to a transition probability in
[0,1].
E. Cost Function
There is a cost space C = {c(x(t), u(t))}. We then define the
cost function as c(x(t), u(t)) = Ototal(t) that c : X × U → R.
In this article, the joint optimization problem is formulated as
2 min
u(t)
T∑
t=1
c(x(t), u(t)) (20)
s.t. an(t) ∈ {0, 1} (21)∑
n∈N
pn(t) ≤ PN, pn(t) ≥ 0 (22)
0 ≤ ln(t) ≤ LN,∀n ∈ N (23)
0 ≤ rco
n,b(t) ≤ W,∀n ∈ N ,∀b ∈ B. (24)
Constraint (21) shows each hybrid task can be executed
locally or offloaded to edge servers for execution. Constraint
(22) indicates the transmit power pn of the device n should be
no more than their limited transmit power PN . The constraint
for offloading distance demonstrated in (23) should be less
than the maximum offloading distance LN . Moreover, through
the utilization of SIC, the cooperative transmit rate should
never exceed the system bandwidth as demonstrated in (24).
Proposition 1: The NOMA-enabled cooperative computa-
tion offloading problem in (20) is NP-hard.
Proof: There are three simplified cases for this problem,
i.e., the problem of whether to offload, the joint problem of
power allocation, and the routing problem. The problem of
whether to offload and routing problem can be reduced to
the cardinality bin packing problem, which is NP-hard [19].
In [36], the joint problem of power allocation for NOMA has
demonstrated to be NP-hard. Since simplified cases of (20)
are NP-hard, Proposition 1 holds.
Based on the system model above, in the following section,
we will then propose a multiagent DRL framework to learn
efficient computation offloading policy by adopting the scatter
network and league learning.
IV. ALGORITHM DESIGN
In this section, we consider the issue of designing a
multiagent DRL framework for achieving fast convergence and
improving the robustness and stability in performance. Fig. 2
shows that our algorithm maintains a hierarchic league, where
upper agents accept the experiences from lower agents for
reinforcement learning, and lower agents periodically update
their network parameters using that of upper agents and initial-
ization agent. The scatter connection structure of the agent’s
policy network and critic network are shown in the top of
Fig. 2.
A. Agent and Network Design
We design an intelligent agent in the actor–critic and
off-policy fashion. It means that the agent maintains a deter-
ministic policy function and an action-value function. They are
both parameterized by deep neural networks. The intelligent
agent’s deterministic policy π can be described as
π
(
x(t)|θπ
) = u(t) (25)
where θπ is the parameters of the policy network. The policy
outputs the action u(t) conditioning on the state x(t). We use
the critic network with the parameters θQ to approximate the
action-value function Qπ as
Qπ
(
x, u|θQ
)
= Eπ [C(t)|x(t), u(t)]. (26)
It describes the expected return after taking an action u(t) in
the state x(t) and, thereafter, the following cost C(t) is:
C(t) =
T∑
i=t
γ (i−t)c(x(i), u(i)). (27)
It means the return from a state x(t) as the sum of the dis-
counted future cost with the discounting factor γ ∈ [0, 1].
When the target policy is deterministic, we also can make
the use of the recursive relationship known as the Bellman
equation
Qπ
(
x(t), u(t)|θQ
)
= c(x(t), u(t))
+ γ Qπ
(
x(t + 1), u(t + 1)|θQ
)
. (28)
To achieve stable performance in spite of the dynamics in
the environment, we adopt the technique of the scatter network
in the construction of a neural network. The scatter networks
extract features from the input of the computation model and
communication model, respectively. Then, the extracted spe-
cial features are input into the core network to make decisions.
Finally, the corresponding actions are output through different
networks. Experimental results show that our scatter network
structure can indeed achieve stable performance in a dynamic
environment. First, in order to distinguish the different inputs
between the computation state and the communication state,
the policy network uses two multilayer perceptions (MLPs),
which contains two fully connected layers, to extract the
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
2370 IEEE INTERNET OF THINGS JOURNAL, VOL. 8, NO. 4, FEBRUARY 15, 2021
Fig. 2. League learning framework for cooperative computation offloading. The framework maintains a hierarchical league, where upper agents accept the
experiences from lower agents for reinforcement learning, and the lower agents periodically update their network parameters using that of upper agents and
initialization agent. The scatter connection structure of the agent’s policy network and critic network are shown in the top of the figure.
features of the computation state and communication state,
respectively. The critic network maintains three MLPs to han-
dle inputs, where the extra MLP is added to extract the features
of action. We then concatenate their outputs and let them
be the input of the core of each network. The core of the
network is a bidirectional long short-term memory (BiLSTM)
[37], which maintains the memory between steps. With the
use of BiLSTM, each device can perceive the features of the
devices nearby. They are also affected by the features of the
far away devices after attenuation. The output actions of the
policy network are given by three MLPs. The agent first out-
puts the offloading decision. Then, the policy network outputs
the distance decision depending on the offloading decision.
Moreover, the agent decides the transmit power that should
be allocated based on the previous offloading and distance
decision. The output of the critic network is given by one
MLP.
However, directly using the output of the policy network
cannot make the agent to discover more strategies. We
constructed an exploration policy u′(t) by adding a noise
interference as
u′(t) = π
(
x(t)|θπ
)+ εt (29)
where the noise εt ∼ N is sampled from a normal distri-
bution and the variance of N annealing during the training
phase.
B. Agent Initialization
Following the agent and network structure defined above,
we next design the initialization method. The existing DRL-
based algorithms applied in the MEC system usually cost a
long training time to achieve convergence. We try to acceler-
ate the convergence by avoiding useless exploration, which
was caused by the agent’s random initialization. Note that
there are many state-of-the-art conventional methods that are
applied to computation offloading problems, which provide
lots of expert strategies for our agent to train. In order to avoid
the early useless exploration caused by random initialization of
network parameters, we initialize our agent using the strategies
designed for the MEC system. Our agent starts with supervised
learning on expert policy and output predictive action of IoT
devices and the corresponding action value. We then compute
the mean-square error between the expert’s policy πe and the
agent’s policy as
Lπ
(
θπ
) = (
π
(
x(t)|θπ
)− πe(x(t))
)2 (30)
where x(t) ∼ pe(x(t)) is the state sample from the state
distribution under expert policy. During imitation learn-
ing process, the estimated gradient with respect to θπ is
calculated as
∇θπ Lπ =
(
π
(
x(t)|θπ
)− πe(x(t))
)∇θπ log π
(
x(t)|θπ
)
. (31)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
LI et al.: NOMA-ENABLED COOPERATIVE COMPUTATION OFFLOADING FOR BLOCKCHAIN-EMPOWERED IoT 2371
Then, the policy network θπ is updated through the mini-
batch stochastic gradient descent method as follows:
θπ ← θπ + α∇θπ Lπ (32)
where α is the learning rate for the policy network. In order to
avoid the agent’s policy network overfitting expert policy, we
impose L2 regularization on the network parameters. Since the
agent policy is constantly learning the expert policy, the critic
network also needs to be constantly updated to suit the process
of following RL. We define the loss function for updating the
critic network as
LQ
(
θQ
)
=
(
Q
(
x(t), u(t)|θQ
)
− c(x(t), u(t))
+ γ Q
(
x(t + 1), π
(
x(t + 1)|θpi
)|θQ
))2
. (33)
Then, the estimated gradient with respect to θQ is calculated
as
∇θQLQ =
(
γ Q
(
x(t + 1), π
(
x(t + 1)|θπ
)|θQ
)
− c(x(t), u(t))
+ Q
(
x(t), u(t)|θQ
))
∇θQ Q
(
x(t), u(t)|θQ
)
. (34)
Thus, we update the critic network θQ by minibatch stochas-
tic gradient ascent as follows:
θQ ← θQ + β∇θQ LQ (35)
where β is the critic learning rate. We save the initial-
ized parameters as θπ
I and θ
Q
I for the following use of
league. Algorithm 1 summarizes the implementation of agent
initialization of the proposed algorithm.
Theorem 1: There exists a bound of distribution change
while the initialized agent’s policy πI is close to the expert’s
policy πe.
Proof: According to the agent’s deterministic policy that
u(t) = πI(x(t)|θπI ), πe is close to πI if πe(u(t) �=
πI(x(t)|θπI )|x(t)) ≤ ε, where ε ∈ [0, 1]. We have
pe(x(t)) = (1− ε)tpI(x(t))+
(
1− (1− ε)t)pmistake(x(t)) (36)
where pe and pI is the state distribution of expert policy and
initialized agent, pmistake is the state distribution of mistake
state. We have the distribution mismatch probability that
|pe(x(t))− pI(x(t))| =
(
1− (1− ε)t)|pmistake(x(t))− pI(xt)|
≤ 2
(
1− (1− ε)t)
≤ 2εt (37)
where we use the identity that (1− ε)t ≥ 1− εt for ε ∈ [0, 1].
So there exist a bound of distribution between pI and pe.
C. League Establishment
We then design a hierarchical league where the agents
explore the environment collaboratively. There are three lev-
els of training the agent in league learning, differing mainly
in their mechanism for parameters reset. In the top level is the
main agent (MA) that is used to evaluate the performance of
the whole league. In the middle level and the third level, the
main exploiter agent (MEA) and the league exploiter agent
(LEA) are used to discover diverse and specialized strategies
Algorithm 1: Agent Initialization
1 Initialization:
2 Initialize θπ
I , θ
Q
I , πe(·), γ, α, β
3 Iteration:
4 for epoch k in 1,2,... do
5 Sample a random minibatch of N state x(t) from
distribution of expert strategy pe(x(t)).
6 Input x(t) into policy network πI and expert policy πe.
7 Calculate policy gradient as (31)
8 Update the policy network θ
Q
I through stochastic gradient
ascent as (32)
9 Calculate critic estiamted gradient as (34)
10 Update the policy network θ
Q
I through stochastic gradient
ascent as (35)
11 Return: θπ
I , θ
Q
I
that capture the complexities of NCoCO. Comparatively, the
MEA’s performance is more stable than the LEA’s. At the
beginning of each trajectory, each exploiter agent (EA) runs
its own policy for n steps in its environment. After n steps,
each EA sends the trajectory through a queue to its upper
level agents, which consists of states, actions, and rewards
s1, a1, r1, . . . , sn, an, rn together with the initial BiLSTM state.
All agents use replay buffers that store trajectories for updat-
ing their network parameters asynchronously. The MA then
continuously updates its network parameters on batches of
trajectories received from MEAs and LEAs. MEAs update
their policies on batches of trajectories collected by itself and
receive from LEAs. Each LEA updates its policy on batches
of trajectories collected by itself.
The critic network parameters are updated by gradient
descent on the loss function with respect to the target network,
i.e., the critic loss is defined as
JQ
(
θQ
)
=
(
yt − Q
(
x(t), u(t)|θQ
))2
(38)
where yt is calculated by target networks
yt = c(x(t), u(t))+ γ Q′
(
x(t + 1), π ′
(
x(t + 1)|θπ ′
)
|θQ′
)
. (39)
During the training phase, the estimated gradient with
respect to θQ is calculated as
∇θQ J = (yt − Q(x(t), u(t)))∇θQQ
(
x(t), u(t)|θQ
)
. (40)
The critic network is updated though a minibatch stochastic
gradient descent method as follows:
θQ ← θQ − β∇θQ JQ (41)
where β is the critic learning rate.
We use policy gradient to update the policy network, which
updates it in the direction of
∇θπ Jπ = ∇θπ π
(
x(t)|θπ
)∇uQ
(
x(t), u|θQ
)
|u=π(x(t)|θπ ). (42)
Then, the policy network is updated by the following
stochastic estimated gradient method:
θπ ← θπ − α∇θπ Jπ (43)
where α is the learning rate of the policy network.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
2372 IEEE INTERNET OF THINGS JOURNAL, VOL. 8, NO. 4, FEBRUARY 15, 2021
D. Populating the League
For the league established above, we populate the league by
periodically resetting the networks of agents in the league to
achieve robust performance. First, the MA resets its networks
as the initialized agent at the beginning of the training phase as
MA ← 
I (44)
where 
I = [θπ
I , θ
Q
I ] are the network parameters of the policy
network and critic network.
Second, MEAs regularly reset their networks with respect
to both initialized agent’s network and MA’s network. Every
TMEA, MEAs reset their networks, i.e.,
MEA ← τMEA
MA + (1− τMEA)
I (45)
where τMEA is the reset ratio of MEA.
Third, LEAs regularly reset their networks with respect to
the initialized agent’s network and MEAs’ networks. Every
TLEA, LEAs reset their networks, i.e.,
LEA ← τLEA
MEA + (1− τLEA)
I (46)
where τLEA denotes the reset ratio of LEA. We set TMEA = n
and represent the number of LEAs within the league. To sim-
plify the problem, we set τMEA = τLEA. However, much
exploration is wasted when an EA’s performance is always
lower than that of MA. It is witty to reset such EA’s network
ahead of time. Moreover, the weight of the network parame-
ters inheriting from the upper level agents will continuously
increase during the training phase. As off-policy, each agent
maintains a replay buffer to make efficient use of hardware
optimizations, which means the agent learns in minibatches,
instead of online. In our algorithm, the input dimension and the
output dimension are determined by the number of IoT devices
N. The number of hidden neurons in MLP and BiLSTM is
set to H. The complexity analysis of MLP computing in the
network is O(NH) and the complexity analysis of BiLSTM
is O(H2). Thereby, the complexity analysis is O(NH + H2)
per decision slot. The pseudocodes of the league learning are
shown in Algorithm 2.
V. SIMULATION RESULT
A. Simulation Setup
We mainly consider the case of the number of IoT devices
N = 50 and the number of edge servers B = 1.The coverage
is set to 2 × 2 km2. The maximum transmit power pmax is
set to 1 W. There is also a switch power Esw = 0.01 W
for each relay’s transmission. In our blockchain-empowered
environment, the background noise σ 2 is set to −87 dBm.
Considering the wireless interference model in the real world,
we set the channel gain gn,s = d−α
n,s , where di,s is the distance
between the device n and the offloading target device s, and
α = 2.2 is the path-loss factor.
In our computation model, the average computation capac-
ities of the blockchain-empowered IoT devices and the edge
servers are 1 and 30 GHz, respectively. The huge computing
resource gap between local and edge makes IoT devices prefer
to offload tasks to edge servers for execution, which is also
more helpful for us to verify our multihop cooperative com-
putation offloading learning algorithm. Time is divided into
Algorithm 2: League Learning
1 Initialization:
2 for agent n ∈ L do
3 Initialize θπ
n , θπ ′
n ← θπ
I , θ
Q
n , θ
Q′
n ← θπ
I ,Dn, τ  1.
4 Iteration:
5 for episode k in 1, 2, ... do
6 for agent n ∈ L do
7 Initialize a random process N for action exploration.
8 Receive initial observation state x(1)
9 for t= 1,...,T do
10 Select exploration action u′(t) = π(x(t)|θπ )+N
according to the current policy and exploration
noise.
11 Execute action u′(t) and observe cost c(r) and new
state x(t + 1).
12 Store transition (x(t), u′(t), c(t), x(t + 1)) in Dn,
and send them to its upper agent.
13 for n and n’s upper agents do
14 Sample a random minibatch of N transition
(x(t), u(t), c(t), x(t + 1)) from Dn.
15 Update critic network use the gradient calculated
in (40)
16 Update the policy network using the sampled
gradient calculated in (42)
17 Update the target network use soft update method:
18 θπ ′
n ← τθπ
n + (1− τ)θπ ′
n ,
19 θ
Q′
n ← τθ
Q
n + (1− τ)θ
Q′
n ,
20 if k%TLEA == 0 or k%TMEA == 0 then
21 Reset agents as (45) or (46).
decision slots in our hybrid task model. At the beginning of
each time slot, the average data size of the task is 3 GB. We
set the average required CPU cycles per data size to 1 Gcycles
per GB data. The maximum decision slot in each episode is
set to T = 32. The system bandwidth is set to 20 MHz and
the spectral efficiency is set to 10. For each IoT device n,
the energy weight βe
n is uniformly distributed in [0,1] and the
delay weight βd
n is set as 1− βe
n .
The stepsizes are the parameters of alpha and beta in the
proposed algorithm. The smaller the stepsizes, the slower the
change of the network weight. In the experiments, we run the
trials as many as possible to adjust the stepsizes and thus set
the stepsizes alpha and beta to 1e−3 that is a reasonable value
ensuring our networks converging to an acceptable solution in
an appropriate time. The smoothing constant is set to τ =
5e − 3. The discounting factor γ is set to 0.99. During the
training phase, the agent initialization takes 4e4 episodes and
the league learning takes 1e6 episodes. We maintain one MA,
one MEA, and two LEAs within the league. The reset period
of the league is set to 1/5 of the time that I-DDPG needs to
achieve convergence and the reset ratio anneals from 100% to
0%. The proposed algorithm and other baseline methods are
implemented using PyTorch on a Python-based simulator.
B. Simulation Design
Evaluation Metrics:
1) Average Cost: The average cost is calculated by the
weighted ratio of the system cost of all local-computing
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
LI et al.: NOMA-ENABLED COOPERATIVE COMPUTATION OFFLOADING FOR BLOCKCHAIN-EMPOWERED IoT 2373
IoT devices to the total system cost. It can be used to
evaluate the overall performance of all IoT devices.
2) Number of Convergence Episodes: The number of con-
vergent episodes is that the learning algorithm could
achieve convergence during the DRL-agent training pro-
cess. In the NCoCO system, episodes can be converted
into a specific clock-runtime.
Baseline Algorithms:
1) Expert Algorithm [9]: The expert algorithm proposed for
the conventional OMA-enabled computation offloading
system that formulates the joint problem as a potential
game and achieves a state-of-the-art Quality-of-Service
(QoS) performance.
2) DDPG [38]: DDPG, a single-agent DRL algorithm, is
regarded as one of the state-of-the-art off-policy deep RL
methods. The DDPG algorithm is able to find policies
whose performance is competitive with those found by
a planning algorithm with full access to the dynamics
of the domain and its derivatives.
3) Initialization-DDPG (I-DDPG) Algorithm: Intelligent
agent represented by scatter networks first uses strategies
designed for the OMA-enabled system for initializa-
tion and subsequently adopts DDPG to improve its
performance through the policy gradient method.
C. Convergence Analysis
To demonstrate the convergence of our proposed learning
algorithm and other baseline algorithms, we plot the dynam-
ics of the average system cost over the episode in Fig. 3. We
evaluate the convergence of the proposed learning algorithm
using the above parameter settings. The convergence of the
proposed learning algorithm is the crucial property for ensur-
ing the obtained policy mapping states to the optimal actions.
Generally. a learning algorithm is considered to converge
when the average system cost curve becomes flat. Therefore,
we plot the cost returned from each training episode under
the scenario where the number of IoT devices is 50. As
the result shown in Fig. 3, the initialized agent takes about
4e3 episodes to achieve an approximate performance for the
expert algorithm in the agent initialization phase. When it
is in the reinforcement learning phase, it takes hundreds of
episodes to achieve convergence. The DDPG algorithm spends
8.76e5 episodes for convergence, and the I-DDPG algorithm
takes 1.94e5 episodes to achieve convergence. With the above
league parameters setting, the proposed learning algorithm
takes 3.18e5 episodes to achieve convergence. In the begin-
ning training phase of the DDPG algorithm, its average cost
does not descend significantly. The DDPG algorithm prefers
random exploration to find novel strategies. Its average cost
is fluctuating, and then can be reduced gradually. Although
the I-DDPG algorithm is the fastest one to reach a convergent
state, its performance is not satisfactory. The reason is that
there is not enough motivation for the intelligent agent after
agent initialization to explore, so it quickly reaches a local-
optimal performance and stops exploring. We can see that the
curve of the proposed algorithm fluctuates more violently dur-
ing the training of league, and then converges to the stable
Fig. 3. Average cost versus the number of episodes.
optimal solution of the system. Furthermore, the intelligent
agent can achieve a convergence shortening at most 77.8%
of the training time with agent initialization. In addition, the
intelligent agent reducing 63.6% of the training time than the
state-of-the-art reinforcement learning algorithm.
D. Performance Evaluation Under Various Environment
Settings
We first compare the proposed learning algorithm with the
baseline algorithms in Table II. We run our simulations with
N = 10, 20, . . . , 50 IoT devices and one edge server. For each
algorithm, we repeated 200 times and computed the average
system cost to give the result shown in Table II. Because of the
large gap between the performance of the expert algorithm and
other baseline algorithms shown in Table II, it is not shown
in the subsequent experimental demonstration figures. When
the number of industrial robots is 10, the performance of the
algorithms is basically similar, and the proposed learning algo-
rithm performs even a little prominently. However, with the
increase in the number of IoT devices, our algorithm is grad-
ually becoming outstanding. Furthermore, with the increase
in the number of IoT devices, devices used in our NCoCO
system have more choices in relay devices, so the unit cost
of the system becomes even lower. This reflects the superi-
ority of our NCoCO system. From Table II, we can see that
the system optimal policy we finally learned can decrease the
system cost by 45% compared with the expert policy. Even the
I-DDPG algorithm can decrease 36.5% cost than the expert
policy. Our proposed algorithm encourages the exploration of
agent, hence the final convergence cost is better than that of
the DDPG algorithm.
Figs. 4 and 5 show the average time cost and the average
energy cost of different algorithms as the increasing numbers
of IoT devices. The variation trends of the time cost or the
energy cost are the same, with the increase in the number of
devices. The performances of DDPG and the proposed algo-
rithm are similar, and the latter is slightly better. The average
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
2374 IEEE INTERNET OF THINGS JOURNAL, VOL. 8, NO. 4, FEBRUARY 15, 2021
TABLE II
COMPARATIVE COST UNDER DIFFERENT NUMBERS OF IOT DEVICES
Fig. 4. Average time cost versus the number of IoTs.
time cost of the I-DDPG algorithm is similar to that of the
other two algorithms, and even better than DDPG sometimes.
But it has a huge gap with the other two in terms of average
energy cost. The energy control of the I-DDPG algorithm is
worse than that of DDPG and the proposed algorithm. The
reason may be that the I-DDPG algorithm is affected by the
expert policy designed for the OMA-enabled system. Note that
the expert policy performs well in the OMA-enabled system.
Fig. 6 shows that the average cost of each algorithm
increases with the increase in the data size of hybrid tasks.
The larger data size will increase the average cost of the
system under the same allocated bandwidth. Compared with
the proposed algorithm and DDPG, the I-DDPG algorithm is
more severely affected by the increase of data size. So that the
I-DDPG algorithm is not good at controlling transmit power
as we mentioned above. In addition, with the increase in the
data size, the average of costs of the DDPG and I-DDPG
algorithms gradually slow down.
Fig. 7 shows that the average cost decrease when the
required CPU cycles per data size increase. Due to the lim-
ited computation resource of IoT devices, it is sensitive to the
required CPU cycles per data size. When the required CPU
cycles per data size are small, the performances of DDPG
and our proposed learning algorithm are basically similar.
When the required CPU cycles per data size increases, their
performance will be distinguished. Thus, with the increase in
the required CPU cycles per data size, IoT devices will be
Fig. 5. Average energy cost versus the number of IoTs.
Fig. 6. Average system cost versus data size.
more inclined to offload tasks to the edge server for execu-
tion. As the computing resources of the edge server is also
limited, the reduction in average system cost will gradually
slow down.
We then present the performances under different compu-
tation capacities of IoT devices in Fig. 8. As the computation
capacity of a single device increases, the relative benefits of
offloading to the edge server become less. Because IoT devices
prefer to execute their tasks locally. We also present the perfor-
mances under different computation capacities of edge servers
in Fig. 9. With an increase in the capacity of the edge server,
the average cost of the system will not change significantly.
Because more attention is paid to the energy control in the
transmission phase and the selection of a relay device in our
NCoCO system, the computing power of the edge side will
not have too much impact on the average cost of the system.
In particular, in different cases of computation capacities, the
system cost of the proposed algorithm is less than that of
DDPG and I-DDPG.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
LI et al.: NOMA-ENABLED COOPERATIVE COMPUTATION OFFLOADING FOR BLOCKCHAIN-EMPOWERED IoT 2375
Fig. 7. Average system cost versus CPU cycles per data size.
Fig. 8. Average system cost versus computation capacity of IoT devices.
As illustrated in Fig. 10, the simulations run under scenarios
where the channel bandwidth allocated in the system ranges
from 15 to 24 MHz. As the channel bandwidth allocated by
the system increases, the relative benefits of offloading to the
edge server become less. In addition, because the selection of
relay devices needs to be considered in the NCoCO system,
the impact on the DDPG and the proposed learning algorithms
is similar.
E. Impact of Different League Settings
In this section, we also consider the impact of different
setting parameters in the league. The network of intelligent
agents is initialized with the same network of agent initial-
ization. Fig. 11 shows the performances under the different
composition of the league, different reset period of agents,
and different reset ratio of agents.
1) Different Compositions of League: We first investigate
how the compositions of league affect the performance of
Fig. 9. Average system cost versus computation capacity of edge servers.
Fig. 10. Average system cost versus the channel bandwidth.
the proposed algorithm and plot the results in Fig. 11(a) and
(d). I-DDPG is equivalent to that there is only one MA in
the league. Its number of episodes for convergence is min-
imum but the corresponding system cost is the worst. The
proposed algorithm adds an MEA into the league to assist MA.
Its number of episodes to achieve convergence increases, but
the average cost of the system becomes smaller. Furthermore,
the proposed algorithm adds one LEA into the league, which
brings more diverse exploration to the league. Hence, the aver-
age cost of the system decreases significantly. However, in our
simulation, adding two LEAs into the league cannot reduce
the system average cost rapidly but increase the number of
episodes.
2) Different Reset Periods of Exploration Agent: We then
evaluate the achieved performances of the proposed algo-
rithm under different reset periods, i.e., TI−D/2, TI−D/5, and
TI−D/10. We use the number of episodes TI−D as a bench-
mark, which ensures the I-DDPG algorithm could achieve
a convergence. As Fig. 11(b) shows, the average costs in
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
2376 IEEE INTERNET OF THINGS JOURNAL, VOL. 8, NO. 4, FEBRUARY 15, 2021
(d) (e) (f)
(a) (b) (c)
Fig. 11. Performance of League learning when varying of league parameters. (a) System cost versus league composition. (b) System cost versus reset period.
(c) System cost versus reset ratio. (d) Number of episodes versus league composition. (e) Number of episodes versus reset period. (f) Number of episodes
versus reset ratio.
TI−D/2 and TI−D/5 are similar and moreover the lowest. From
Fig. 11(e), TI−D/2 of convergence is nearly twice as much
time as TI−D/5. When the reset period is TI−D/2, much time
is wasted for exploration, whereas TI−D/5 achieves a more
effective exploration than TI−D/2.
3) Different Reset Ratios of Exploration Agent: We record
the results of the proposed algorithm under different reset
ratios and plot them in Fig. 11(c) and (f). “Anneal” repre-
sents that the reset ratio anneals from 100% to 0%. At the
beginning of learning, i.e., the reset ratio is 100%. EAs are
used to explore the environment adequately. Then, the EAs
inherit the experience of the upper agents in the later phase of
the training. In this way, the league can prevent the previous
experience of EAs exploration from being ignored by later
populated agents. When the reset ratio is 0%, the league con-
verges fastest but its performance is not satisfactory. Then,
system costs in the cases of the reset ratio 50% and Anneal
are similar. But the number of episodes in the case of the reset
ratio 50% is a half and one of that in the case of Anneal.
VI. CONCLUSION
In this article, we proposed a novel multiagent DRL frame-
work to minimize the long-term cost of cooperative com-
putation offloading. To accelerate convergence, we trained
our intelligent agent using expert strategies to balance explo-
ration and exploitation. To enhance the robustness, we adopted
league learning to allow hierarchical agents within a league to
explore the environment collaboratively, so that the agent can
discover more novel strategies and remain robust under differ-
ent superparameters. To achieve stable performance in spite
of the dynamics of the environment, we constructed scatter
networks for the intelligent agent to distinguish information
observed from the environment and actions. Simulation results
demonstrated that the proposed algorithm has a more stable
and robust performance than the state-of-the-art algorithms,
and moreover, it could reduce almost 60% of training time
whatever the system parameters change. In our future work,
we will try to design the DRL-based framework for coopera-
tive computation offloading with untrusted relays, to minimize
information leakage at the untrusted relays and to achieve
secure cooperative computation offloading. In addition, we
plan to study the sparse representation in reinforcement learn-
ing to further improve the exploitation, which is a key problem
for DRL.
REFERENCES
[1] X. Qiu, L. Liu, W. Chen, Z. Hong, and Z. Zheng, “Online deep rein-
forcement learning for computation offloading in blockchain-empowered
mobile edge computing,” IEEE Trans. Veh. Technol., vol. 68, no. 8,
pp. 8050–8062, Aug. 2019.
[2] Z. Yang, K. Yang, L. Lei, K. Zheng, and V. C. M. Leung, “Blockchain-
based decentralized trust management in vehicular networks,” IEEE
Internet Things J., vol. 6, no. 2, pp. 1495–1505, Apr. 2019.
[3] Z. Li, Z. Yang, S. Xie, W. Chen, and K. Liu, “Credit-based payments
for fast computing resource trading in edge-assisted Internet of Things,”
IEEE Internet Things J., vol. 6, no. 4, pp. 6606–6617, Aug. 2019.
[4] Z. Ma, L. Feng, and F. Xu, “Design and analysis of a distributed
and demand-based backscatter mac protocol for Internet of Things
networks,” IEEE Internet Things J., vol. 6, no. 1, pp. 1246–1256,
Feb. 2019.
[5] X. Chen, “Decentralized computation offloading game for mobile
cloud computing,” IEEE Trans. Parallel Distrib. Syst., vol. 26, no. 4,
pp. 974–983, Apr. 2015.
[6] Y. C. Hu, M. Patel, D. Sabella, N. Sprecher, and V. Young, “Mobile edge
computing—A key technology towards 5G,” Sophia Antipolis, France,
ETSI, White Paper, vol. 11, pp. 1–16, 2015.
[7] Z. Wang, Q. Zhao, L. Feng, and F. Xu, “How much benefit can dynamic
frequency scaling bring to WiFi?” IEEE Trans. Mobile Comput., early
access, Dec. 9, 2019, doi: 10.1109/TMC.2019.2958323.
[8] H. Zhou, H. Wang, X. Li, and V. C. M. Leung, “A survey on mobile data
offloading technologies,” IEEE Access, vol. 6, pp. 5101–5111, 2018.
[9] Z. Hong, H. Huang, S. Guo, W. Chen, and Z. Zheng, “Qos-aware cooper-
ative computation offloading for robot swarms in cloud robotics,” IEEE
Trans. Veh. Technol., vol. 68, no. 4, pp. 4027–4041, Apr. 2019.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
http://dx.doi.org/10.1109/TMC.2019.2958323
LI et al.: NOMA-ENABLED COOPERATIVE COMPUTATION OFFLOADING FOR BLOCKCHAIN-EMPOWERED IoT 2377
[10] Q.-V. Pham et al. (2019). A Survey of Multi-Access Edge Computing in
5G and Beyond: Fundamentals, Technology Integration, and State-of-
the-Art. [Online]. Available: https://arxiv.org/abs/1906.08452.
[11] R. A. Howard, Dynamic Programming and Markov Processes.
Cambrige, MA, USA: MIT Press, 1960.
[12] Z. Chen and X. Wang. (2018). Decentralized Computation Offloading for
Multi-User Mobile Edge Computing: A Deep Reinforcement Learning
Approach. [Online]. Available: https://arxiv.org/abs/1812.07394.
[13] C. Qiu, H. Yao, C. Jiang, S. Guo, and F. Xu, “Cloud computing assisted
blockchain-enabled Internet of Things,” IEEE Trans. Cloud Comput.,
early access, Jul. 23, 2019, doi: 10.1109/TCC.2019.2930259.
[14] S. Zhou, H. Huang, W. Chen, Z. Zheng, and S. Guo. (2019). Pirate: A
Blockchain-Based Secure Framework of Distributed Machine Learning
in 5g Networks. [Online]. Available: https://arxiv.org/abs/1912.07860.
[15] Z. Xiong, S. Feng, W. Wang, D. Niyato, P. Wang, and Z. Han,
“Cloud/fog computing resource management and pricing for blockchain
networks,” IEEE Internet Things J., vol. 6, no. 3, pp. 4585–4600,
Jun. 2019.
[16] Z. Xiong, Y. Zhang, N. C. Luong, D. Niyato, P. Wang, and N. Guizani,
“The best of both worlds: A general architecture for data management
in blockchain-enabled Internet-of-Things,” IEEE Netw., vol. 34, no. 1,
pp. 166–173, Jan. 2020.
[17] Z. Xiong, J. Kang, D. Niyato, P. Wang, and H. V. Poor, “Cloud/edge
computing service management in blockchain networks: Multi-leader
multi-follower game-based admm for pricing,” IEEE Trans. Services
Comput., vol. 13, no. 2, pp. 356–367, Mar./Apr. 2020.
[18] Q. Zhou, H. Huang, Z. Zheng, and J. Bian, “Solutions to scalabil-
ity of blockchain: A survey,” IEEE Access, vol. 8, pp. 16440–16455,
2020.
[19] X. Chen, L. Jiao, W. Li, and X. Fu, “Efficient multi-user computation
offloading for mobile-edge cloud computing,” IEEE/ACM Trans. Netw.,
vol. 24, no. 5, pp. 2795–2808, Oct. 2016.
[20] Z. Hong, W. Chen, H. Huang, S. Guo, and Z. Zheng, “Multi-hop coop-
erative computation offloading for industrial iot-edge-cloud computing
environments,” IEEE Trans. Parallel Distrib. Syst., vol. 30, no. 12,
pp. 2759–2774, Dec. 2019.
[21] Z. Ning, X. Wang, J. J. Rodrigues, and F. Xia, “Joint computation
offloading, power allocation, and channel assignment for 5g-enabled
traffic management systems,” IEEE Trans. Ind. Informat., vol. 15, no. 5,
pp. 3058–3067, May 2019.
[22] W. Chen et al., “Cooperative and distributed computation offloading
for blockchain-empowered industrial Internet of Things,” IEEE Internet
Things J., vol. 6, no. 5, pp. 8433–8446, Oct. 2019.
[23] D. Silver et al., “Mastering the game of go with deep neu-
ral networks and tree search,” Nature, vol. 529, no. 7587, p. 484,
2016.
[24] X. He, K. Wang, H. Huang, T. Miyazaki, Y. Wang, and S. Guo, “Green
resource allocation based on deep reinforcement learning in content-
centric IoT,” IEEE Trans. Emerg. Topics Comput., early access, Feb. 13,
2018, doi: 10.1109/TETC.2018.2805718.
[25] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Optimized
computation offloading performance in virtual edge computing systems
via deep reinforcement learning,” IEEE Internet Things J., vol. 6, no. 3,
pp. 4005–4018, Jun. 2019.
[26] J. Feng, F. R. Yu, Q. Pei, X. Chu, J. Du, and L. Zhu, “Cooperative
computation offloading and resource allocation for blockchain-enabled
mobile edge computing: A deep reinforcement learning approach,” IEEE
Internet Things J., vol. 7, no. 7, pp. 6214–6228, Jul. 2020.
[27] M. Srinivas and L. M. Patnaik, “Adaptive probabilities of crossover
and mutation in genetic algorithms,” IEEE Trans. Syst., Man, Cybern.,
vol. 24, no. 4, pp. 656–667, Apr. 1994.
[28] A. Asheralieva and D. Niyato, “Learning-based mobile edge com-
puting resource management to support public blockchain networks,”
IEEE Trans. Mobile Comput., early access, Dec. 16, 2019,
doi: 10.1109/TMC.2019.2959772.
[29] O. Vinyals et al., “Grandmaster level in starcraft II using multi-agent
reinforcement learning,” Nature, vol. 575, no. 7782, pp. 350–354, 2019.
[30] Y. Zhan, S. Guo, P. Li, and J. Zhang, “A deep reinforcement learn-
ing based offloading game in edge computing,” IEEE Trans. Comput.,
vol. 69, no. 6, pp. 883–893, Jun. 2020.
[31] Y. Zhan, C. H. Liu, Y. Zhao, J. Zhang, and J. Tang, “Free market of
multi-leader multi-follower mobile crowdsensing: An incentive mech-
anism design by deep reinforcement learning,” IEEE Trans. Mobile
Comput., early access, Jul. 9, 2019, doi: 10.1109/TMC.2019.2927314.
[32] A. M. Haimovich, R. S. Blum, and L. J. Cimini, “MIMO radar with
widely separated antennas,” IEEE Signal Process. Mag., vol. 25, no. 1,
pp. 116–129, Dec. 2008.
[33] T. S. Rappaport et al., Wireless Communications: Principles and
Practice, vol. 2. Upper Saddle River, NJ, USA: Prentice-Hall, 1996.
[34] Y. Li, X. Wang, X. Gan, H. Jin, L. Fu, and X. Wang, “Learning-
aided computation offloading for trusted collaborative mobile edge
computing,” IEEE Trans. Mobile Comput., early access, Aug. 14, 2019,
doi: 10.1109/TMC.2019.2934103.
[35] Y. Wen, W. Zhang, and H. Luo, “Energy-optimal mobile application
execution: Taming resource-poor mobile devices with cloud clones,” in
Proc. IEEE INFOCOM, 2012, pp. 2716–2720.
[36] B. Di, L. Song, and Y. Li, “Sub-channel assignment, power allocation,
and user scheduling for non-orthogonal multiple access networks,” IEEE
Trans. Wireless Commun., vol. 15, no. 11, pp. 7686–7698, Nov. 2016.
[37] Z. Huang, W. Xu, and K. Yu. (2015). Bidirectional LSTM-
CRF Models for Sequence Tagging. [Online]. Available:
http://arxiv.org/abs/1508.01991.
[38] T. P. Lillicrap et al. (2015). Continuous Control With Deep
Reinforcement Learning. [Online]. Available: http://arxiv.org/abs/1509.
02971.
Zhenni Li (Member, IEEE) received the B.Sc.
degree from the School of Physical Science and
Electronics, Shanxi Datong University, Datong,
China, in 2009, the M.Sc. degree from the School
of Physics and Optoelectronic, Dalian University of
Technology, Dalian, China, in 2012, and the Ph.D.
degree from the School of Computer Science and
Engineering, University of Aizu, Aizuwakamatsu,
Japan, in 2015.
She is currently an Associate Professor with
the Guangdong Key Laboratory of IoT Information
Technology, School of Automation, Guangdong University of Technology,
Guangzhou, China. Her research interests include machine learning, sparse
representation, and resource allocation in cloud/edge computing.
Minrui Xu is currently pursuing the B.S. degree
with the School of Data and Computer Science, Sun
Yat-sen University, Guangzhou, China.
His research interests include deep reinforcement
learning, edge/cloud computing, Internet of Things,
and blockchain.
Jiangtian Nie (Student Member, IEEE) received the
B.Eng. degree (Hons.) in electronics and information
engineering from the Huazhong University of
Science and Technology, Wuhan, China, in 2016.
She is currently pursuing the Ph.D. degree
with ERI@N, Interdisciplinary Graduate School,
Nanyang Technological University, Singapore.
Her research interests include incentive mecha-
nism design in crowdsensing and game theory.
Jiawen Kang received the M.S. and Ph.D. degrees
from the Guangdong University of Technology,
Guangzhou, China, in 2015 and 2018, respectively.
He is currently a Postdoctoral Fellow with
Nanyang Technological University, Singapore. His
research interests mainly focus on blockchain, and
security and privacy protection in wireless commu-
nications and networking.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
http://dx.doi.org/10.1109/TCC.2019.2930259
http://dx.doi.org/10.1109/TETC.2018.2805718
http://dx.doi.org/10.1109/TMC.2019.2959772
http://dx.doi.org/10.1109/TMC.2019.2927314
http://dx.doi.org/10.1109/TMC.2019.2934103
2378 IEEE INTERNET OF THINGS JOURNAL, VOL. 8, NO. 4, FEBRUARY 15, 2021
Wuhui Chen (Member, IEEE) received the bache-
lor’s degree from Northeast University, Shengyang,
China, in 2008, and the master’s and Ph.D. degrees
from the University of Aizu, Aizuwakamatsu, Japan,
in 2011 and 2014, respectively.
From 2014 to 2016, he was a Research Fellow
with the Japan Society for the Promotion of Science,
Tokyo, Japan. From 2016 to 2017, he was a
Researcher with the University of Aizu. He is cur-
rently an Associate Professor with Sun Yat-sen
University, Guangzhou, China. His current research
interests include edge/cloud computing, cloud robotics, and blockchain.
Shengli Xie (Fellow, IEEE) received the M.S.
degree in mathematics from Central China Normal
University, Wuhan, China, in 1992, and the Ph.D.
degree in control theory and applications from the
South China University of Technology, Guangzhou,
China, in 1997.
He is the Director of the Laboratory for Intelligent
Information Processing and a Full Professor with the
Guangdong University of Technology, Guangzhou.
He has authored or coauthored two monographs and
more than 100 scientific papers published in journals
and conference proceedings. His current research interests include automatic
control and signal processing, especially blind signal processing and image
processing.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:45 UTC from IEEE Xplore.  Restrictions apply. 
<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles false
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.4
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /LeaveColorUnchanged
  /DoThumbnails false
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo false
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts false
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
    /Arial-Black
    /Arial-BoldItalicMT
    /Arial-BoldMT
    /Arial-ItalicMT
    /ArialMT
    /ArialNarrow
    /ArialNarrow-Bold
    /ArialNarrow-BoldItalic
    /ArialNarrow-Italic
    /ArialUnicodeMS
    /BookAntiqua
    /BookAntiqua-Bold
    /BookAntiqua-BoldItalic
    /BookAntiqua-Italic
    /BookmanOldStyle
    /BookmanOldStyle-Bold
    /BookmanOldStyle-BoldItalic
    /BookmanOldStyle-Italic
    /BookshelfSymbolSeven
    /Century
    /CenturyGothic
    /CenturyGothic-Bold
    /CenturyGothic-BoldItalic
    /CenturyGothic-Italic
    /CenturySchoolbook
    /CenturySchoolbook-Bold
    /CenturySchoolbook-BoldItalic
    /CenturySchoolbook-Italic
    /ComicSansMS
    /ComicSansMS-Bold
    /CourierNewPS-BoldItalicMT
    /CourierNewPS-BoldMT
    /CourierNewPS-ItalicMT
    /CourierNewPSMT
    /EstrangeloEdessa
    /FranklinGothic-Medium
    /FranklinGothic-MediumItalic
    /Garamond
    /Garamond-Bold
    /Garamond-Italic
    /Gautami
    /Georgia
    /Georgia-Bold
    /Georgia-BoldItalic
    /Georgia-Italic
    /Haettenschweiler
    /Helvetica
    /Helvetica-Bold
    /HelveticaBolditalic-BoldOblique
    /Helvetica-BoldOblique
    /Impact
    /Kartika
    /Latha
    /LetterGothicMT
    /LetterGothicMT-Bold
    /LetterGothicMT-BoldOblique
    /LetterGothicMT-Oblique
    /LucidaConsole
    /LucidaSans
    /LucidaSans-Demi
    /LucidaSans-DemiItalic
    /LucidaSans-Italic
    /LucidaSansUnicode
    /Mangal-Regular
    /MicrosoftSansSerif
    /MonotypeCorsiva
    /MSReferenceSansSerif
    /MSReferenceSpecialty
    /MVBoli
    /PalatinoLinotype-Bold
    /PalatinoLinotype-BoldItalic
    /PalatinoLinotype-Italic
    /PalatinoLinotype-Roman
    /Raavi
    /Shruti
    /Sylfaen
    /SymbolMT
    /Tahoma
    /Tahoma-Bold
    /Times-Bold
    /Times-BoldItalic
    /Times-Italic
    /TimesNewRomanMT-ExtraBold
    /TimesNewRomanPS-BoldItalicMT
    /TimesNewRomanPS-BoldMT
    /TimesNewRomanPS-ItalicMT
    /TimesNewRomanPSMT
    /Times-Roman
    /Trebuchet-BoldItalic
    /TrebuchetMS
    /TrebuchetMS-Bold
    /TrebuchetMS-Italic
    /Tunga-Regular
    /Verdana
    /Verdana-Bold
    /Verdana-BoldItalic
    /Verdana-Italic
    /Vrinda
    /Webdings
    /Wingdings2
    /Wingdings3
    /Wingdings-Regular
    /ZapfChanceryITCbyBT-MediumItal
    /ZWAdobeF
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 200
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages false
  /ColorImageDownsampleType /Average
  /ColorImageResolution 300
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 200
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages false
  /GrayImageDownsampleType /Average
  /GrayImageResolution 300
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 400
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages false
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False
  /CreateJDFFile false
  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Recommended"  settings for PDF Specification 4.01)
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice