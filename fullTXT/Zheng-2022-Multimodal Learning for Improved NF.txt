Multimodal Learning for Improved NFT Price Prediction
Multimodal Learning for Improved NFT Price
Prediction
Xi Zheng
School of Management
Xi’an Jiaotong University
Xi’an, China
xizheng328@gmail.com
Abstract—NFTs (Non-fungible tokens) refer to digital assets
in the form of art, game items and other collectibles that are
encoded in smart contracts on blockchain. Starting from 2021,
the NFT market has been growing exponentially. However, the
overall structures, evolutions and trends of the market have not
been sufficiently explored. In this study, we analyze data of 9,045
NFTs which were on sale at OpenSea in April 2021 and predict
their price using both visual and non-visual information by a
two-stage machine learning approach and an end-to-end deep
learning approach. To examine the effectiveness of multimodality,
models trained on unimodal and multimodal data are compared.
Besides, the effect of different feature fusion techniques on model
performance is analyzed. To identify important predictors, the
top ten most influential features are also presented. Results show
that among two-stage models, the VGG-RF model trained on
multimodal data gives the best performance of R squared 69.75%
and RMSE 565.49. Among end-to-end models, the multimodal
neural network with self and cross-attention achieves the best
performance of R squared 69.25% and RMSE 569.13. Our results
also demonstrate that the visual aspect of NFT have an impact
on its price, but this impact is weaker than some non-visual
information such as total number of bids in history and the
collection the NFT comes from. We expect findings of this study
facilitate future researches on NFT trading and pricing strategies,
and help investors make more advisable investment decisions.
Index Terms—Non-fungible token, multimodal learning, neural
network, machine learning, price prediction
I. INTRODUCTION
Non Fungible Tokens (NFTs) are blockchain-based digital
assets that can store images, gifs, audios, videos and other sorts
of storable digital content [1]. Each NFT is a unit of data
stored on the blockchain and encrypted by the blockchain’s
smart contract, thus providing a unique digital certificate of
ownership [2]. NFTs are often traded in online marketplace
with cryptocurrencies such as Bitcoin and Ethereum [3]. The
advent of NFTs in recent years has created a new medium for
artists and creators to showcase and sell their creations [4] [5].
CryptoPunks was one of the earliest NFT projects devel-
oped on the Ethereum blockchain, which consists of 10,000
algorithmically generated portraits [6]. Then in November
2017, a group of virtual cat NFTs known as CryptoKitties
became popular in Ethereum’s gaming scene, where players
are allowed to purchase, collect, breed and sell their cats [7]. In
December 2017, the craze of CryptoKitties caused Ethereum
network to crash several times [8]. In 2021, the NFT market
showed a second round of exponential growth with sales
volume totalled $17.6 billion, which is more than 10 times
than that in 2020 [9]. In this context, many auction houses
announced that they began to accept digital art NFT auction
[10]. In March 2021, Jack Dorsey (CEO of Twitter) raised $2.9
million for charity by auctioning his first tweet [11]; the artist
Beeple’s NFT work ”Daily: The First 5,000 Days” sold for
$69.34 million at Christie’s, making it the third most expensive
artwork ever sold at auction [12]. The strong profitability and
potential business opportunities of NFT prompt celebrities to
create and sell their NFT peripherals [13]. A typical example
is that NFT peripherals of NBA players and famous football
players are sold for hundreds of thousands of dollars [9].
Previous researches point out that NFT sales are shaped by
several factors [14] including but not limited to visual content,
trader behavior and creator reputation. Thus, it is possible to
utilize multimodal data including visual information such as
images and videos of NFTs as well as non-visual information
such as category, collection and historical sales of NFTs which
serve as proxy variables for creator reputation and trader
behavior to jointly predict price in future transactions. Most
past researches do not consider the visual aspect of NFTs when
evaluating their price. With recent development of computer
vision and multimodal learning, visual and non-visual informa-
tion can be retrieved from online trading platforms, processed
and fused together to improve the predictability of price.
In this study, a two-stage machine learning framework
and an end-to-end deep learning architecture is proposed to
jointly learn from visual and non-visual information and better
predict the price of NFTs. The performance of different models
trained using these two approaches are compared, and the
effect of different fusion operations on models’ performance
is investigated. To examine the effectiveness of multimodality,
models trained on unimodal and multimodal data are analysed.
The top ten most influential features are also presented to
identify influential predictors in this price prediction task.
II. RELATED WORK
Since NFT is a recent phenomenon, there are limited re-
searches in NFT related fields. Some previous researches focus
on the technical aspect of NFT and blockchain, such as the
protocols, standards and security. For example, Sylve Chevet
et.al [15] analyzed the technical underpinnings of cryptocur-
rencies and blockchain, specifically of NFTs and other crypto-
74
2022 IEEE International Conference on e-Business Engineering (ICEBE)
978-1-6654-9244-7/22/$31.00 ©2022 IEEE
DOI 10.1109/ICEBE55470.2022.00022
20
22
 IE
EE
 In
te
rn
at
io
na
l C
on
fe
re
nc
e 
on
 e
-B
us
in
es
s E
ng
in
ee
rin
g 
(IC
EB
E)
 |
 9
78
-1
-6
65
4-
92
44
-7
/2
2/
$3
1.
00
 ©
20
22
 IE
EE
 |
 D
O
I: 
10
.1
10
9/
IC
EB
E5
54
70
.2
02
2.
00
02
2
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:47:19 UTC from IEEE Xplore.  Restrictions apply. 
collectibles as well as the changes these innovations can bring
about in the online art market. Qin Wang et.al [16] offered
a systematic study of the current NFT ecosystems including
the technical components, security and the future prospect and
trends of the system.
Besides, the sudden rise of NFT also sparked an interest
in understanding the correlation between the NFT market and
the cryptocurrency market. For example, Lennart Ante et.al
[3] investigated the interrelationships between NFT sales, NFT
users and price of Bitcoin and Ether based on daily data
from January 2018 to April 2021. Their findings show that
a Bitcoin price shock can trigger an increase in NFT sales
while Ether price shocks reduce the number of active NFT
wallets, indicating the cryptocurrency market affect the growth
and development of the NFT market.
The valuation of NFT is one of the most emerging topics in
NFT related fields. Matthieu Nadini et.al [14] built machine
learning models to investigate the predictability of NFT sales
using sales history and visual features but ignored the social
media features. Arnav Kapoor et.al [17] gauged the impact of
Twitter features on an NFT’s value. They modeled the problem
as a multi-class classifcation task and their results show that
social media features including count of user membership lists,
number of likes and retweets can improve the accuracy by 6%
over baseline models that use only NFT platform features.
III. DATA
The data used in this study is collected from OpenSea [18],
the world’s largest digital marketplace for crypto collectibles
and non-fungible tokens. The data contains both visual and
non-visual information of 9,045 NFTs from six collections in-
cluding Art Blocks, Hashmasks, OpenStoreShare, CryptoKitty,
SuperRare and Sorare that were sold at OpenSea [18] in April
2021. Visual information includes the image or video of each
NFT. Non-visual information includes the unique item id,
category, collection, total number of bids in history and total
number of withdrawal of bids in history of each NFT. Detailed
description of non-visual information is presented in Table. I.
In the preprocessing stage, since some NFTs are in the form
of images (.jpeg or .png files) while others are in the form
of videos (.gif or .mp4 files), we pause videos at a random
timestamp and take the snapshots to treat all videos as static
images in the following analysis. We then delete NFTs with
either visual or non-visual information missing. After that,
we detect and remove outliers by applying the IQR (inter
quartile range) method to numerical variables. To be more
specific, for each numerical variable we take 1.5 times the
inter quartile range and then subtract this value from the first
quartile and add this value to the third quartile, which gives us
the minimum and maximum fence posts that we compare each
observation to. Any observations that are out of this range are
considered outliers need to be removed. To this end, 355 NFTs
are removed and the rest 8,690 of them remain in the data.
The descriptive statistics of the price of NFTs from different
collections after preprocessing is presented in Table. II. Note
that price is measured in U.S. dollars.
IV. MODEL ARCHITECTURES
In this section, a two-stage machine learning architecture
and an end-to-end deep learning architecture is proposed to
jointly learn from visual and non-visual information of NFTs
to better predict their price.
A. Two stage model
The framework of the two-stage model is displayed in
Fig. 1. In the first stage, a pretrained CNN (convolutional
neural network) image encoder is used to extract features from
NFT images. Since the dimensionality of image features is
generally much larger than that of non-visual features, PCA
(principal component analysis) [19], a widely used dimen-
sionality reduction algorithm with the key idea of projecting
features to a lower-dimensional space that preserves maximal
data variance, is used to reduce the dimensions of visual
features. Otherwise, the high dimensional visual features may
overwhelm the low dimensional non-visual features when
fused together. Besides, high dimensional visual features are
also computationally expensive and can significantly slow
down the model training and testing process. In the second
stage, non-visual features and visual features are fused together
by concatenation and fed into machine learning models to
output the predicted price for each NFT. Note that model
training and hyperparameter tuning are implemented using 10-
fold cross validation. RMSE (root mean squared error) and R
squared of the testing set are calculated as evaluation metrics
of model performance.
In this article, we use pretrained VGG-16 convolutional
neural network [20] as the image encoder. In terms of machine
learning models, we use four different models in the exper-
iments, including two single models (elastic net regression
and polynomial regression) and two ensemble models (random
forest and XGBoost). Elastic net regression is a type of
penalized linear regression model that includes both the L1 and
L2 penalties in its loss function. Polynomial regression is an
extension of linear regression which fits non-linear polynomial
relationships between independent variables and the dependent
variable. Random forest is a supervised ensemble learning
algorithm which aggregates predictions from multiple decision
trees to make a final prediction with higher accuracy and lower
variance than a single model. XGBoost is another decision
tree based ensemble learning algorithm, which builds weak
learners sequentially where each model tries to predict the
error left over by the previous model to make a more accurate
final prediction.
B. End-To-End Model
The framework of the end-to-end model is presented in
Fig. 2, which consists of three modules: a self-attentive
visual encoder, a self-attentive non-visual encoder and a cross-
attentive feature fusion module. In the self-attentive visual
encoder, the input image goes through a convolutional neural
network for feature extraction, and then passes the CBAM
(Convolutional Block Attention Module) [21]. The CBAM
uses both channel-wise and spatial-wise attention to relate
75
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:47:19 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I
DESCRIPTION OF NON-VISUAL INFORMATION IN THE DATA
Feature Name Description
Item ID The unique item id of the NFT.
Category The category of the NFT, which includes artwork, collectible and game-item.
Collection The collection of the NFT, which includes Art Blocks, Hashmasks, OpenStoreShare, CryptoKitty, SuperRare and Sora
Total Number of Bids in History The total number of bids of the NFT from the time it was created to April, 2021.
Total Number of Withdrawal of Bids in History The total number of withdrawal of bids of the NFT from the time it was created to April 2021.
TABLE II
DESCRIPTIVE STATISTICS OF PRICE OF NFTS FROM DIFFERENT COLLECTIONS
Art Blocks Hashmasks OpenStoreShare CryptoKitty SuperRare Sorare
count 1955 1679 2378 2243 185 250
mean 1057.01 2300.91 192.71 32.61 467.41 161.88
std 837.31 784.96 367.71 133.41 700.89 197.22
min 42.98 358.63 0.03 0.01 0.24 2.42
25% 450.74 1597.44 33.64 0.42 60.26 52.16
50% 774.06 2270.85 85.95 1.27 165.68 88.24
75% 1406.63 2987.29 184.00 12.20 500.61 186.39
max 3702.28 3702.28 3697.38 3598.28 3399.78 1514.45
Fig. 1. Framework of the two-stage model
different parts of visual features together, thus empowering the
learning of a more effective visual representation. In the self-
attentive non-visual encoder, the input non-visual features go
through a fully connected layer to embed features into a latent
embedding space. After that, a multi-head self-attention layer
is used to interact the non-visual features among themselves,
which helps to learn a more effective non-visual representa-
tion. After obtaining non-visual representation from the upper
branch and visual representation from the lower branch, cross
attentions are used to perform cross modality fusion. More
specifically, two multi-head cross attentions are computed in
the fusion module: one uses non-visual representation as query
and visual representation as key and value, while the other uses
visual representation as query and non-visual representation
as key and value. Then, the two cross attention outputs are
concatenated and fed into a multi-layer perceptron for final
prediction. Note that the entire framework is trained in an end-
to-end manner, and we use pretrained VGG-16 as the CNN
image encoder in the experiments.
V. EXPERIMENTS
A. Experimental Setups
In this section, we train and test different two-stage and
end-to-end models on our data. Training and testing data are
obtained by randomly splitting the original data into training
(90%) and testing set (10%). Note that in terms of two-stage
models, we use 10-fold cross validation for model training
and hyperparameter tuning. In terms of end-to-end models,
all models are trained using PyTorch 1.10.0 under identical
training regimes: same optimizer, learning rate, batch size etc.
In this article, we adopt RMSE (root mean squared error) and
R squared of the testing set as evaluation metrics.
B. Experiments of Two-Stage Model
1) Model Results: The experimental results of two-stage
model are presented in Table. III. Several conclusions can
be drawn: First, the VGG-RF model trained on multimodal
features achieves the best predictive performance with R
squared 69.75% and RMSE 565.49 on the testing set. Another
76
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:47:19 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III
EXPERIMENTAL RESULTS OF TWO-STAGE MODELS ON VISUAL, NON-VISUAL AND VISUAL & NON-VISUAL DATA
Model Modality Train R2(%) Train RMSE Test R2(%) Test RMSE
Visual & Non-Visual 64.15 610.85 63.30 622.81
VGG-ENR Non-Visual 63.19 618.98 62.73 627.62
Visual 40.85 784.63 43.01 776.13
Visual & Non-Visual 71.06 548.87 66.74 592.91
VGG-PR Non-Visual 71.12 548.26 67.56 585.59
Visual 60.34 642.50 58.71 660.62
Visual & Non-Visual 95.81 208.78 69.75 565.49
VGG-RF Non-Visual 78.30 475.25 64.36 613.82
Visual 94.70 234.85 62.45 630.01
Visual & Non-Visual 96.29 196.45 68.03 581.30
VGG-XGB Non-Visual 95.94 205.63 66.83 592.09
Visual 94.12 247.41 58.33 663.66
*ENR stands for Elastic Net Regression, PR stands for Polynomial Regression, RF stands for Random Forest and XGB stands for XGBoost.
ensemble model, the VGG-XGB model, also achieves a decent
performance with R squared 68.03%. Results of these two
models indicate that multimodal features can explain approx-
imately 70% of variability in the price of NFTs and can be
used to make fairly trustworthy predictions. Second, in terms
of single models, polynomial regression significantly outper-
forms elastic net regression, indicating non-linear relationships
between independent variables and dependent variable are
fairly helpful for model prediction. Third, models trained
on multimodal features generally outperform corresponding
ones trained on either visual or non-visual unimodal features,
which demonstrates the effectiveness of multimodality. Last
but not least, models trained on unimodal non-visual features
outperform corresponding ones trained on unimodal visual
features. This indicates non-visual features are relatively more
important predictors compared with visual ones in the NFT
price prediction task.
TABLE IV
TOP TEN INFLUENTIAL FEATURES OF THE BEST TWO-STAGE MODEL
Feature Name Importance Score
Total Number of Bids in History 0.5918
Collection 0.1258
Visual Component 2 0.0591
Visual Component 0 0.0591
Visual Component 4 0.0497
Visual Component 3 0.0477
Visual Component 1 0.0473
Total Number of Withdrawal of Bids in History 0.0152
Category 0.0040
Quantity 0.0002
2) Analysis of Feature Importance: In this section, we
analyze the feature importance of the VGG-RF model since it
gives the best performance among all the two-stage models.
Top ten influential features and the corresponding importance
score are displayed in Table. IV. Results show that non-visual
features ‘Total Number of Bids in History’ and ‘Collection’ are
dominant predictors with importance score 0.5918 and 0.1258
respectively, indicating the price of a NFT is strongly related
to its historical sales as well as the collection it comes from.
Five visual features, which are extracted by convolutional
neural network and transformed by PCA (principal component
analysis), take up the 3rd to the 7th place with feature
importance score around 0.05. This indicates while the two
non-visual features ‘Total Number of Bids in History’ and
‘Collection’ play a decisive role, the visual features also play
an auxiliary role in model prediction. This demonstrates the
visual aspect also has an impact on the price of NFTs and
therefore, it is reasonable to regard NFT price prediction as a
multimodal task.
C. Experiments of End-To-End Model
1) Effectiveness of Attention Module: The experimental
results of ablation study of attention modules in the end-to-
end model are presented in Table. V. Note that we use simple
concatenation to perform feature fusion in the model with self-
attention but without cross-attention. Results show the model
with both self and cross attention gives the best performance
of R squared 69.25% and RMSE 569.13, followed by the one
with only self-attention which achieves R squared 67.01%.
This shows a 2.24% performance degradation without cross
attention. The model with cross-attention but without self-
attention gives a even worse performance with R squared
60.74%, showing a significant performance degradation of
8.51% in terms of R squared. This indicates the two self-
attentions which facilitate intra-modal interactions are rela-
tively more important than the cross-attention which boosts
inter-modal interactions in this price prediction task. Overall,
the performance degradation when either self-attention or
cross-attention is removed indicates the significance of the
attention modules in the end-to-end architecture.
2) Unimodality versus Multimodality: The experimental
results of ablation study of modality in the end-to-end model
are presented in Table. VI. Several conclusions can be drawn:
First, the model trained on multimodal features outperform
ones trained on either non-visual or visual unimodal features,
which demonstrates the effectiveness of multimodality. Sec-
ond, the model trained on only non-visual features shows a
slight performance degradation of R squared 0.93%, while
model trained on only visual features shows a significant
performance degradation of R squared 9.16% compared with
the orginal model trained on multimodal features. Similar as
two-stage models, this indicates while both non-visual and
77
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:47:19 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 2. Framework of the end-to-end model
TABLE V
RESULTS OF ABLATION STUDY OF ATTENTION MODULE IN THE END-TO-END MODEL
Attention Module Train R2(%) Train RMSE Test R2(%) Test RMSE
Cross & Self-Att 77.83 483.59 69.25 569.13
Self-Att 92.12 278.51 67.01 590.52
Cross-Att 91.03 304.11 60.74 644.23
*The bold indicates the best results on testing set.
TABLE VI
RESULTS OF ABLATION STUDY OF MODALITY IN THE END-TO-END MODEL
Modality Train R2(%) Train RMSE Test R2(%) Test RMSE
Visual&Non-Visual 77.83 483.59 69.25 569.13
Non-Visual 71.02 552.65 68.32 575.68
Visual 91.72 290.83 60.09 649.21
*The bold indicates the best results on testing set.
visual features play a role in the prediction of price, the role
of non-visual features is relatively more significant than that
of visual ones.
VI. CONCLUSION AND FUTURE WORK
This study utilizes two-stage and end-to-end architectures
to jointly learn from visual and non-visual information and
predict the price of NFTs. In the two-stage model, we start
by extracting visual features using pretrained convolutional
networks and use principal component analysis to reduce
the dimensionality of visual features. After that, visual and
non-visual features are fused together and fed into machine
learning models to make final predictions. In the end-to-end
model, visual and non-visual features first go through separate
encoders with self-attention, and then fused together using two
cross-attention modules. Models trained on different modali-
ties are compared to verify the effectiveness of multimodality.
Ablation study of attention modules is conducted to examine
the impact of self attention and cross attention on model
performance. The top ten influential features are also analysed,
which helps to identify the most influential predictors in this
price prediction task. Main conclusions of this study can be
summarized as follows:
• The best two-stage model, the VGG-RF model trained
on visual and non-visual features, achieves a decent R
squared of 69.75% and RMSE of 565.49. This shows
that the multimodal features successfully explain approx-
imately 70% of variability in the price of NFTs and can
make fairly trustworthy predictions for future price with
low RMSE.
• In terms of single two-stage models, results indicate that
polynomial regression significantly outperforms elastic
net regression. Thus, non-linear relationships between
multimodal features and the price of NFTs are fairly
important for model prediction.
• The best end-to-end model achieves R squared 69.25%
and RMSE 569.13, which performs slightly weaker than
the best two-stage model. Ablation study of attention
modules shows self-attention which boosts intra-modal
interaction play a relatively more important role than
cross-attention which facilitate inter-modal interaction in
78
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:47:19 UTC from IEEE Xplore.  Restrictions apply. 
the end-to-end model.
• Models trained on multimodal features outperform corre-
sponding ones trained on unimodal features in both two-
stage and end-to-end models. This verifies the impact of
visual aspects on the price of NFTs and the necessity of
regarding the price prediction as a multimodal task.
Based on this study, future researches can utilize more com-
prehensive multimodality data including textual reviews data
from social media platforms and time series data of historical
sales to examine if they can further improve the predicability
of the price of NFTs. More advanced feature fusion techniques
such as graph-based fusion can also be utilized to fuse the mul-
timodal features to achieve better performance. Besides, since
the variations and predictability of the price of NFTs from
different collections may be very different, future researches
can build collection-specific prediction models to achieve more
accurate price or sales predictions.
REFERENCES
[1] Chohan, U. W. (2021). Non-fungible tokens: Blockchains, scarcity, and
value. Critical Blockchain Research Initiative (CBRI) Working Papers.
[2] https://doi.org/10.48550/arXiv.2105.07447
[3] Ante, L. (2022). The non-fungible token (NFT) market and its relation-
ship with Bitcoin and Ethereum. FinTech, 1(3), 216-224.
[4] Kugler, L. (2021). Non-fungible tokens and the future of art. Commu-
nications of the ACM, 64(9), 19-20.
[5] Wilson, K. B., Karg, A., Ghaderi, H. (2021). Prospecting non-fungible
tokens in the digital economy: Stakeholders and ecosystem, risk and
opportunity. Business Horizons.
[6] Frye, B. L. (2022). Are CryptoPunks Copyrightable?. Available at SSRN
4029323.
[7] Arslanian, H. (2022). Non-Fungible Tokens. In The Book of Crypto (pp.
249-257). Palgrave Macmillan, Cham.
[8] De Domenico, M., Baronchelli, A. (2019). The fragility of decentralised
trustless socio-technical systems. EPJ Data Science, 8(1), 1-6.
[9] https://nonfungible.com/reports/2021/en/yearly-nft-market-report
[10] Fazli, MohammadAmin, Ali Owfi, and Mohammad Reza Taesiri. ”Under
the skin of foundation nft auctions.” arXiv preprint arXiv:2109.12321
(2021).
[11] Okonkwo, I. E. (2021). NFT, copyright and intellectual property com-
mercialization. International Journal of Law and Information Technol-
ogy, 29(4), 296-304.
[12] Cera, A. Ethics as Research and Profession.
[13] Pawelzik, L., Thies, F. (2022). SELLING DIGITAL ART FOR
MILLIONS-A QUALITATIVE ANALYSIS OF NFT ART MARKET-
PLACES.
[14] Nadini, M., Alessandretti, L., Di Giacinto, F. et al. Mapping the NFT
revolution: market trends, trade networks, and visual features. Sci Rep
11, 20902 (2021). https://doi.org/10.1038/s41598-021-00053-8
[15] Chevet, S. (2018). Blockchain technology and non-fungible tokens: Re-
shaping value chains in creative industries. Available at SSRN 3212662.
[16] Wang, Q., Li, R., Wang, Q., Chen, S. (2021). Non-fungible to-
ken (NFT): Overview, evaluation, opportunities and challenges. arXiv
preprint arXiv:2105.07447.
[17] Kapoor, A., Guhathakurta, D., Mathur, M., Yadav, R., Gupta, M.,
Kumaraguru, P. (2022). TweetBoost: Influence of Social Media on NFT
Valuation. arXiv preprint arXiv:2201.08373.
[18] https://opensea.io/
[19] Abdi, H., Williams, L. J. (2010). Principal component analysis. Wiley
interdisciplinary reviews: computational statistics, 2(4), 433-459.
[20] Simonyan, K., Zisserman, A. (2014). Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556.
[21] Woo, S., Park, J., Lee, J. Y., Kweon, I. S. (2018). Cbam: Convolutional
block attention module. In Proceedings of the European conference on
computer vision (ECCV) (pp. 3-19).
79
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:47:19 UTC from IEEE Xplore.  Restrictions apply.