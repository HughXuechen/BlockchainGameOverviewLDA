Improved Reduction from the Bounded Distance
Decoding Problem to the Unique Shortest Vector
Problem in Lattices∗†
Shi Bai1, Damien Stehlé2, and Weiqiang Wen3
1 ENS de Lyon, Laboratoire LIP (U. Lyon, CNRS, ENSL, INRIA, UCBL), Lyon,
France
shi.bai@ens-lyon.fr
2 ENS de Lyon, Laboratoire LIP (U. Lyon, CNRS, ENSL, INRIA, UCBL), Lyon,
France
damien.stehle@ens-lyon.fr
3 ENS de Lyon, Laboratoire LIP (U. Lyon, CNRS, ENSL, INRIA, UCBL), Lyon,
France
weiqiang.wen@ens-lyon.fr
Abstract
We present a probabilistic polynomial-time reduction from the lattice Bounded Distance Decoding
(BDD) problem with parameter 1/(
√
2 · γ) to the unique Shortest Vector Problem (uSVP) with
parameter γ for any γ > 1 that is polynomial in the lattice dimension n. It improves the BDD
to uSVP reductions of [Lyubashevsky and Micciancio, CRYPTO, 2009] and [Liu, Wang, Xu
and Zheng, Inf. Process. Lett., 2014], which rely on Kannan’s embedding technique. The main
ingredient to the improvement is the use of Khot’s lattice sparsification [Khot, FOCS, 2003]
before resorting to Kannan’s embedding, in order to boost the uSVP parameter.
1998 ACM Subject Classification F.2.1 Numerical Algorithms and Problems
Keywords and phrases Lattices, Bounded Distance Decoding Problem, Unique Shortest Vector
Problem, Sparsification
Digital Object Identifier 10.4230/LIPIcs.ICALP.2016.76
1 Introduction
A (full-rank) lattice L in dimension n is the set of all integer linear relations of n linearly
independent vectors b1, . . . ,bn ∈ Qn. The minimum λ1(L) quantifies the discreteness of L:
the smallest Euclidean distance between two distinct lattice vectors is λ1(L). A standard
computational problem on lattices is the so-called Bounded Distance Decoding problem
(BDDα): Given as inputs a basis B = (bi)i of a lattice L and a vector t ∈ Qn (called
target vector) within distance α · λ1(L) of L, the goal is to find a vector b ∈ L closest to t.
Here α > 0 is a problem parameter, which may be a function of the lattice dimension n. The
hardness of BDD was initially studied in the context of linear codes by Vardy in [22], and
later in the context of lattices by Liu et al. in [14].
In communications theory, BDD models the task of decoding in the context of continuous
channels with white Gaussian noise [6]. The information to be transmitted is stored in a
∗ A full version of the paper is available at http://eprint.iacr.org/2016/753.
† This work has been supported by ERC Starting Grant ERC-2013-StG-335086-LATTAC.
EA
T
C
S
© Shi Bai, Damien Stehlé, and Weiqiang Wen;
licensed under Creative Commons License CC-BY
43rd International Colloquium on Automata, Languages, and Programming (ICALP 2016).
Editors: Ioannis Chatzigiannakis, Michael Mitzenmacher, Yuval Rabani, and Davide Sangiorgi;
Article No. 76; pp. 76:1–76:12
Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany
http://dx.doi.org/10.4230/LIPIcs.ICALP.2016.76
http://eprint.iacr.org/2016/753
http://creativecommons.org/licenses/by/3.0/
http://www.dagstuhl.de/lipics/
http://www.dagstuhl.de
76:2 Improved Reduction from BDD to USVP in Lattices
lattice vector, and the receiver should recover this vector from a noisy version thereof. The
knowledge of the signal-to-noise ratio implies a bound on the distance from the noisy vector
to the lattice. Decoding a white Gaussian noise channel can be seen as a version of BDD in
which the distance to the lattice follows a prescribed distribution.
In cryptography, BDD is closely related to the Learning With Errors problem (LWE) [19],
which serves as a security foundation for numerous cryptographic primitives. When the
number of requested LWE samples is bounded (which is most often the case in cryptographic
constructions), LWE may be viewed as a variant of BDD in which the offset from t to L is
Gaussian (like in the decoding context), and L is randomly sampled.
A common approach to solve BDD is via Kannan’s embedding technique [9, Se. 6]. The
principle is to map the offset between t and a closest lattice vector to t, to a shortest
non-zero vector in an (n+ 1)-dimensional lattice. Lattice reduction [12, 20] and short lattice
vector enumeration [8, 7] may then be used to find shortest non-zero vectors in the (n+ 1)-
dimensional lattice. Formally, Kannan’s embedding technique is a reduction from BDD to a
variant of the Shortest Vector Problem (SVP) in which the pair of shortest non-zero vectors
in the lattice under scope are known to be much shorter than any other lattice vector not
parallel to them. For a lattice L, we define the second minimum λ2(L) as the minimal radius
of a zero-centered ball that contains two or more linearly independent vectors from L. The
unique Shortest Vector Problem (uSVPγ) of parameter γ ≥ 1 consists in finding a shortest
non-zero vector in a lattice L described by an input basis B = (bi)i, under the promise
that λ2(L) ≥ γ · λ1(L). This reduction was analyzed by Lyubashevsky and Micciancio
in [15], who showed that BDD1/(2γ) reduces to uSVPγ for any γ ≥ 1. Later, Liu et al. [13]
refined the analysis of Lyubashevsky and Micciancio and proved that BDD1/γ1 reduces
to uSVPγ with γ1 =
√
3/(4− γ2)γ + 1, for any γ ∈ (1, 1.9318). It is folklore [3, 17] that the
analysis can be tightened even more, resulting in a proof that BDD1/γ1 reduces to uSVPγ
with γ1 = (2γ2 + 2bγcbγ + 1c)/(2bγc+ 1), for any γ ≥ 1. For the sake of completeness, we
give a proof in Appendix A.1 of the full version. Note that in the case of γ = 1 (and in fact
all integral γ), all three results are identical: BDD1/2 reduces to uSVP1.
Our result. We give a probabilistic polynomial-time reduction from BDD1/(
√
2γ) to uSVPγ ,
for any γ ≥ 1 that is polynomially bounded as a function of n. As clearly visible in Figure 1,
this reduction supersedes all prior results with respect to the BDD problem parameter. In
particular, we reduce BDD1/
√
2 to uSVP1. Our improvement comes with two weaknesses: the
reduction is probabilistic and restricted to polynomially bounded γ. Like prior reductions, the
dimension of the uSVP instance is only one more than the dimension of the BDD instance.
Technical overview. We illustrate our improvement with the case of BDD1/2. Given the
BDD1/2 instance (B, t), Kannan’s embedding consists in constructing the following uSVP1
instance:
B′ =
(
B t
0 d
)
∈ Qn+1,
with d = dist(t,L) ≤ λ1(L)/2, where L is the lattice spanned by B (in fact, the reduction
does not know d, but this is a mere technical problem which can be handled easily, as
explained in Section 2). If c denotes a closest vector to t in L then it may be proved that
the vector s′ = ((c− t)T,−d)T is a shortest non-zero vector of lattice L′ of basis B′. Now,
let s denote a shortest non-zero vector in L and assume that t is exactly halfway between c
and c + s. Then both s′ and s′+ (sT, 0)T in L′ have norm
√
2 ·d but are linearly independent.
S. Bai, D. Stehlé, and W. Wen 76:3
Folklore [3,17]
Theorem 12
0.1
0.2
0.3
0.4
0.5
0.6
0.7
1 1.2 1.4 1.6 1.8 2 2.2 2.4
α
γ
Lyubashevsky and Micciancio [15]
Liu et al. [13]
Figure 1 Comparison between prior reductions from BDDα to uSVPγ , and ours.
0
w
c
c+w
c+ s
c+ s+w
t
t+w
Figure 2 An example of sparsification for BDD1/2 (here w ∈ Lp,z/L).
This shows that we can have λ2(L′) = λ1(L′) and obtain a uSVPγ instance with γ = 1. This
is a limitation of Kannan’s embedding and hence of its analyzes.
We modify the reduction to increase the ratio λ2(L′)/λ1(L′). To achieve this, we use
lattice sparsification on L. It provides a full-rank sublattice Lsparse ⊆ L that still contains a
closest vector c ∈ L to t, but no other close-by vector. We consider the vectors of L whose
coordinates with respect to a basis B satisfy a linear equation modulo some prime integer p:
Lsparse = Lp,z = {b ∈ L : 〈z,B−1b〉 = 0 mod p}, for some vector z ∈ Znp . This technique was
first introduced by Khot in [10, 11]. To guarantee that vectors in L remain in the sparsified
set with probability close to 1/p, a uniform coset of Lp,z (modulo L) was considered in [4, 5].
Technically, we use the formulation from [21] of the latter variant.
The aim of sparsification in our context is to keep a closest vector c ∈ L to t, and remove
as many as nearby vectors of L as possible. After sparsification, vector c remains in the
sparse lattice (with non-negligible probability), and all other remaining vectors are much
further away from t. For BDD1/2, a simple example of sparsification is shown in Figure 2:
there are two points simultaneously closest to the target point; then sparsification is used to
remove one of the closest points (either is fine); in the sparse lattice, the closest vector is
much closer than any other lattice vector. In Figure 2, after sparsification, all the lattice
vectors labelled with filled dots are kept, e.g., vector c + w, and other vectors labelled with
hollow dots are removed, e.g., vector c + s + w.
The probability of keeping a vector of L in the sparsified set is essentially 1/p. As we want
the probability of keeping c to be non-negligible, we are hence restricted to taking p ≤ poly(n).
As a result, we cannot remove more than polynomially many close-by vectors, because each
ICALP 2016
76:4 Improved Reduction from BDD to USVP in Lattices
one individually is removed with probability ≈ 1 − 1/p (a precise statement is given in
Lemma 8). To assess the limitation of our reduction, we are hence interested in the largest
value of α such that for any lattice L and any vector t, there are at most poly(n) vectors
within distance α · λ1(L) from t. The quantity α · λ1(L) can be viewed as the worst-case
list-decoding radius. Interestingly, this problem was studied by Ajtai [1] and Micciancio [16]
in the context of proving hardness of SVP. Proofs of the following two statements may be
found in [18, Chap. 5]:
For any lattice L and vector t, there are ≤ 2n vectors of L within distance λ1(L)/
√
2
from t.
For any α > 1/
√
2, there exists ε > 0 such that for any sufficiently large n we can find
an n-dimensional lattice L and a vector t such that there are ≥ 2nε vectors of L within
distance α · λ1(L) from t.
The overall reduction consists in first sparsifying L to Lp,z and shifting t (as we use a coset
of Lp,z), and then resorting to Kannan’s embedding. To increase the ratio λ2(L′)/λ1(L′), we
decrease the bottom-right entry in B′ from d to k · d for some k < 1. Geometrically, this
has the effect of limiting the contribution of the extra dimension. This idea was already
used in [13], but we decrease k even further, to 1/poly(n). An additional difficulty, related
to this decrease of k, is that short vectors in L′ may be obtained by using multiples of t.
Let m ≥ 2 and d ∈ L closest to mt. Then, vector ((d−mt)T,mkd)T may be very short (if
very unlucky, it has norm mkd). We remove such annoying vectors with sparsification.
Open problems. In [15], Lyubashevsky and Micciancio considered the relative hardness
of BDD and uSVP. They obtained a reduction from BDD1/(2γ) to uSVPγ , and a reduction
from uSVPγ to BDD1/γ (for all γ ≥ 1). This led them to conjecture that it may be possibly
to show that (i)- uSVPγ/2 reduces to BDD1/γ , or (ii)- BDD1/γ reduces to uSVPγ , or (iii)-
uSVPγ reduces to BDD1/(
√
2γ) and BDD1/(
√
2γ) reduces to uSVPγ . By showing the second
half of (iii), (i) becomes very unlikely.
Independently, it would be interesting to make our reduction deterministic and let it work
even for parameters γ that are not ≤ poly(n).
Notation. For a lattice L, a point t, a radius r, we define B(t, r) = {x : ‖x− t‖ ≤ r}. We
let dist(t,L(B)) denote the distance between t and lattice L(B). We always represent the
basis of lattice in column form. If S is a finite set, we let #S denote its cardinality.
2 Reminders
In this section, we recall basic facts on lattices and lattice problems. We then consider lattice
sparsification and its use in the context of BDD instances.
2.1 Lattice problems
We refer the reader to [18] for an introduction to the computational aspects of lattices.
I Definition 1 (Lattice). An n-dimensional lattice L ⊆ Qm (m ≥ n) is a discrete additive
subgroup of Rm. The lattice L is the set of all integral linear combinations of n linearly
independent basis vectors B = {b1, · · · ,bn} ⊆ Qm. In other words, we have
L(B) =
∑
i∈[n]
uibi : u ∈ Zn
 .
S. Bai, D. Stehlé, and W. Wen 76:5
I Definition 2 (Successive minima). For any lattice L, the i-th minimum λi(L) is the radius
of the smallest ball with center 0 and containing i linearly independent lattice vectors:
λi(L) = inf{r : dim(span(L ∩ B(0, r))) ≥ i}.
In this work, we investigate the respective hardness of uSVPγ and BDDα defined below,
when the lattice dimension n goes to infinity. The problem parameters γ and α can be
functions of n.
I Definition 3 (Unique Shortest Vector Problem (uSVPγ)). Let γ ≥ 1. Given as input a
lattice basis B such that λ2(B) ≥ γ · λ1(B), the goal is to find a non-zero vector v ∈ L(B)
of norm λ1(L(B)). The Shortest Vector Problem (SVP) corresponds to γ = 1.
In the literature (in [15], for example), uSVP is sometimes be defined with a strict lower
bound on λ2(B). We allow equality (as in [13]), as it is more convenient in our proofs. Note
that Lemma 5 below implies that these two variants are equivalent.
I Definition 4 (Bounded Distance Decoding (BDDα)). Let α > 0. Given as inputs a lattice
basis B and a vector t such that dist(t,L(B)) ≤ α · λ1(B), the goal is to find a lattice vector
v ∈ L(B) closest to t.
Note that in some works, the range of α is restricted to (0, 1/2). This is to guarantee that
there is exactly one element of L in the ball of radius α · λ1(L) centered on t. The problem
is well-defined even for large α, and in this work we actually consider α ≥ 1/2.
In the next lemma, it is stated that BDDα is equivalently hard for any parameter α′ that
is within a factor (1− 1/n)c of α, for any constant c.
I Lemma 5 ([15, Cor. 2]). For any α > 0, any constant c > 0, there is a polynomial-time
reduction from BDDα to BDDα(1−1/n)c .
2.2 Approximation results
Given as input an n-dimensional lattice basis B ∈ Qn×n, it is possible to find a non-zero
vector that has norm at most 2n/2 · λ1(L(B)) in time polynomial in n and also the bit-sizes
of the entries of B, by using the LLL algorithm [12]. Further, by using the Babai round-off
algorithm [2] with inputs an n-dimensional lattice basis B ∈ Qn×n and a target vector t ∈ Qn,
one obtains an approximation of the distance between t and L(B) within a factor 2n/2 in
time polynomial in n and also the bit-sizes of the entries of B and t.
I Lemma 6 ([12, Prop. 1.6]). There exists a polynomial-time algorithm that, given as input
an n-dimensional lattice basis B ∈ Qn×n, outputs ` ∈ λ1(L) · [1, 2n/2).
I Lemma 7 ([2, Thm. 3.1]). There exists a polynomial-time algorithm that, given as input an
n-dimensional lattice basis B ∈ Qn×n and a target vector t ∈ Qn, outputs d ∈ dist(t,L(B)) ·
[1, 2n/2).
We will rely on much tighter approximations to λ1(L(B)) (resp. dist(t,L(B))) than
provided by Lemmata 6 and 7. We explain here why we may assume that we know ` ∈
λ1(L(B)) · [1, 1/(1− 1/n)) (resp. d ∈ dist(t,L(B)) ∈ [1, 1/(1− 1/n))).
Our reduction is from BDD, whose candidate solutions can be compared in polynomial
time. Assume the reduction finds the optimal solution in one case among polynomially many,
but that we do not know which one. Then we may call the reduction this polynomially many
times, and keep a best solution among the returned ones. Concretely, our reduction will be
ICALP 2016
76:6 Improved Reduction from BDD to USVP in Lattices
proved correct if we know a tight approximation to λ1(L(B)) and dist(t,L(B)), where (B, t)
is the BDD instance. We can assume without loss of generality that we have these tight
approximations, as the interval [1, 2n/2) may be covered by polynomially many intervals of
the form x · [1, 1/(1− 1/n)) for well-chosen rational x’s.
2.3 Lattice sparsification
Our techniques rely on lattice sparsification, and, more concretely, on the following lemma.
I Lemma 8 ([21, Cor. 2.16]). For any prime p, collection of vectors v1, · · · ,vN ∈ Znp \ {0},
and x /∈ {vi}i≤N , we have
1
p
− N
p2 −
N
pn−1 ≤ Pr
z,u←↩U(Znq )
[
∀i, 〈z,vi + u〉 6= 0 mod p
〈z,x + u〉 = 0 mod p
]
≤ 1
p
+ 1
pn
.
The upper bound in Lemma 8 is not used in this work, but we keep it to show that
the difference between the upper and lower bound is small, and thus that the lower bounds
is almost tight. Lemma 8 leads to the definition of a sublattice that will be used in our
reduction from BDD to uSVP. The lemma below explains that we can efficiently compute a
basis of the sublattice.
I Lemma 9. There exists a polynomial-time algorithm which, given as inputs a basis
B ∈ Qn×n of an n-dimensional lattice L, an integer p and a vector z ∈ Znp , outputs a basis
Bp,z of the lattice Lp,z = {x ∈ L | 〈z,B−1x〉 = 0 mod p}.
Proof. According to the definition of the lattice Lp,z, we have
〈z,y〉 = 0 mod p,
where y = B−1x and x ∈ Lp,z. We can obtain a basis S of the kernel y over Zn. We compute
the column Hermite normal form of
[
S pIn
]
∈ Zn×2n; and obtain the nonzero columns
S′ ∈ Zn×n. The columns of S′ generate the lattice orthogonal to z (mod p). In the end, we
compute Bp,z = BS′, which is a basis for the lattice Lp,z. J
Below, we state that for any two lattice vectors in L with distance smaller than p · λ1(L)
where p is an integer, the coordinates of these two lattice vectors differ modulo p.
I Lemma 10. For any basis B, any integer p and any pair of lattice vectors x 6= v with
‖x− v‖ < p · λ1(L(B)), we have that B−1x 6= B−1v mod p.
Proof. For all lattice vector a, we let ã denote its coordinate vector under the basis B.
Assume by contradiction that x̃ = ṽ mod p. Equivalently, we have x− v ∈ p · L(B).
Combined with x 6= v, we have ‖x − v‖ ≥ p · λ1(L), which is in contradiction with
‖x− v‖ < p · λ1(L). As a result, we have x̃ 6= ṽ mod p. J
The proof from [18] of the lemma below is by induction. It goes fast over a subtle counting
argument when reducing the problem in dimension n+ 1 to dimension n. We briefly recall
the proof and give more explanations on the counting argument in Appendix A.2 of the full
version.
I Lemma 11 ([18, Thm. 5.2]). For any n-dimensional lattice L and any vector t ∈ Qn, we
have #L ∩ B(t, λ1(L)/
√
2) ≤ 2n.
S. Bai, D. Stehlé, and W. Wen 76:7
We will use the above three lemmas in the following way to tackle BDD. Consider the
coordinate vectors (with respect to some arbitrary basis) of all lattice vectors in B(t, r)
with r = λ1(L)/
√
2 and some arbitrary target vector t. First, according to Lemma 10 with
p > 2
√
2, we can obtain that one of the coordinate vectors differs from all the others modulo p.
Further, by Lemma 8, a uniformly chosen vector z over Zp is orthogonal to exactly one of
the coordinate vectors (shifted by another uniformly chosen vector u) with non-negligible
probability. Assume that this orthogonal coordinates vector is the coordinates vector of a
closest lattice vector to t: this occurs with non-negligible probability as #L ∩ B(t, r) ≤ 2n.
We can consider the sublattice Lp,z, which contains just this BDD solution and none of the
other vectors of L ∩ B(t, r). This will help us ensuring a large gap between the first two
minima of the uSVP lattice in the BDD to uSVP reduction.
Note that u is necessary, as otherwise some superfluous vectors (including vector 0) could
be multiples of the solution vector and hence always stay in Lp,z if the solution vector does.
3 Reducing BDD1/(
√
2γ) to uSVPγ(1+ε)
In this section, we use a uSVPγ(1+ε) solver with ε = Ω(1/n) to solve BDD1/(
√
2γ).
I Theorem 12. Let γ(n) ≤ poly(n). There is a probabilistic polynomial-time reduction from
BDD1/(
√
2γ) to uSVPγ(1+ε), where ε = Ω(1/n).
Thanks to Lemma 5, it suffices to reduce BDD(1−1/n)/(
√
2γ) to uSVPγ(1+ε). Let us first
describe the reduction.
Algorithm 1. The BDD(1−1/n)/(
√
2γ) to uSVPγ(1+ε) reduction.
Input: a basis B = {bi}i∈[n] of an n-dimensional lattice L ⊆ Qn, and a target point
t ∈ Qn.
Output: a lattice point c such that ‖c− t‖ = dist(t,L).
0. Guess d0 ∈ [d, d/(1−1/n)) and `0 ∈ [`, `/(1−1/n)), where d = dist(t,L) and ` = λ1(L)
(see Section 2.2).
1. Compute p the smallest prime greater than 4γn2.
Sample z,u uniformly and independently in Znp .
Compute w = Bū ∈ L, such that ū = u mod p and ‖t + w‖ ≥ (n+ 1)`0/
√
2.
Use the algorithm of Lemma 9 to compute a basis Bp,z of Lp,z = {b ∈ L : 〈z,B−1b〉 =
0 mod p}.
2. Set k = 1/(n− 1). Define
B′ =
(
Bp,z t + w
0 kd0
)
.
3. Run the uSVPγ(1+ε) solver on input B′. Let s′ = ((s′1)T, s′2)T be its output. Output
s′1 + t.
It may be checked that the above algorithm runs in polynomial time. The rest of the
section is devoted to proving its correctness.
In this reduction, we are given a BDD(1−1/n)/(
√
2γ) instance (B, t). Let c ∈ L be a
closest vector to t. In order to construct a uSVP instance, our strategy is to use lattice
sparsification to keep only one closest vector c + w for some lattice shift w closest to t + w
(the shift vector w comes from Lemma 8). As the sparsification results in a lattice, we
not only keep c + w, but also the m · (c + w)’s for all m ≤ γn. Simultaneously, all other
vectors inside the balls with centers {m · (t + w)}m≤γn and radius λ1(L)/
√
2 are regarded
ICALP 2016
76:8 Improved Reduction from BDD to USVP in Lattices
as superfluous vectors and removed through sparsification. For the first γ balls, we have
m · (c + w) ∈ B(m · (t + w), λ1(L)/
√
2). We can keep exactly one vector inside every ball
with sparsification over these balls. However, for m > γ, all closest points to t may fall out
of the corresponding ball, but may end up in another relevant ball: vector i · (c + w) may
belong to B(j · (t + w), λ1(L)/
√
2) for some j 6= i. As a consequence, there can be more than
one lattice vector inside a ball, which may result in no gap between first two minima of the
uSVP oracle input lattice. In order to avoid this, we make every two balls far away from
each other by choosing w such that t + w is long.
I Lemma 13. Consider a basis B of an n-dimensional lattice L, a vector c ∈ L and a vector
t ∈ Rn such that ‖c− t‖ ≤ r = λ1(L)/(
√
2γ) for some γ > 0. Let p prime with p ≥ n+ 1.
For any z ∈ Znp , We have
Pr
u,z←↩U(Znp )
[
c + w ∈ Lp,z ∩ B(t + w, γ · r)
Z · (c + w) ⊇ Lp,z ∩ ∪
i≤γn
B(i · (t + w), γ · r)
]
≥ 1
p
− N
p2 −
N
pn−1 ,
where w is arbitrary such that B−1w = u mod p and N = #L ∩ ∪i≤γnB(i · (t + w), γ · r).
Further, if w is chosen such that ‖t + w‖ > γ(n+ 1)r, we have, for all i ∈ [γn],
i · (c + w) 6∈
⋃
j 6=i,j∈[γn]
B (j · (t + w), γ · r) .
Proof. For i ∈ [γn], we define Ni = #L ∩ B(i · t, γ · r) \ {i · c} and {vij}j∈[Ni] = (L ∩ B(i ·
t, γ · r)) \ {i · c}. For any v ∈ L, we use ṽ to denote the coordinate of v under the basis
B. We claim that, with probability ≥ 1/p − N/p2 − N/pn−1, the vector z is orthogonal
(modulo p) to c̃, and at the same time not orthogonal to any ṽij for i ∈ [γn] and j ∈ [Ni].
We have, for all i ∈ [γn] and j ∈ [Ni],
‖i · c− vij‖ ≤ i · ‖c− t‖+ ‖i · t− vij‖ ≤ (n+ 1)(γr) = n+ 1√
2
· λ1(L).
By choice of p, this is smaller than p ·λ1(L). Thanks to Lemma 10, we have i · c̃ 6= ṽij mod p.
Moreover, as p is prime and p ≥ γn+ 1 > i, we have c̃ 6= 1
i · ṽij mod p.
Now we apply Lemma 8 with c̃ and { 1
i · ṽij}i∈[γn],j∈[Ni]. We have
Pr
z,u←↩U(Znp )
[
∀i, j : 〈z, 1
i · ṽij + u〉 6= 0 mod p
〈z, c̃ + u〉 = 0 mod p
]
≥ 1
p
− N
p2 −
N
pn−1 .
As p is prime and sufficiently large, the inequality 〈z, 1
i · ṽij + u〉 6= 0 mod p is equivalent
to 〈z, ṽij + i · u〉 6= 0 mod p. Therefore
Pr
z,u←↩U(Znp )
[
∀i, j : 〈z, ṽij + i · u〉 6= 0 mod p
〈z, c̃ + u〉 = 0 mod p
]
≥ 1
p
− N
p2 −
N
pn−1 .
This proves the first claim of the lemma.
Let i 6= j ≤ γn. Then, by the triangle inequality and the assumption on w, we have:
‖i · (c + w)− j · (t + w)‖ ≥ |j − i| · ‖t + w‖ − i‖c− t‖ > γ(n+ 1)r − (γn)r = γr.
This completes the proof of the lemma. J
S. Bai, D. Stehlé, and W. Wen 76:9
t
λ1(L(B))/
√
2
c
w
c+w
t+w
2(c+w)
2(t+w)
t c
λ1(L(B))/
√
2
w
t+w
c+w
2(t+w)
2(c+w)
3(t+w)
3(c+w)
Figure 3 Sparsification for a BDD1/
√
2 instance (left) and for a BDD1/(2
√
2) instance (right).
As we have p > 4γn2 ≥ 2N (thanks to Lemma 11), with non-negligible probability, none
of the vectors of L belonging to the γn balls is in the sparser lattice Lp,z, except possibly
those in {i · (c + w)}i∈[γn]. In the rest of the reduction analysis, we assume that we are in
this situation and do not repeatedly state that this occurs with non-negligible probability.
As an illustration of Lemma 13, we include Figure 3. In the case of γ = 1 (left subfigure),
there are several plain balls with radius λ1(L), centered in t, t + w and 2(t + w). The dashed
balls illustrate the distance between i · (c + w) and i · (t + w) for all i ∈ [γn]. We can see that
c + w (within the dashed ball) is inside the plain ball, and 2 · (c + w) (within the dashed ball)
is outside of its corresponding plain ball. Similarly, in the case of γ = 2 (right subfigure),
vector i · (c + w) is outside of its corresponding plain ball only when i > 2. Note in particular
that in the case of γ = 2, vector 0 is not the closest point to the target vector, but belongs to
the plain ball with center t. Thus vector 0 should be removed via sparsification. As it is kept
in any sparsified lattice, this is impossible to achieve. This illustrates why center t is shifted
to a new point t + w (then the shift w of 0 may be removed via sparsification). In both
figures, the red crosses denote the points that are removed from the lattice via sparsification.
In Step 2 of the reduction, we construct a basis B′ of an (n+ 1)-dimensional lattice L′ by
using Kannan’s embedding technique. In Step 3, we call the uSVPγ(1+ε) oracle with input
basis B′. The correctness of the reduction is provided by Lemmata 14, 15 and 16.
Any vector in lattice L′ can be written as b′ = ((b+m(t+w))T,mkd0)T with b ∈ Lp,z and
m ∈ Z. We claim that the vector s′ = (((c + w)− (t + w))T,−kd0)T = ((c− t)T,−kd0)T is a
shortest non-zero vector in L′ and also that λ2(L′)/λ1(L′) = γ(1 + Ω(1/n)). Thus ±s′ will be
output by the uSVPγ(1+ε) oracle. We can then obtain the vector c = (c+w)−(t+w)+t ∈ L.
In the following, we give lower bounds for the norm of b′ = ((b + m(t + w))T,mkd0)T
not parallel to s′, which depend on the value of m. Without loss of generality, we restrict
ourselves to m ≥ 0.
The following lemma is analogous to the ‘m = 0 case’ of the Lyubashevsky-Micciancio
reduction [15]. Note that the lower bound in the statement is essentially 2γ2.
I Lemma 14. If m = 0 and b′ 6= 0, then ‖b′‖2/‖s′‖2 ≥ 2γ2/(1 + 1/(n− 1)2).
ICALP 2016
76:10 Improved Reduction from BDD to USVP in Lattices
Proof. As m = 0 and b′ 6= 0, we must have b 6= 0. As a result, we have
‖b′‖2 = ‖b‖2 ≥ λ2
1(L) ≥ 2γ2d2
(1− 1
n )2 ≥ 2γ2d2
0.
Thus, in this case, we have the gap
‖b′‖2
‖s′‖2 ≥
2γ2d2
0
d2 + d2
0k
2 ≥
2γ2
1 + k2 = 2γ2
1 + 1
(n−1)2
.
J
The second lemma bounds the gap for small m’s. It is where our improvement over prior
reductions stems from. Note that the lower bound in the statement is essentially γ2.
I Lemma 15. If m ≤ γn and b′ is linearly independent with s′, then ‖b′‖2/‖s′‖2 ≥
(γ2 + 1/n2)/((1− 1/n)2 + 1/(n− 1)2).
Proof. By Lemma 13, we have
c + w ∈ Lp,z
⋂ ⋃
i≤γn
B
(
i · (t + w), λ1(L)√
2
)
⊆ Z · (c + w).
Thus, as b 6∈ Z · (c + w) (by assumption), we have
‖b′‖2 = ‖b−m · (t + w)‖2 +m2d2
0k
2 ≥ λ2
1(L)
2 +m2d2
0k
2 ≥
(
dγ
1− 1
n
)2
+m2d2
0k
2.
Thus, in this case, we have the gap
‖b′‖2
‖s′‖2 ≥
( dγ
1− 1
n
)2 +m2d2
0k
2
d2 + d2
0k
2 ≥
( dγ
1− 1
n
)2 +m2d2k2
d2 + ( d
1− 1
n
)2k2 =
γ2 + m2
n2
(1− 1
n )2 + 1
(n−1)2
.
The gap is an increasing function in m and hence it suffices to consider m = 1. J
The third lemma bounds the gap for larger m’s. This corresponds to the ‘large m case’
of the Lyubashevsky-Micciancio reduction. As in the previous case, the lower bound in the
statement is essentially γ2.
I Lemma 16. If m > γn, then ‖b′‖2/‖s′‖2 ≥ γ2/((1− 1/n)2 + 1/(n− 1)2).
Proof. For any b ∈ L, we have ‖b′‖2 ≥ m2k2d2
0. Thus, in this case, we have the gap
‖b′‖2
‖s′‖2 ≥
m2k2d2
0
d2 + k2d2
0
≥ m2k2d2
d2 + ( d
1− 1
n
)2k2 = m2
(n− 1)2 + n
(n−1)2
.
The gap is an increasing function in m and hence it suffices to consider the m = γn. J
Now, we complete the proof of Theorem 12. According to Lemmata 14, 15 and 16, the
uSVP gap satisfies, for large enough n
λ2
2(L′)
λ2
1(L′) ≥ min
(
2γ2
1 + 1
(n−1)2
,
γ2 + 1
n2
(1− 1
n )2 + 1
(n−1)2
,
γ2
(1− 1
n )2 + 1
(n−1)2
)
≥ γ2
(
1 + Ω
(
1
n
))
.
S. Bai, D. Stehlé, and W. Wen 76:11
t
w
t+w
m = 1
c+w
2(t+w)
2(c+w)
m = 2
|γnkd0|
|γnkd0|
((c− t)T,−kd0)T
(−(c− t)T, kd0)
T
(2(c− t)T,−2kd0)T
(−2(c− t)T, 2kd0)
T
((a1 − (t+w))T,−kd0)T
((a2 − (t+w))T,−2kd0)T
a1
a2
x
y
z
Figure 4 Geometric illustration of the reduction.
We include Figure 4 to geometrically illustrate the overall reduction. For convenience,
we take k = −1/(n − 1) in the figure. We use filled dots to label points of 2-dimensional
lattice L, and hollow dots to label points of 3-dimensional lattice L′ that are not in L (recall
that L ⊆ L′). With Kannan’s embedding technique, the offset between the vectors of L (e.g.,
c + w) and the shifted target t + w are mapped to L′ (e.g., ((c− t)T,−kd0)T). Thanks to
sparsification, all the points of L′ belonging to the drawn cylinder (of height |2γnkd0|) are
multiples of the shortest non-zero vector s′ = ((c − t)T,−kd0)T, e.g., ±((c − t)T,−kd0)T
and ±(2(c− t)T,−2kd0)T. All other points in L′ that are linearly independent from s′ lie
outside of the cylinder, e.g., ((a1 − (t + w))T,−kd0)T and ((a2 − (t + w))T,−2kd0)T. This
cylinder forces the second minimum λ2(L′) to be large, and, more concretely, larger than
γλ1(L′). This corresponds to Lemma 15 (Lemma 14 handles the points of L and Lemma 16
handles the points of L′ whose z-component is large).
Acknowledgments. We thank Steven Galbraith, Daniele Micciancio and Jinming Wen for
helpful discussions.
References
1 M. Ajtai. The shortest vector problem in l2 is NP-hard for randomized reductions (extended
abstract). In Proc. of STOC, pages 284–293. ACM, 1998.
2 L. Babai. On Lovász lattice reduction and the nearest lattice point problem. Combinatorica,
6:1–13, 1986.
3 S. Bai and S. Galbraith. Private communication, 2015.
4 D. Dadush and G. Kun. Lattice sparsification and the approximate closest vector problem.
In Proc. of SODA, pages 1088–1102. SIAM, 2013.
5 D. Dadush, O. Regev, and N. Stephens-Davidowitz. On the closest vector problem with a
distance guarantee. In Proc. of CCC, pages 98–109. IEEE Computer Society Press, 2014.
6 R. de Buda. The upper error bound of a new near-optimal code. IEEE Trans. on Inform-
ation Theory, 21(4):441–445, 1975.
ICALP 2016
76:12 Improved Reduction from BDD to USVP in Lattices
7 U. Fincke and M. Pohst. A procedure for determining algebraic integers of given norm. In
Proc. of EUROCAL, volume 162 of LNCS, pages 194–202, 1983.
8 R. Kannan. Improved algorithms for integer programming and related lattice problems. In
Proc. of STOC, pages 99–108. ACM, 1983.
9 R. Kannan. Minkowski’s convex body theorem and integer programming. Math. Oper.
Res., 12(3):415–440, 1987.
10 S. Khot. Hardness of approximating the shortest vector problem in high Lp norms. In Proc.
of FOCS, pages 290–297. IEEE Computer Society Press, 2003.
11 S. Khot. Hardness of approximating the shortest vector problem in lattices. J. ACM,
52(5):789–808, 2005.
12 A. K. Lenstra, H. W. Lenstra, Jr., and L. Lovász. Factoring polynomials with rational
coefficients. Math. Ann, 261:515–534, 1982.
13 M. Liu, X. Wang, G. Xu, and X. Zheng. A note on BDD problems with λ2-gap. Inf.
Process. Lett., 114(1-2):9–12, January 2014.
14 Y. K. Liu, V. Lyubashevsky, and D. Micciancio. On bounded distance decoding for general
lattices. In Proc. of RANDOM, volume 4110 of LNCS, pages 450–461. Springer, 2006.
15 V. Lyubashevsky and D. Micciancio. On bounded distance decoding, unique shortest vec-
tors, and the minimum distance problem. In Proc. of CRYPTO, pages 577–594, 2009.
16 D. Micciancio. The shortest vector problem is NP-hard to approximate to within some
constant. SIAM J. Comput, 30(6):2008–2035, 2001.
17 D. Micciancio. Private communication, 2015.
18 D. Micciancio and S. Goldwasser. Complexity of Lattice problem: A Cryptography Perspect-
ive. Kluwer, 2009.
19 O. Regev. On lattices, learning with errors, random linear codes, and cryptography. J.
ACM, 56(6), 2009.
20 C. P. Schnorr. A hierarchy of polynomial lattice basis reduction algorithms. Theor. Comput.
Science, 53:201–224, 1987.
21 N. Stephens-Davidowitz. Discrete Gaussian sampling reduces to CVP and SVP. In Proc.
of SODA, pages 1748–1764. SIAM, 2016.
22 A. Vardy. Algorithmic complexity in coding theory and the minimum distance problem. In
Proc. of STOC, pages 92–109. ACM, 1997.
	Introduction
	Reminders
	Lattice problems
	Approximation results
	Lattice sparsification
	Reducing BDD to uSVP