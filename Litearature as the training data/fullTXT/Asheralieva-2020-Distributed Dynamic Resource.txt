Distributed Dynamic Resource Management and Pricing in the IoT Systems With Blockchain-as-a-Service and UAV-Enabled Mobile Edge Computing
1974 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
Distributed Dynamic Resource Management and
Pricing in the IoT Systems With
Blockchain-as-a-Service and UAV-Enabled
Mobile Edge Computing
Alia Asheralieva and Dusit Niyato , Fellow, IEEE
Abstract—In this article, we study the pricing and resource
management in the Internet of Things (IoT) system with
blockchain-as-a-service (BaaS) and mobile-edge computing
(MEC). The BaaS model includes the cloud-based server to per-
form blockchain tasks and the set of peers to collect data from
local IoT devices. The MEC model consists of the set of terres-
trial and aerial base stations (BSs), i.e., unmanned aerial vehicles
(UAVs), to forward the tasks of peers to the BaaS server. Each
BS is also equipped with an MEC server to run some blockchain
tasks. As the BSs can be privately owned or controlled by dif-
ferent operators, there is no information exchange among them.
We show that the resource management and pricing in the BaaS-
MEC system are modeled as a stochastic Stackelberg game with
multiple leaders and incomplete information about actions of
leaders/BSs and followers/peers. We formulate a novel hierar-
chical reinforcement learning (RL) algorithm for the decision
makings of BSs and peers. We also develop an unsupervised
hierarchical deep learning (HDL) algorithm that combines deep
Q-learning (DQL) for BSs with the Bayesian deep learning (BDL)
for peers. We prove that the proposed algorithms converge to sta-
ble states in which the peers’ actions are the best responses to
optimal actions of BSs.
Index Terms—Bayesian methods, blockchain, deep learning,
game theory, incomplete information, Internet of Things (IoT),
Markov processes, mobile-edge computing (MEC), reinforcement
learning (RL), resource allocation, unmanned aerial vehicles
(UAVs).
Manuscript received August 26, 2019; revised November 6, 2019 and
November 29, 2019; accepted December 19, 2019. Date of publication
December 24, 2019; date of current version March 12, 2020. This work was
supported in part by the National Natural Science Foundation of China under
Project 61950410603; in part by the Singapore NRF National Satellite of
Excellence, Design Science and Technology for Secure Critical Infra-Structure
NSoE under Grant DeST-SCI2019-0007; in part by the A*STAR-NTU-
SUTD Joint Research Grant Call on Artificial Intelligence for the Future
of Manufacturing RGANS 1906, WASP/NTU M4082187 (4080); in part by
the Singapore MOE Tier 1 under Grant 2017-T1-002-007 RG122/17 and
MOE Tier 2 under Grant MOE2014-T2-2-015 ARC4/15; in part by the
Singapore NRF under Grant 2015-NRF-ISF001-2277; and in part by the
Singapore EMA Energy Resilience under Grant NRF2017 EWT-EP003-041.
(Corresponding author: Alia Asheralieva.)
Alia Asheralieva is with the Department of Computer Science and
Engineering, Southern University of Science and Technology, Shenzhen
518055, China (e-mail: aasheralieva@gmail.com).
Dusit Niyato is with the School of Computer Science and
Engineering, Nanyang Technological University, Singapore 639798 (e-mail:
dniyato@ntu.edu.sg).
This article has supplementary downloadable material available at
http://ieeexplore.ieee.org, provided by the author.
Digital Object Identifier 10.1109/JIOT.2019.2961958
I. INTRODUCTION
RAPID proliferation of the Internet of Things (IoT)
systems poses new challenges to data management and
analytics, as vast volumes of sensing and control data gener-
ated by the IoT devices must be gathered, sorted, validated,
and stored in a secure and decentralized way [1], [2]. Due
to limited scalability, single points of failure, and absence of
transparency, the centralized ledger approaches cannot meet
these challenges [3]. Instead, a distributed ledger (DL) tech-
nology called blockchain has been recently proposed for scal-
able, secure, and transparent IoT data management [2], [3]. In
blockchains, data are organized in the form of blocks, e.g.,
records of IoT transactions, as a linked list data structure
to preserve logical relations in the appended blocks. Blocks
are verified, copied, and distributed across the blockchain
network—a connected system formed by the geo-scattered
blockchain nodes or peers. This allows to store and process IoT
data with built-in robustness, integrity, and reliability [4], [5].
Although blockchains have already been adopted in sev-
eral distributed scenarios, e.g., connected vehicles and smart
grids [6], [7] and content delivery networks [8], they are still
not widely deployed in the IoT systems. The reason is that
the typical IoT devices, e.g., small sensors, have scarce bat-
tery, computing, and caching resources. Thus, they cannot
independently run many compute-intensive blockchain tasks,
e.g., block processing and confirmation, and store transaction
records [9]. To address this problem, along with leading tech
giants, such as Microsoft, IBM, and Amazon, many startups
are now offering a viable solution through the blockchain-
as-a-service (BaaS) model [10], [11]. BaaS is a fusion of
blockchain and cloud computing which allows its customers to
leverage cloud-based solutions for blockchain applications. As
such, a BaaS provider maintains the required infrastructure and
resource/security management for a fee paid by its customers.
Unfortunately, despite many benefits, there are several issues
that restrict the adoption of BaaS in the IoT systems [10], [11].
First, in the current service model, the BaaS customer must be
connected to a cloud where its data are managed by the core
servers. This yields high propagation delay, backhaul load,
and power consumption for the IoT devices which are typi-
cally located at the network “edges,” i.e., far from the cloud. In
addition, it may be unprofitable to perform spectrum/resource
allocation for the large number of spatially distributed IoT
2327-4662 c© 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
https://orcid.org/0000-0002-4430-5928
https://orcid.org/0000-0002-7442-7416
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1975
devices, as the capital and operating expenditure (CAPEX and
OPEX) on the necessary infrastructure can be overwhelming.
To tackle the above issues, the BaaS model can be inte-
grated with mobile-edge computing (MEC) [12]–[15]. In this
case, the spectrum/resource allocation can be implemented
at the base stations (BSs) operated by some external, i.e.,
different from the BaaS provider, MEC operator(s). This
will significantly reduce the expenses of the BaaS provider.
Next, note that due to their mobility and ease of deploy-
ment, current MEC operators increasingly utilize unmanned
aerial vehicles (UAVs) as aerial BSs [15]–[17]. Hence, it is
reasonable to assume that some BSs can be represented by
UAVs to extend connectivity and facilitate cost-effective solu-
tions. Furthermore, to minimize backhaul load and propagation
delays, the BaaS provider can deploy the set of blockchain
peers represented, e.g., by mobile terminals and desktop com-
puters, in proximity to the IoT devices. The peers can manage
the IoT-blockchain data, e.g., gather, process, verify, and
store, confirmed the IoT transaction records. Some compute-
intensive tasks can also be offloaded to the core servers or to
MEC servers installed at BSs.
In this article, we study the integration of BaaS with MEC,
where MEC services are provided through terrestrial BSs and
UAVs acting as aerial BSs. The main challenge here is how
to provide the efficient BaaS support for IoT applications and,
at the same time, maintain the profitability of the BaaS-MEC
model. More specifically, each BS charges a certain fee for
offloading/communication services from blockchain peers. To
maximize its payoff, the BS must determine its optimal service
fee and, in the case of aerial BS, its optimal flight parame-
ters. On the other hand, the objective of a blockchain peer is
to finish its block task before a given deadline at a smallest
possible fee. In particular, the peer has the following options:
1) perform the task locally; 2) offload the task to the MEC
server of some BS, in which case it must pay the processing
fee to the respective BS; and 3) offload the task to the core
server, in which case it must pay the forwarding fee to the BS
that forwards the task. As such, we must analyze the interac-
tions between the BSs and peers while also considering the
following critical issues.
1) In mobile blockchains, communication among peers
results in additional delay and energy overheads that
yield growing latencies and power costs. To minimize
data exchange, the peers should select their offload-
ing options simultaneously and independently, without
informing each other about their decisions. As such,
peers have incomplete information about offloading
decisions of other peers.
2) Due to the absence of communications among BSs,
each BS selects its optimal service cost and (in the
case of aerial BSs) its flight parameters simultaneously
and independently. As such, similar to the peers, BSs
have incomplete information about cost assignments and
flight parameters of other BSs.
3) The state of a wireless environment (e.g., locations of
peers and aerial BS, and channel quality) is highly
stochastic. Thus, to maximize their payoffs in the BaaS-
MEC model, the peers and BSs should be able to
dynamically adjust their decisions in response to the
random environmental changes.
Consequently, in this article, we introduce a novel game-
theoretic, reinforcement learning (RL) and deep learning
framework aimed to model the interactions between the BSs
and peers, and to address the aforementioned issues. The main
contributions of this article are listed as follows.
1) We formulate a detailed analytical model of the IoT
system with BaaS-MEC support that considers dynamic
parameters of the blockchain tasks and wireless chan-
nels. Based on this model, we derive exact expressions
of the delay and energy for completing each task and the
corresponding costs and payoffs of the BSs and peers.
2) We represent the interactions between BSs and peers
as a stochastic Stackelberg game with multiple lead-
ers, where both the leaders/BSs and followers/peers
have incomplete information about the actions of other
leaders and followers, respectively. The game is stochas-
tic because the payoffs of leaders and followers are
affected by random environmental states with unknown
transitions.
3) As no follower observes the actions of other followers,
we model its decision making by a partially observ-
able Markov decision process (POMDP). We prove
that a solution of the POMDP determines the best
response strategies of followers which form a per-
fect Bayesian equilibrium (PBE). We also develop
a Bayesian RL (BRL) algorithm that allows each fol-
lower to update its belief about unobservable states of
the POMDP and predict its strategy through the interac-
tions with the other followers and with the environment.
4) To reduce the complexity of the POMDP, we devise
a novel polynomial-time Bayesian deep learning (BDL)
algorithm in which uncertainties about unobservable
states are modeled by a Bayesian neural network
(BNN)—neural network (NN) with random weights dis-
tributed according to a predefined prior. The algorithm
is unsupervised, as it enables a self-organized “online”
learning that does not need a prior “offline” training
and/or pre-existing training data sets. We prove that the
BDL algorithm converges to a stable state in which
the followers’ strategies form a myopic PBE (MPBE)—
the PBE with short-sighted players that aim to maximize
their one-stage (rather than long-term) payoffs.
5) We derive a stochastic optimization problem for each
leader of a Stackelberg game. The problem incorporates
a random state defined by the followers’ best responses.
To solve this problem, we model it as a Markov decision
process (MDP), the solution of which is equivalent to
the solution of the leader’s problem. Based on the MDP,
we develop the RL algorithm that allows the leader to
maximize its individual expected long-term payoff by
dynamically adjusting its MEC service cost and flight
parameters (in the case of aerial BSs).
6) We formulate a polynomial-time deep Q-learning (DQL)
algorithm in which the complexity of the MDP is
reduced by approximating the value function with the
output of the NN in an unsupervised manner, i.e.,
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
1976 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
TABLE I
LIST OF MAIN NOTATIONS USED IN THIS ARTICLE
without any prior training. We prove that the DQL
algorithm converges to a stable state that maximizes the
leader’s long-term payoff.
The remainder of this article is organized as follows. In
Section II, we review the related works on resource man-
agement and pricing for the IoT/mobile blockchains. In
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1977
Section III, we present a general architecture of the IoT system
with BaaS and MEC. In Section IV, we derive the analytical
model and delay/energy expressions for IoT-blockchain tasks.
In Section V, we define a Stackelberg model of the BaaS-
MEC support. In Section VI, we formulate a stochastic game,
devise the BRL algorithm for the followers, and propose the
BDL algorithm in Section VII. In Section VIII, we develop the
game and present the RL and DQL algorithms for the leaders.
In Section IX, we conduct a numerical performance evaluation
of the framework.
II. RELATED WORK
Most works on blockchain applications in the mobile and
IoT systems (e.g., [5], [7], [8] and [18]–[21]) study the imple-
mentation and design of the blockchain-based protocols for
decentralized and secure data communication, unrelated to
the framework of this article. The only works which consider
resource management and pricing for mobile/IoT blockchains
have been conducted in [22]–[27] where the blockchain peer
can offload its blockchain tasks to one of the BSs in the MEC
network. The focus here is on the mining under the proof-of-
work (PoW) consensus [4], [5] which results in the competi-
tion among peers to win a mining reward. In [22] and [23],
the interplay between the peers and the MEC service provider
(SP) has been represented as a two-stage Stackelberg game. At
stage one, a leader/SP assigns the price per unit hash rate with
the objective to maximize its one-stage profit. At stage two,
given the unit price, the followers/peers select hash rates aim-
ing to maximize their one-stage payoffs. The auction-theoretic
resource allocation approaches for blockchains with MEC have
been analyzed in [24] and [25]. The objective of the auction
in [24] is to maximize a social welfare while ensuring its
incentive compatibility, i.e., trustfulness for peers. In [25], the
performance of auction designed to maximize the revenues
of MEC SPs has been improved by adopting deep learning.
In [26] and [27], resource allocation, offloading, and block
size adaptation for blockchains in the MEC have been repre-
sented as a binary programming problem with the objective to
maximize the average reward of peers. To reduce the problem
size, we adopt the alternating direction method of multipliers
(ADMMs) that yields a distributed algorithm.
To summarize, the works in [22]–[27] study the possibil-
ity of enhancing the computing abilities of mobile devices
with MEC services. Yet, they still have several limitations.
First, the main assumption of these studies is that information
about peers’ actions, e.g., offloading or hash rate deci-
sions, is public, i.e., known to other peers. However, in the
blockchain networks, the peers take actions simultaneously
and independently [28]. In addition, in mobile settings, com-
munications among peers lead to additional delay and energy
overheads. Second, the game and auction models presented in
these studies ignore dynamics and randomness of the wireless
environment, e.g., locations of peers and channel quality. As
such, the games/auctions are played in a static and determinis-
tic environment and result in short-term (one-stage) solutions.
Third, the works in [22]–[27] are designated specially for
a PoW consensus and cannot be used with other consensus
Fig. 1. General architecture of the IoT system with BaaS-MEC support.
methods. Although PoW is a popular protocol, it is compute-
intensive and has a poor scalability, since each mining task
must be computed by all blockchain nodes [29]. As a result,
PoW is not suitable for scenarios with large numbers of nodes
such as IoT systems.
Similar to the above works, we analyze the pricing
and resource management in blockchains supported by
MEC. However, our framework is based on more realistic and
applicable to the IoT considerations are as follows.
1) The information about actions of blockchain peers is
private, i.e., unknown to other peers. As a result, the
peers must form their beliefs about unobservable actions
and estimate their best response strategies by adopting
BRL or BDL.
2) The proposed Stackelberg model is stochastic. That is,
it is played in a dynamic and random environment and
produces long-term (multistage) solutions.
3) The framework is not linked to a specific consen-
sus scheme, i.e., it can be applied with any arbitrary
consensus protocol.
III. IOT SYSTEM WITH BAAS-MEC SUPPORT
A. Architecture and System Model
The main notations used in this article are listed in Table I.
First, we introduce a general architecture of the IoT system
with BaaS-MEC support (as shown in Fig. 1) that categorizes
all system participants into four categories as follows.
1) IoT Devices: Fixed and mobile IoT devices, such as
sensors, that are exploited only for basic functions—
sense, collect, and send data to be recorded in the IoT
blockchains to the peers operated by the BaaS provider.
2) Peers: Semistatic blockchain nodes located at the
network edges, each of which can be represented by
the desktop computer or mobile terminal (e.g., Laptop,
Smartphone, or Tablet). The main function of peers is
to manage the IoT data collected from the IoT devices
in their transmission ranges, i.e., record data as mining
tasks, process and verify tasks, and store verified tasks
in the form of transaction records.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
1978 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
3) MEC Network: It can be formed by terrestrial BSs with
links to the blockchain servers and aerial BSs, i.e.,
UAVs. The function of the MEC network is to connect
peers at the network edges with blockchain servers at
the network core. In addition, the MEC servers installed
at BSs can be used to process some mining tasks.
4) Blockchain Servers: Resource-rich core servers managed
by the BaaS provider. Unlike peers, servers are only
accessed through the terrestrial BSs.
We now present a general model of the IoT system with
BaaS-MEC support where IoT devices are served by the
set N = {1, . . . , N} of peers labeled as P1, . . . , PN and
a blockchain server. The main function of peers is to man-
age the IoT data, e.g., sensing records, collected from the IoT
devices in their transmission ranges. In addition, each peer
Pn, n ∈ N, has a local processor of the computing power
ρn in CPU cycles per second to process mining tasks. We
assume that the peers do not communicate with each other,
because such information exchange introduces heavy signaling
over a wireless medium that increases delays and transmis-
sion costs for the peers. The MEC network is formed by
M = MT +MA BSs, including MT terrestrial BSs connected to
the blockchain server via fiber backhaul links, and MA aerial
BSs, i.e., the UAVs. For notation consistency, terrestrial BSs
are labeled as BSN+1, . . . , BSN+MT ; and aerial BSs are labeled
as BSN+MT+1, . . . , BSN+MT+MA. Thus, the set of terrestrial
BSs is defined by MT = {N + 1, . . . , N + MT}, the set of
aerial BSs is MA = {N +MT +1, . . . , N +MT +MA}, the total
set of BSs is M = MT ∪ MA = {N + 1, . . . , N + M}.
Since BSs can be privately owned or controlled by differ-
ent operators, in general, there is no communication among
the BSs. Each BSm, m ∈ M, operates on an orthogonal full-
duplex (FD) band denoted by Bm which may overlap with the
spectra of other BSs. The MEC server of BSm represents a pro-
cessor of a computing power ρm in CPU cycles per second to
perform some mining tasks. On the contrary, a resource-rich
blockchain server includes several processors to run multiple
blockchain operations in parallel and confirm IoT-blockchain
transactions. We consider that the blockchain server has MT
processors, each of which has the computing power of ρ0 in
CPU cycles per second and is connected to a terrestrial BS to
support parallel processing of the tasks forwarded by BSs.
B. Mining Process and System Parameters
The process of blockchain mining consists of the number of
stages, denoted as t = 0, . . . , T . The peers and BSs remain in
the system for indefinitely long time, i.e., T →∝. During each
stage, the network parameters (e.g., channel quality, locations
of peers, IoT devices, and UAVs) remain fixed, although the
parameters can change, as we move to the next stage. At the
beginning of stage t, each BSm, m ∈ M, assigns the stage cost
cm
t = (cm(I)
t , cm(P)
t ) ∈ Cm for its MEC services per mining task
and announces the cost to peers in its service range. The cost
cm
t comprises: 1) forwarding cost cm(I)
t , i.e., cost of forwarding
the task to the blockchain server via BSm and 2) processing
cost cm(P)
t , i.e., cost of processing the task at the MEC server
of BSm. Forwarding services are provided only by the ter-
restrial BSs that have links to the blockchain server. Hence,
cm(I)
t = 0, for an aerial BSm, m ∈ MA. All costs are counted in
terms of units of a certain currency (e.g., bitcoin). Hence, the
set of possible costs, Cm = {(0, 0), (0, 1), . . . , (Cm(I)
max , Cm(P)
max )},
where Cm(I)
max and Cm(P)
max are the maximal processing and for-
warding costs, is discrete and finite. Meanwhile, each peer
Pn, n ∈ N, collects IoT data and records these data as a min-
ing task. In order to be recorded in the IoT blockchain, after
the task is processed, its output must be confirmed by other
peers. Once all the task’s outputs are received and confirmed,
the current stage t of the mining process concludes and the
next stage t + 1 begins.
Let lnt ∈ Ln ⊂ R, n ∈ N, denote the 2-D location of peer
Pn at stage t, where Ln is the set of possible 2-D locations
of peer Pn; and R is the service area of the network. Let
lmt = (hm
t , �m
t ) ∈ Lm, m ∈ M, be the 3-D location of BSm at
stage t, i.e., the altitude hm
t and the projected (to the ground)
2-D location �m
t of BSm, where Lm is the set of possible 3-D
locations of BSm. Note that hm
t = 0 and lmt = lm0 , for the
terrestrial BSm, m ∈ MT . To be compatible with the framework
in this article, all distances in the network are discretized, i.e.,
the distance axis is partitioned into equal intervals of a small
length �l (in meters). Hence, the sets Ln, Lm, and R are finite.
Given that the service range Rm(lmt ) ⊂ R of BSm is bounded
by a circle of a radius Rm ∈ R, we obtain the following logical
relation:
lnt ∈ Rm
(
lmt
) ⇐⇒
√
∣∣hm
t
∣∣2 + ∥∥lnt − �m
t
∥∥2 ≤ Rm ∀n ∈ N (1a)
for all m ∈ M, at any stage t, where the 3-D location lmt of
BSm is such that
√∣∣hm
t − hm
t−1
∣∣2 + ∥∥�m
t − �m
t−1
∥∥2 ≤ vm
max�t
where vm
max is a maximal speed of BSm in meters per second
(vm
max = 0, for a terrestrial BSm, m ∈ MT ), �t is the stage
duration (in seconds). Then, at any stage t, given the past 3-D
location lmt−1 = (hm
t−1, �
m
t−1) of BSm, the set Lm is defined by
Lm =
{
lmt = (
hm
t , �m
t
)
∣∣∣∣
∣
∣∣hm
t − hm
t−1
∣∣2 + ∥∥�m
t − �m
t−1
∥∥2
≤ (
vm
max�t
)2
}
(1b)
for all m ∈ MA, and Lm = {lmt = (hm
t , �m
t ) = (0, lm0 )}, for all
m ∈ MT .
Unlike terrestrial BSs, aerial BSs or UAVs have limited
fly duration and endurance, e.g., the average endurance of
rotary-wind UAVs is 30 min. Hence, no UAV can be con-
stantly active and must be recalled for battery recharging or
swapping [15]–[17]. Let zt = {zm
t ∈ {0, 1}, zi
t = 1|m ∈
MA, i ∈ MT} be the BSs’ activity vector, such that zm
t
= 1, if BSm is active at stage t. The activity of aerial BSs
can be controlled to achieve various objectives. For example,
to prolong the endurance and reduce the energy consump-
tion, the UAV can be deactivated when there are no users
in its service ranges. As such, at any stage t, we have
�t
∑t
τ=t−Tm
max/�t zm
τ ≤ Tm
max, for m ∈ MA, where Tm
max is
the maximal endurance of aerial BSm (in seconds). In words,
BSm can remain active in the air as long as its “ON period”
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1979
�t
∑t
τ=t−Tm
max/�t zm
τ does not exceed Tm
max. Thus, at any stage
t, given the activity history zm
t−1, . . . , zm
t−Tm
max/�t, the set Zm of
possible values of zm
t is defined by
Zm =
⎧
⎨
⎩
zm
t ∈ {0, 1}
∣∣∣∣z
m
t ≤ Tm
max
�t
−
t−1∑
τ=t−Tm
max/�t
zm
τ
⎫
⎬
⎭
∀m ∈ MA
(1c)
and Zm = {zm
t = 1}, for m ∈ MT .
IV. ANALYTICAL MODEL OF THE SYSTEM
A. Blockchain Mining Model
To be recorded as a confirmed transaction in the blockchain,
IoT data must be: 1) collected by the peers from IoT devices
in their transmission ranges and 2) processed and verified.
Note that as the number of IoT devices is commonly very
large, it is rather inefficient (e.g., in terms of spectrum
costs/utilization) to allocate them with the separate cellu-
lar spectrum [1]. Instead, data transmissions between the IoT
devices and the peers can be realized over device-to-device
(D2D) links overlaying the downlink (DL) spectrum of the
MEC network. The DL cellular spectrum is favored because
it eliminates interference between cellular and D2D trans-
missions, since the uplink (UL) channel load in the MEC
network is usually much higher than the DL load [14], [15].
As such, at any stage t, data collected by peer Pn, n ∈ N,
from IoT devices represents a mining task defined by the tuple
θn
t = (θ
n(I)
t , θ
n(P)
t , θ
n(D)
t , θ
n(O)
t ) ∈ �n, where θ
n(I)
t is the input
size in bits; θ
n(P)
t is the processing size, i.e., the number of
CPU cycles required to finish the task; θ
n(D)
t is the completion
deadline in seconds; and θ
n(O)
t is the output size in bits. The set
�n = {(0, 0, 0, 0), (0, 0, 0, 1), . . . , (θ
n(I)
max, θ
n(P)
max , θ
n(D)
max , θ
n(O)
max )}
contains all possible parameters of the task θn
t , where θn
t =
(0, 0, 0, 0), if no task is recorded at stage t; θ
n(I)
max, θ
n(P)
max , θ
n(D)
max ,
and θ
n(O)
max are the maximal input size, processing size, com-
pletion deadline, and output size, respectively. To estimate
the parameters θ
n(I)
t , θ
n(P)
t , θ
n(D)
t , θ
n(O)
t , one of the analysis
methods, e.g., call graph analysis [30], can be applied.
Upon recording its task θn
t , peer Pn has the following
options.
1) Option 1: Offload the task processing to one of the active
BSs in the transmission range of the peer.
2) Option 2: Offload the task processing to a blockchain
server via a terrestrial BS in the transmission range of
the peer.
3) Option 3: Process the task locally.
Once the task θn
t is completed, its output θ
n(O)
t that repre-
sents the unconfirmed transaction must be verified by other
peers, i.e., the peers different from peer Pn. The transaction is
verified only if the total transaction delay, i.e., the sum of pro-
cessing and transmission delay, for the task θn
t does not exceed
its deadline θ
n(D)
t . If confirmed, the output θ
n(O)
t is appended
to the blockchain and stored by the peer for future references,
e.g., data analytics or system control. Otherwise, the output
θ
n(O)
t is discarded, which is called orphaning [23]–[25].
Now, consider peer Pn that has recorded the mining task θn
t
at stage t. Let Mn = {m ∈ M|zm
t = 1, lnt ∈ Rm(lmt )} be the
set of active BSs within the transmission range of peer Pn.
The offloading decision an
t = (yn
t , xn
t ) ∈ An for the task θn
t
of peer Pn at stage t consists of the processing decision yn
t ∈
{0, 1} and forwarding decision xn
t ∈ {0} ∪ Mn. The decisions
yn
t and xn
t are such that: 1) yn
t = 0, if the task is processed
locally (at the MEC server of a respective BS or at the local
processor of peer Pn); 2) ym
t = 1, if the task is computed at the
blockchain server; 3) xn
t = 0, if the task θn
t is not forwarded;
and 4) xn
t = m ∈ Mn, if the task θn
t is forwarded to BSm. Note
that the task can be offloaded to the server only if there is at
least one terrestrial BS in the set Mn, i.e., Mn∩MT = ∅. Thus,
a set An of possible offloading decisions (yn
t , xn
t ) of peer Pn
is given by
An =
⎧
⎨
⎩
an
t = (
yn
t , xn
t
)
∣∣
∣∣∣∣
yn
t ∈ {0, 1}, xn
t ∈ {0} ∪ Mn
yn
t = 0 =⇒ xn
t ∈ {0} ∪ Mn
yn
t = 1 =⇒ xn
t ∈ Mn ∩ MT
⎫
⎬
⎭
∀n ∈ N.
(2)
Then, in Option 1, yn
t = 0 and xn
t = m ∈ Mn. That is, peer
Pn offloads the task input θ
n(I)
t to BSm through a UL cellular
channel of some data rate Rn,m
U (xt) that can be estimated, as in
Rn,m
U (xt) = β
n,m
U (xt)log2
(
1 + pnGn,m
∑
i∈M\{m} bm,i
U p̃iG̃i,m+σ 2
)
(3a)
where β
n,m
U (xt) is the UL bandwidth of a cellular link between
peer Pn and BSm assumed to be distributed equally among all
peers transmitting to BSm (for the fairness purpose), i.e.,
β
n,m
U (xt) = Bm∑
j∈N 1
xj
t=m
(3b)
pn is a transmit power of peer Pn; Gn,m is the gain of a cellular
link between peer Pn and BSm; bm,i
U ∈ {0, 1} is a UL spectrum
overlap indicator, such that bm,i
U = 1, if a UL spectrum of BSm
overlaps with a UL spectrum of BSi; p̃i is an average transmit
power of peers transmitting to BSi; G̃i,m is an average channel
gain of cellular links between BSi and the peers transmitting to
BSi; σ 2 is a variance of a zero-mean additive white Gaussian
noise (AWGN) power. In Option 2, yn
t = 1 and xn
t = m ∈ Mn
∩MT . In this case, a terrestrial BSm receives the task θn
t from
peer Pn over the UL cellular channel with data rate Rn,m
U (xt)
and forwards it to the blockchain server via a fiber backhaul
link of a capacity Rm,0
f . In Option 3, yn
t = 0 and xn
t = 0, i.e.,
the peer processes the task itself.
B. Transaction Delay and Energy Consumption
Since each peer Pn and each BSm are equipped with the
local processors, the processing of tasks by the peer and the
BS can be represented by the virtual processor queues denoted
by Qn and Qm, respectively. On the contrary, the blockchain
server has MT parallel processors, each of which is connected
to the terrestrial BS. Therefore, the processing of tasks at
the server can be defined by MT parallel queues {Qm,0}m∈MT
,
where Qm,0 is the queue connected to a terrestrial BSm. Let Qn
t ,
Qm
t , and Qm,0
t be the backlogs of queues Qn, Qm, and Qm,0 at
stage t. Propositions 1 and 2 below establish the relationships
between the offloading decisions at = {an
t }n∈N = {(yn
t , xn
t )}n∈N
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
1980 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
made by all the peers and the transaction delay Dn and
energy En spent on transacting, i.e., completing, the task θn
t
of any peer Pn given the parameters (θn
t , lnt , lt, zt, Qn
t ), for
lt = {lmt }m∈M and Qn
t = {Qn
t } ∪ {Qm
t }m∈M ∪ {Qm,0
t }m∈MT
.
Proposition 1: The transaction delay Dn for the mining task
θn
t recorded by peer Pn at stage t is the sum of transmission
delay Dn(I) and processing delay Dn(P). That is
Dn
(
at
∣
∣∣θn
t , lnt , lt, zt, Qn
t
)
= Dn(I)(at|θn
t , lnt , lt, zt
)
+ Dn(P)
(
at|θn
t , lnt , lt, zt, Qn
t
)
(4a)
where
Dn(I)(at|θn
t , lnt , lt, zt
)
=
∑
m∈M
zm
t 1lnt ∈Rm(lmt )
⎛
⎝1xn
t =m
θ
n(I)
t
Rn,m
U (xt)
+1m∈MT
θ̃ (I)
Rm,0
f
∑
j∈N
yj
t1xj
t=m
⎞
⎠
(4b)
Dn(P)
(
at|θn
t , lnt , lt, zt, Qn
t
)
= 1
�t
⎛
⎝
(
1−yn
t
)(
1−xn
t
)
ρn
(
Qn
t +θ
n(P)
t
)
+
∑
m∈M
zm
t 1lnt ∈Rm(lmt )1
xn
t =m
×
⎛
⎝ yn
t
ρ0
⎛
⎝Qm,0
t +θ
n(P)
t + θ̃ (P)
2
∑
i∈N\{n}
1xi
t=myi
t
⎞
⎠+1−yn
t
ρm
×
⎛
⎝Qm
t +θ
n(P)
t + θ̃ (P)
2
∑
i∈N\{n}
1xi
t=m
(
1−yi
t
)
⎞
⎠
⎞
⎠
⎞
⎠
(4c)
where θ̃ (I) and θ̃ (P) are the average input and processing sizes,
respectively, of the peers’ mining tasks.
Proposition 2: The energy En on transacting the mining
task θn
t recorded by peer Pn at stage t is the sum of energy
En(I) on transmitting and energy En(P) on processing the task.
That is
En(at|θn
t , lnt , lt, zt
) = En(I)(at|θn
t , lnt , lt, zt
)
+ En(P)
(
an
t |θn
t , lnt , lt, zt
)
(5a)
where
En(I)(at|θn
t , lnt , lt, zt
) = En,0(I)(an
t |θn
t , lnt , lt, zt
)
+
∑
m∈MT
En,m(I)(at|lnt , lmt
)
(5b)
and
En(P)
(
an
t |θn
t , lnt , lt, zt
) = En,0(P)
(
an
t |θn
t , lnt , lt
)
+
∑
m∈M
En,m(P)
(
an
t |θn
t , lnt , lmt , zm
t
)
. (5c)
In (5b), En,0(I) is the energy spent by peer Pn on transmitting
the task, given by
En,0(I)(an
t |θn
t , lnt , lt, zt
) = pnθ
n(I)
t
∑
m∈M
zm
t 1lnt ∈Rm(lmt )1xn
t =m
Rn,m
U (xt)
(6a)
and En,m(I) is the energy consumed by the terrestrial BSm on
forwarding the task to the blockchain server, given by
En,m(I)(at|lnt , lmt
) = 1lnt ∈Rm(lmt )
θ̃ (I)
Rm,0
f
∑
j∈N
yj
tp
m,0
f 1
xj
t=m
(6b)
where pm,0
f is the transmit power of a signal sent by BSm to the
blockchain server. In (5c), En,0(P) is the energy spent by peer
Pn or the blockchain server on processing the task, given by
En,0(P)
(
an
t |θn
t , lnt , lt
) = (
1 − yn
t
)(
1 − xn
t
)
ϕnθ
n(P)
t
+ yn
t
∑
m∈MT
1lnt ∈Rm(lmt )1xn
t =mϕ0θ
n(I)
t (7a)
where ϕn and ϕ0 are the energies spent per CPU cycle by peer
Pn and a blockchain server, respectively; En,m(P) is the energy
spent by BSm on processing the task, given by
En,m(P)
(
an
t |θn
t , lnt , lmt , zm
t
)
= zm
t 1lnt ∈Rm(lmt )1
xn
t =m
(
1 − yn
t
)
ϕ
m
θ
n(P)
t (7b)
where ϕm is the energy spent per CPU cycle by BSm.
The proofs of Propositions 1 and 2 are given in
Appendixes A and B, in the supplementary material,
respectively.
V. STACKELBERG MODEL OF THE BAAS-MEC SUPPORT
A. Payoffs of the Peers from the BaaS Support
For any verified task output θ
n(O)
t , peer Pn receives a certain
transaction fee ξn > 0. The output θ
n(O)
t is confirmed only if
the transaction delay for the task θn
t in (4a) does not exceed
its completion deadline, i.e., Dn(at|θn
t , lnt , lt, zt, Qn
t ) ≤ θ
n(D)
t .
Else, if output θ
n(O)
t is discarded due to orphaning, the trans-
action fee for the task is nullified. If Option 1 is selected for
the task θn
t , i.e., peer Pn offloads the task processing to some
BSm in its transmission range, the peer must pay a processing
cost cm(P)
t to BSm. If Option 2 is selected, i.e., peer Pn offloads
the task processing to the blockchain server via a terrestrial
BSm in its transmission range, the peer must pay a forwarding
cost cm(P)
t to BSm. As such, the instantaneous payoff that peer
Pn receives for the task θn
t recorded at stage t is given by
un(at|θn
t , lnt , ct, lt, zt, Qn
t
) = 1
Dn(at|θn
t ,lnt ,lt,zt,Qn
t )≤θ
n(D)
t
ξn
− Cn(an
t |θn
t , lnt , ct, lt, zt, Qn
t
)
(8a)
where ct = {cm
t }m∈M ∈ C = ×m∈MCm are the stage costs for
MEC services of BSs
Cn(an
t |θn
t , lnt , ct, lt, zt, Qn
t
)
=
∑
m∈M
zm
t 1lnt ∈Rm(lmt )1
xn
t =m
((
1 − yn
t
)
cm(P)
t + yn
t cm(I)
t
)
− ζ nEn,0(an
t |θn
t , lnt , lt, zt
)
(8b)
are the total stage costs paid by peer Pn at stage t; ζ n is the
cost per energy unit paid by peer Pn
En,0(an
t |θn
t , lnt , lt, zt
) = En,0(I)(an
t |θn
t , lnt , lt, zt
)
+ En,0(P)
(
an
t |θn
t , lnt , lt
)
(8c)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1981
is the total energy spent by peer Pn or the blockchain server
on transacting the task, i.e., the sum of energy consumed by
peer Pn on transmitting the task and energy spent by peer Pn
or the blockchain server on processing the task.
The payoff un in (8a) is a function of: 1) offloading option
an
t ∈ An which is selected by peer Pn at the beginning of stage
t; 2) task parameters and location (θn
t , lnt ) ∈ �n × Ln of peer
Pn at stage t; 3) BSs’ parameters (ct, lt, zt, Qn
t ) observable by
peer Pn at the beginning of stage t; and 4) offloading decisions
a−n
t = {ai
t}i∈N\{n} = A−n = ×i∈N\{n}Ai of other peers that are
unobservable by peer Pn. Since peer Pn is uncertain about
the values of a−n
t , it cannot calculate its payoff in (8a) when
making its decision an
t . However, the peer can construct its
own estimate of a−n
t . This estimate or “belief” represents the
joint probability distribution Bn
t = {Bn
t (a
−n)}a−n∈A−n , where
Bn
t (a
−n) = Prn{a−n
t = a−n} = ∏
i∈N\{n} Bn
t (a
i) is the probabil-
ity that peer Pn assigns to other peers having the offloading
profile a−n
t = a−n ∈ A−n at stage t, and Bn
t (a
i) = Prn{ai
t = ai}
is the marginal probability. Let sn
t = (θn
t , lnt , ct, lt, zt, Qn
t ) ∈ Sn
be the parameters that are observable by peer Pn at the begin-
ning of stage t, where Sn = �n ×Ln×m∈M(Cm, Lm, Zm)×�n
is a finite set of possible values of sn
t ; and �n is a finite set
of possible queue backlogs observable by peer Pn. Then, at
any stage t, upon observing the parameters sn
t , the peer can
compute the expected (with respect to its current belief Bn
t )
stage payoff from its offloading decision an
t ∈ An, as in
Un(Bn
t , an
t |sn
t
) =
∑
a−n∈A−n
Bn
t
(
a−n)un(an
t , a−n|sn
t
)
=
∑
a−n∈A−n
Bn
t
(
a−n)1
Dn(an
t ,a−n|sn
t )≥θ
n(D)
t
ξn
− Cn(an
t |sn
t
)
. (9)
Since peer Pn is selfish, i.e., it aims to maximize the payoffs
from its offloading decisions. If the peer is myopic or short
sighted [31], [32], then at any stage t, it selects a decision
an
t = argmax
an∈An
Un(Bn
t , an|sn
t
) ∀n ∈ N (10a)
which maximizes its expected stage payoff in (9). However, if
the peer is fully rational, i.e., long-visional [31], [32], it selects
a decision
an
t = argmax
an∈An
Vn(Bn
t , an|sn
t
) ∀n ∈ N (10b)
that maximizes its expected long-term payoff Vn
(
Bn
t , an|sn
t
)
.
Since the peer remains in the system for indefinitely long time,
its long-term payoff is defined by an infinite discounted sum
of its stage payoffs in (8a) [32], i.e.,
Vn(Bn
t , an
t |sn
t
) = E
{ ∞∑
τ=t
γ τ−tun(an
τ , a−n
τ |sn
τ
)|Bn
t , sn
t , an
t
}
= E
{ ∞∑
τ=t
γ τ−tUn(Bn
τ , an
τ |sn
τ
)|Bn
t , sn
t , an
t
}
= Un(Bn
t , an
t |sn
t
)+ γ
∑
śn∈Sn
Pr
{
śn|sn
t , an
t , Bn
t
}
× Vn(Bn
t , an
t |śn)
=
∑
a−n∈A−n
Bn
t
(
a−n)Vn(an
t , a−n|sn
t
)
(11a)
where γ ∈ (0, 1] is a given discount rate; Pr{śn|sn
t , an
t , Bn
t }
is the probability of transiting to śn ∈ Sn from (sn
t , an
t , Bn
t );
Vn(an
t , a−n|sn
t ) is the expected long-term payoff of peer Pn
from the decision profile (an
t , a−n) given sn
t , defined as
Vn(an
t , a−n|sn
t
) = E
{ ∞∑
τ=t
γ τ−tun(an
τ , a−n
τ |sn
τ
)|sn
t , an
t , a−n
}
= un(an
t , a−n|sn
t
)+ γ
∑
śn∈Sn
Pr
{
śn|sn
t , an
t , a−n}
× Vn(an
t , a−n|śn) (11b)
where Pr{śn|sn
t , an
t , a−n} is the probability of transiting to śn
∈ Sn from (sn
t , an
t , a−n).
B. Payoffs of the Base Stations from MEC Services
Since each BSm is selfish and fully rational, at any stage t, it
aims at maximizing its long-term payoff Vm
t = ∑∞
τ=t γ
τ−tum
τ
defined as an infinite discounted sum of stage payoffs (as BSm
stays in the system indefinitely long). Let am
t = (cm
t , lmt , zm
t )
∈ Am = Cm × Lm × Zm denote the stage parameters of BSm,
i.e., cost cm
t , 3-D location lmt , and activity zm
t at stage t. Then,
the instantaneous payoff um
t that BSm receives from its MEC
services with parameters am
t ∈ Am is given by
um
t = um(am
t |at, θt, lpt
)
= zm
t
∑
n∈N
1lnt ∈Rm(lmt )1
xn
t =m
×
((
1 − yn
t
)
cm(P)
t + yn
t cm(I)
t
−ζm(Em
a + En,m(at|θn
t , lnt , lmt , zm
t
)))
. (12a)
In (12a), θt = {θn
t }n∈N ∈ � = ×n∈N�n and lpt = {lnt }n∈N ∈
Lp = ×n∈NLn are, respectively, the task parameters and 2-D
locations of peers at stage t; ζm is the cost per energy unit
payable by BSm; Em
a is the energy consumption of active aerial
BSm per mining stage (for a terrestrial BSm, m ∈ MT , we have
Em
a = 0); and
En,m(at|θn
t , lnt , lmt , zm
t
) = En,m(I)(at|lnt , lmt
)
+ En,m(P)
(
an
t |θn
t , lnt , lmt , zm
t
)
(12b)
is the total energy spent by BSm on transacting the task, i.e.,
the sum of energy consumed by BSm on forwarding the task
to the blockchain server and energy spent by peer BSm on
processing the task at its local MEC server.
The payoff in (12a) is a function of: 1) stage parameters
am
t of BSm and 2) offloading decisions at, task parameters θt,
and 2-D locations lpt of peers. Note that BSm does not know
the exact values of (at, θt, lpt ) prior to selecting its parame-
ters am
t at the beginning of stage t, although it can observe
these values once all the peers have decided on their offload-
ing options, e.g., at the end of stage t. As such, no BSm
can directly estimate its stage payoff in (12a) when select-
ing am
t . On the other hand, as each peer Pn is selfish, its
offloading decision an
t ∈ An should satisfy (10a) for myopic
peers or (10b) for fully rational peers. As such, we obtain
the following optimization problem that reflects the decision
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
1982 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
process of BSm
max
am
t ∈Am
Vm(am
t |at
)
subject to:
{
(10a), for myopic peers
(10b), for fully-rational peers
(13a)
where Vm(am
t |at) is the expected long-term payoff of BSm
from parameters am
t ∈ Am given the decision profile at of the
peers, expressed by
Vm(am
t |at
) =
{ ∞∑
τ=t
γ τ−tum
(
am
τ
∣
∣∣aτ , θτ , lpτ
)∣∣∣am
t , at
}
. (13b)
To solve the problem in (13a), BSm must be able to compute
the expected stage payoff Un(Bn
t , an|sn
t ) or expected long-term
payoff Vn(Bn
t , an|sn
t ) of each peer Pn. Both the stage and long-
term payoffs depend on the peer’s belief Bn
t and parameters sn
t
= (θn
t , lnt , ct, lt, zt, Qn
t ) = (θn
t , lnt , am
t , a−m
t , Qn
t ), where a−m
t =
{ai
t}i∈M\{m} = {ci
t, lit, zi
t}i∈M\{m} ∈ A−m are the parameters of
all BSs other than BSm. Note that BSm knows neither the
peer’s beliefs Bn
t nor the parameters a−m
t selected by other BSs,
as there is no information exchange among BSs. As a result, no
BSm can directly estimate the expected payoffs of peers and,
thus, their decisions at. Therefore, (13a) casts as a stochastic
optimization problem where the decisions at, task parameters
θt, and 2-D locations lpt of peers are the random variables—the
values of (at, θt, lpt ) can be observed by BSm only after it has
selected its own parameters am
t .
C. Stackelberg Model of the BaaS-MEC Support
Based on the provided description of the BaaS-MEC model,
the interactions between the BSs and peers can be modeled
as a stochastic Stackelberg game with multiple leaders. The
game is played repeatedly at each stage of the mining pro-
cess. In the game, both the leaders/BSs and followers/peers
act selfishly and independently. At the beginning of stage
t, each leader/BSm independently, without coordinating its
actions with other BSs, selects its stage parameters am
t =
(cm
t , lmt , zm
t ) ∈ Am maximizing its expected long-term payoff
Vm by solving the problem in (13a). Upon observing the stage
parameters at = {am
t }m∈M ∈ A of BSs, each follower/peer
Pn independently, without coordinating its actions with other
peers, selects the offloading option an
t = (yn
t , xn
t ) ∈ An which
satisfies (10a) if the peer is myopic or (10b) if the peer is
fully rational. As such, the action am
t of BSm is observable
only by peers, but not by other BSs. On the other hand, the
action an
t of peer Pn is observable only by BSs, but not by other
peers. Hence, both BSm and peer Pn operate under incomplete
information about actions a−m
t ∈ A−m and a−n
t ∈ A−n of other
BSs and peers, respectively. Note that the game is stochas-
tic, since the payoffs of leaders/BSs and followers/peers are
defined by the stochastic parameters with unknown transitions.
In particular, the payoff un of peer Pn depends on parameters
sn
t = (θn
t , lnt , at, Qn
t ) which include not only the actions at of
BSs but also on the peer’s task parameters, location (θn
t , lnt ),
and queue backlogs Qn
t . The payoff um of BSm depends on
the peers’ actions, task parameters, and locations (at, θt, lpt ).
The data flow between leaders and followers in the proposed
Stackelberg model is shown in Fig. 2.
Fig. 2. Data flow between leaders and followers in the Stackelberg model.
Recall that in the conventional Stackelberg games,
a leader/BSm selects its action am
t by predicting actions at
of its followers/peers by backward induction [33]. However,
in our game, the action an
t selected by peer Pn depends on the
actions a−m
t of other BSs and the peer’s belief Bn
t , unknown
to BSm. Thus, no BSm can directly estimate the peers’ actions
at. Instead, it can learn to predict these actions during their
repeated interactions with the followers. In the following, we
formulate the game to describe the interactions of follow-
ers/peers in the Stackelberg model, develop the POMDP model
to find the game solution, and propose the novel unsuper-
vised BRL and BDL algorithms that allow each follower to
update its belief and estimate its optimal strategy in response
to actions of other followers and leaders. Next, we represent
the decisions of each leader/BS in the Stackelberg model as
an MDP and devise the unsupervised RL and DQL algorithms
for the leader to select its actions in the way maximizing its
long-term payoff.
VI. GAME OF FOLLOWERS IN THE STACKELBERG MODEL
A. Stochastic Game of Followers With Unobservable Actions
The interactions of followers/peers in the Stackelberg model
can be modeled as a stochastic noncooperative game with un-
observable players’ actions, denoted as �. The game is played
repeatedly by a set N of players, i.e., peers, at each stage of the
mining process. At any stage t, the action an
t ∈ An of player Pn
represents its offloading decision. As no player Pn observes the
actions a−n ∈ A−n of other players, information about a−n is
incomplete and the player constructs its belief Bn(a−n). At the
beginning of stage t, player Pn is in state sn
t = (θn
t , lnt , at, Qn
t )
∈ Sn that combines all parameters known to the player: 1) task
parameters and location (θn
t , lnt ) ∈ �n×Ln; 2) actions at ∈ An
of leaders/BSs; and 3) queue backlogs Qn
t ∈ �n. As such, the
current state sn
t is observable by the player before it decides
on its action an
t , but the next state sn
t+1 ∈ Sn is unknown, i.e.,
the state is fully observable, but stochastic. As a result, game
� is defined as follows.
Definition 1: A stochastic game with unobservable players’
actions is the tuple � = (N,�n, Sn, An, Pr, un) defined by
the following elements: 1) set of N players N and, for each
player Pn; 2) partially observable state space �n = Sn × A−n
which combines observable space Sn and unobservable space
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1983
A−n; 3) set of actions An defined in (2); 4) transition dynam-
ics Pr{śn, á−n|sn, an, a−n}, i.e., probability of transiting to state
(śn, á−n) ∈ �n given action an ∈ An taken in state (sn, a−n)
∈ �n which can be factored into two conditional distribu-
tions, Pr{śn|sn, an, a−n} and Pr{á−n|a−n}; and 5) instantaneous
payoff un(an, a−n|sn) from action an ∈ An taken in state
(sn, a−n) ∈ �n defined in (8a).
Note that in conventional stochastic games with observable
actions, the player’s pure strategy αn : �n → An is a mapping
from its state space �n to action space An [33]. However, in
game �, state space �n comprises two subsets, Sn and A−n,
where A−n is unobservable. As such, the player’s pure strategy
αn(Bn, sn) = an ∈ An is the mapping from its belief Bn and
observable state sn ∈ Sn to the action an ∈ An [32], [34]. The
game proceeds as follows: 1) at the beginning of stage t, upon
observing state sn
t , player Pn selects its best-response strat-
egy αn(Bn
t , sn
t ) = an
t and 2) after taking action an
t , the player
receives a payoff un(an, a−n|sn). The procedure is repeated at
each game stage.
One of the possible solution concepts applicable to game �
is a Bayesian Nash equilibrium (BNE) [33]. In words, in the
BNE, no player believes that it can increase its expected stage
payoff by deviating from its pure strategy. Formally, a BNE
for game � can be defined as follows.
Definition 2: A BNE for game � is the tuple (α, B) defined
by the players’ pure strategy profile α = {αn}n∈N ∈ A and the
beliefs B = {Bn}n∈N, such that
Un(Bn
, αn|sn) ≥ Un
(
B
n
, a
n|sn
)
∀an ∈ An ∀sn ∈ Sn (14)
for all n ∈ N, where Un(Bn, an|sn) is an expected stage payoff
of player Pn from the action an taken in state sn with belief
Bn defined in (9).
Note that as the number of players N and their state–action
space S × A are finite, at any stage, game � has a BNE [33].
Unfortunately, a BNE concept is rather incomplete for analysis
of stochastic games played repeatedly over multiple stages, as
it puts no restrictions on the dynamics (i.e., updating rule) of
players’ beliefs [32]–[34]. Hence, in order to refine a solution
generated by the BNE, the notion of an MPBE [32] can be uti-
lized. In words, MPBE is a BNE where at any stage t, the players’
beliefs are consistent, i.e., updated with the Bayesian inference,
and the players’ pure strategies are sequentially rational, i.e., for
any information set or history (sn
0, an
0, . . . , sn
t−1, an
t−1, sn
t ), the
strategies of players maximize their expected payoffs. Formally,
the MPBE for game � can be defined as follows.
Definition 3: The MPBE for the game � is the tuple (α, B)
defined by the players’ pure strategy profile α = {αn}n∈N ∈ A
and the beliefs B = {Bn}n∈N, satisfying (14), where at every
stage t, the beliefs are updated according to the Bayes’ rule
based on the last observed transition (sn, an, śn), for all n ∈ N,
as in
Bn(a−n) = Bsn,śn
an
(
a−n)
= Pr
{
śn|sn, an, a−n}Bn
(
a−n
)
∑
á−n∈A−n Pr
{
śn|sn, an, á−n}Bn
(
á−n
) . (15)
Similar to the BNE, game � admits an MPBE, as the num-
ber of players and their state–action space are finite [33].
Note that in the MPBE, players are myopic or short sighted,
since their strategies maximize one-stage (rather than long-
term) payoffs. In this regard, the PBE concept [32], [33] is
more suitable for game �. In words, PBE is the MPBE in
which the players’ pure strategies maximize their expected
long-term payoffs. Formally, a PBE for game � can be defined
as follows.
Definition 4: A PBE for game � is the tuple (α, B) defined
by the players’ pure strategy profile α = {αn}n∈N ∈ A and the
beliefs B = {Bn}n∈N, such that
Vn(Bn
, αn|sn) ≥ Vn(Bn
, an|sn) ∀an ∈ An ∀sn ∈ Sn (16a)
for all n ∈ N, where Vn(Bn, an|sn) is the expected long-term
payoff of player Pn from the action an taken in state sn with
belief Bn, defined by
Vn(Bn, an|sn) = Un(Bn, an|sn)+ γ
∑
śn∈Sn
Pr
{
śn|sn, an, Bn}
× Vn
(
Bsn,śn
an , an|śn
)
=
∑
a−n∈A−n
Bn(a−n)Vn(an, a−n|sn) (16b)
where for any transition (sn, an, śn), the player’s belief Bsn,śn
an
is updated with Bayes’ rule in (15); and Vn(an, a−n|sn) is the
expected long-term payoff of player Pn from the action profile
(an, a−n) given state sn defined in (11b).
Similar to the BNE and MPBE, game � admits a PBE,
as the number of players and their state–action space are
finite [33]. A quantitative comparison between the BNE, PBE,
and MPBE is provided, e.g., [33].
B. Bayesian Reinforcement Learning by the Followers
In the repeated game settings, any player can deduce the
unknown state transition dynamics and update its belief with
stochastic dynamic programming and RL [34]–[36]. In the fol-
lowing, we develop the BRL framework that allows every
player/peer to compute its best response strategy and estimate
the distributions of observable states and unobservable actions
via repeated interactions with each other and with a stochas-
tic environment. We begin with the definition of POMDPs
which will be further used to develop our RL model. Formally,
a POMDP is defined as follows [32], [36].
Definition 5: POMDP is a tuple (�, S, A, Pr, u, γ ) defined
by the following elements: 1) partially observable state space
� = S × � which combines observable space S and unob-
servable space �; 2) action set A; 3) transition dynamics
Pr{ś, θ́ |s, a, θ} = Pr{ś|s, a, θ}Pr{θ́ |θ}—probability of transit-
ing to state (ś, θ́ ) ∈ � given action a ∈ A taken in state
(s, θ) ∈ �; and 4) payoff u(a|s) from action a ∈ A taken in
state s ∈ S; v) discount rate γ ∈ (0, 1].
As the transition Pr{θ́ |θ} of unknown state θ ∈ � can-
not be observed directly, a learner must rely on an observed
transition (s, a, ś) to infer an underlying probability distribu-
tion or belief B = {B(θ)}θ∈�, for B(θ) = Pr{θ}. The BRL
problem is to find a pure strategy α(B) ∈ A that maximizes
the value Vα(B) = ∑∝
t=0 γ tu(α(Bt)|st), i.e., long-term payoff
of a learner. As such, the optimal pure strategy α maximizes
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
1984 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
the value in all beliefs states, i.e., Vα(B) ≥ Vα(B), for all α,
and satisfies the Bellman optimality equation [32], [36]
Vα(B) = max
a∈A
⎛
⎝u(a|s) + γ
∑
ś∈S
Pr
{
ś|s, a, B
}
Vα
(
Bs,ś
a
)
⎞
⎠ (17)
where the belief Bs,ś
a is updated according to Bayes’ rule.
Apparently, the problem of determining the best response of
each player in a stochastic game under incomplete information
can be cast as a POMDP [34], [35]. In particular, when applied
to game �, the POMDP for player Pn is the tuple (�n, Sn, An,
Pr, un, γ ) with elements defined in Section VI-A. The player’s
strategy αn(Bn, sn) ∈ An maps from its belief Bn and state sn
to action an. Hence, for any player Pn, the BRL problem is to
find the best response strategy αn which maximizes the value
or expected long-term payoff Vn(Bn, an|sn). We can solve this
problem by stochastic dynamic programming [36], [37]. Then,
we obtain a dynamic programming algorithm where, at every
stage t of mining/learning process, we store the set of |Sn ×A|
updated values Vn
t+1(a
n, a−n|sn) indexed by the state sn ∈ Sn
and actions (an, a−n) ∈ A.
The algorithm is as follows. At stage t = 0, we initialize the
belief Bn
0 and value Vn
0 . At stage t = 0, . . . , T (where Trd ≤ T
is a recursion depth [38] which, in general, can be infinite,
i.e., Trd = T → ∞), we repeat the following two steps.
1) Step 1: Observe the current state sn
t and update the belief
Bn
t (a
−n) = Bsn,śn
an (a−n) with Bayes’ rule in (15) for the
last observed transition (sn, an, śn), where sn = sn
t−1,
an = an
t−1, śn = sn
t .
2) Step 2: Based on the stored values Vn
t (an, a−n|sn), find
the best response strategy αn(Bn
t , sn
t ) = an
t for the current
belief Bn
t and state sn
t , and compute and store values
Vn
t+1(a
n, a−n|sn) by solving the Bellman equation
an
t = argmax
an∈An
Vn
t+1
(
Bn
t , an|sn
t
)
(18a)
for
Vn
t+1
(
Bn
t , an|sn
t
) = Un(Bn
t , an|sn
t
)
+ γ
∑
śn∈Sn
Pr
{
śn|sn
t , an, Bn
t
}
× Vn
t
(
Bsn,śn
an , an|śn
)
=
∑
a−n∈A−n
Bn
t
(
a−n)Vn
t+1
(
an, a−n|sn
t
)
∀an ∈ An (18b)
where
Vn
t+1
(
an, a−n|sn
t
) = un(an, a−n|sn
t
)
+ γ
∑
śn∈Sn
Pr
{
śn|sn
t , an, a−n}
× Vn
t
(
an, a−n|śn) (18c)
is computed and stored for all (an, a−n) ∈ A.
The above BRL algorithm does not require any additional
exploration, as it is implicit in the computation of the Bellman
equation in (18b) and (18c). Nonetheless, when the exploration
ability is limited, e.g., due to a finite recursion depth Trd, it
can be useful to utilize some explicit exploration, e.g., a simple
ε-greedy exploration [38] in which, for any arbitrary small ε ∈
[0, 1), the best response strategy an
t is taken with probability
1 − ε, and any other strategy án
t ∈ An\{an
t } is selected with
probability ε. The practical implementation of the algorithm,
such as the estimation of Bayesian inference in step 1 and state
transition probabilities in step 2, is discussed in Appendix C,
in the supplementary material. Proposition 3 shows that the
proposed BRL algorithm defined by the sequence of iterations
{Vn
t }t∈N
in (18a)–(18c) converges to the optimal value V
n
,
i.e., the value of the best response strategy αn that represents
a solution of the Bellman optimality equation.
Proposition 3: The BRL algorithm defined by the sequence
of iterations {Vn
t }t∈N
in (18a)–(18c) converges to the optimal
value V
n
, for all n ∈ N, with probability one as time tends to
infinity.
The proof of Proposition 3 is provided in Appendix D, in
the supplementary material. From Proposition 1, we obtain
Corollary 1 which shows that the BRL algorithm {Vn
t }t∈N
con-
verges to a stable state where the pure strategies and beliefs
or the players form the PBE of game �.
Corollary 1: With probability one as time tends to infin-
ity, the BRL algorithm defined by the sequence of iterations
{Vn
t }t∈N
in (18a)–(18c), for all n ∈ N, converges to a stable
state where the players’ pure strategies and beliefs (α, B) are
in the PBE of game �.
The proof of Corollary 1 is provided in Appendix E, in
the supplementary material. The worst-case computational
complexity and convergence rate of the BRL algorithm is
established in Proposition 4.
Proposition 4: The BRL algorithm defined by the sequence
of iterations {Vn
t }t∈N
in (18a)–(18c) has the worst-case time
complexity equal to O(cM+N), for c = max{maxm∈M |Am|,
maxn∈N |An|}, and the asymptotic rate of convergence equal
to O(1/t1−γ ), for γ > 0.5 and O(
√
log(log t)/t), otherwise.
The proof of Proposition 4 is provided in Appendix F, in
the supplementary material.
VII. BAYESIAN DEEP LEARNING BY THE FOLLOWERS
From Proposition 4, the proposed BRL algorithm has the
sublinear convergence rate and the time complexity O(cM+N)
exponential in the total number of leaders/BSs and follow-
ers/peers M + N. Therefore, the BRL algorithm is intractable
if the number M + N is large and, to be practically realizable,
it requires an approximation. Consequently, in this section, we
propose a novel unsupervised polynomial-time BDL algorithm
where uncertainties about unobservable states are modeled by
BNNs. This allows us to reduce the size of the unobserv-
able state space and, hence, the algorithm complexity. The
rest of this section is structured as follows. In Section VII-A,
we present the existing BDL framework. In Section VII-B, we
devise the BNN model of the decisions of followers/peers. In
Section VII-C, we develop the BDL algorithm for the peers.
A. Deep Learning With Bayesian Neural Networks
Although DL algorithms have a proven record in producing
accurate solutions of MDPs for various problems (e.g., [39]
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1985
and [40]), they are not efficient when applied to the POMDPs.
The main reason is that the DL methods cannot model
epistemic uncertainties about unobservable model parame-
ters which are inherent in POMDPs where such uncertainties
are formalized as probability distributions over unobservable
states or beliefs [41]–[43]. However, epistemic uncertainties
can be efficiently modeled by the BDL methods where these
uncertainties are captured with BNNs—NNs with random
weights distributed according to a predefined prior [41]. Then,
instead of directly optimizing the network weights, as in DL,
we average over all possible weights, which is referred to
as a marginalization [43]. A brief description of the BDL
methods is provided below (for more details, see [41]–[43]).
Consider a data set (X, Y) = {(xt, yt)}t=1,...,F which con-
sists of F data points (xt, yt), where xt ∈ R
1×DI is the row
vector of inputs with DI elements and yt ∈ R
1×DO is the
row vector of observed outputs or observation targets with
DO elements. Let Pr{yt|f W(xt)} be the likelihood of data point
(xt, yt), where f W(xt) = ŷt ∈ R
1×DO is the random row output
vector of the BNN with some prior distribution over its weights
W, e.g., standard normal distribution Pr{W} = N(0, I). In
regression, the likelihood is usually represented by a Gaussian
distribution Pr{yt|f W(xt)} = N(f W(xt), σ
2), with mean f W(xt)
defined by the model and some given observation noise vari-
ance σ 2. In classification, the observed target output yt ∈ Y
= {1, . . . ,DO} is a label in the set Y = {1, . . . ,DO}. As such,
a random output f W(xt) = ŷt = (ŷ1
t , . . . , ŷDO
t ) ∈ R
1×DO of
the BNN model is predicted being classified with the label
yt = i ∈ Y. To use the model for classification, we pass
the BNN output through an elementwise softmax function to
obtain the likelihood of the output label yt = i ∈ Y, defined
as [41], [43]
Pr
{
yt = i|f W(xt)
}
= Softmax
(
i, f W(xt)
)
= eŷi
t
∑DO
j=1 eŷj
t
. (19)
Given the likelihood Pr{yt|f W(xt)}, we must esti-
mate the posterior Pr{W|X, Y} over weights W given
(X, Y). In a model-based BRL, the posterior is esti-
mated with the Bayesian inference Pr{W|X, Y} = Pr{W}
Pr{Y|X, W}/ Pr{Y|X}. However, with the BNNs, the com-
putation of inference is intractable, because the marginal
probability Pr{Y|X} cannot be evaluated analytically [43].
Hence, instead of direct inference, in BDL, a variational
inference [44] is utilized where a posterior Pr{W|X, Y} is fit-
ted with a simple distribution qθ (W) parameterized by θ , so
that Pr{W|X, Y} ∼ qθ (W), where qθ (W) is an optimal dis-
tribution. This replaces the intractable problem of averaging
over model weights W with a simpler problem of optimizing
parameters θ . In particular, the optimal parameters θ minimize
the Kullback–Leibler (KL) divergence to the true posterior
Pr{W|X, Y}, i.e.,
qθ (W) = min
θ
DKL
(
qθ (W)|Pr{W|X, Y})) (20a)
where
DKL
(
qθ (W)
∣∣∣Pr{W|X, Y})
)
=
∫
qθ (W)log
qθ (W)
Pr{W|X, Y}dW
(20b)
is the KL divergence, i.e., difference between the distribution
qθ (W) and the posterior Pr{W|X, Y}. Note that minimization
of the KL divergence in (20b) is equivalent to maximization
of the evidence lower bound (ELBO), given by
ELBO(θ) =
∫
qθ (W)log Pr{Y|X, W}dW
− DKL
(
qθ (W)|Pr{W}))
=
F∑
t=1
∫
qθ (W) log Pr
{
yt|f W(xt)
}
dW
− DKL
(
qθ (W)|Pr{W}))
≤ log Pr{Y|X} (20c)
where log Pr{Y|X} is the log-evidence. By maximizing the first
term in (20c), we maximize the expected log-likelihood. By
maximizing the second term, we minimize the KL divergence
between the distribution qθ (W) and a prior Pr{W}.
A practical approach to approximate inference in
the large and complex BNNs is a dropout variational
inference [39], [45]–[47] where dropout is interpreted as a
variational Bayesian approximation and the approximating
distribution qθ (W) is a mixture of two Gaussians with small
variances and the mean of one of Gaussians fixed at zero.
The objective is to minimize the approximation loss, given
by [41]
L(θ) = −ELBO(θ) = −
F∑
t=1
∫
qθ (W) log Pr
{
yt|f W(xt)
}
dW
+ DKL
(
qθ (W)|Pr{W}))
= −
F∑
t=1
log Pr
{
yt|f Ŵt(xt)
}
+
∫
qθ (W)log
qθ (W)
Pr{W} dW
(21a)
where the weights Ŵt are distributed according to qθ (W), i.e.,
Pr{Ŵt} = qθ (W). Note that in classification, a log-likelihood
of each observed output label yt = i ∈ Y takes the form
log Pr
{
yt = i|f Ŵt(xt)
}
= log
(
Softmax
(
i, f Ŵt(xt)
))
(21b)
where f Ŵt(xt) = ŷt = (ŷ1
t , . . . , ŷDO
t ). Thus, to find the optimal
parameters θ of qθ (W), we must estimate the derivatives of
loss L(θ) with respect to θ , which can be done with the Monte
Carlo (MC) estimation [45].
A rather efficient MC estimator is a pathwise derivative esti-
mator (PDE), also called an infinitesimal perturbation, a repa-
rameterization trick, or stochastic backpropagation [41], [43].
In the PDE, qθ (W) is reparameterized as a parameter-free dis-
tribution q(ε) subject to Ŵt = g(θ, ε), where ε > 0 is an
infinitesimal number; and g(·, ·) is a deterministic differen-
tiable bivariate transformation. For example, for a Gaussian
qθ (W) = N(μ, σ 2), with θ = (μ, σ ), we can set Ŵt =
g(θ, ε) = μ + σε and q(ε) = N(0, I). Then, for the normal
prior Pr{W} = N(0, I), the loss L(θ) takes the form [41], [43]
L(θ, λ) = 1
2
(1 − λ)‖θ‖2 −
F∑
t=1
log Pr
{
yt|f g(θ,ε)(xt)
}
(22a)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
1986 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
where λ ∈ [0, 1) is a dropout probability or rate, which also
must be optimized. In classification, a log-likelihood of every
observed output label yt = i ∈ Y is given by
log Pr
{
yt = i|f g(θ,ε)(xt)
}
= log
(
Softmax
(
i, f g(θ,ε)(xt)
))
(22b)
where f g(θ,ε) = ŷt = (ŷ1
t , . . . , ŷDO
t ).
Then, based on the theoretical findings in [41], if the num-
ber of data points F in the data set (X, Y) is sufficiently large,
then:
1) loss L(θ, λ) in (22a) approaches the loss L(θ) in (21a),
i.e.,
L(θ, λ) −→
F→∞L(θ) = −ELBO(θ)
= DKL
(
qθ (W)|Pr{W|X, Y}))
2) weights Ŵt generated according to optimal approximat-
ing distribution qθ (W) = qθ (W) approach weights Wt
updated with the Bayesian inference, i.e., Ŵt −→ Wt
and, hence
⎧
⎪⎪⎨
⎪⎪⎩
Pr{W|X, Y} −→
F→∞qθ (W) = Pr
{
Ŵ
}
Pr{y|x, X, Y} −→
F→∞
∫
Pr
{
y|f Ŵ(x)
}
qθ (W)dW = qθ (y|x)
Pr
{
Y|X, Ŵ
} −→
F→∞ Pr{Y|X}.
B. Bayesian Neural Network Model for the Followers
Based on the existing BDL framework presented above, in
this section, we formulate the BNN model of the decision
taken by each follower/peer Pn. To approximate unobserv-
able state transitions, the model adopts a feedforward BNN
f W(x) defined by random weights W that follow a stan-
dard normal prior distribution Pr{W} = N(0, I). The data set
(Xt, Yt)= {(xi, yi)}i=t−T,...,t−1 available at stage t of the learn-
ing/mining process represents the history about the past
T = min(t, F) consecutive observations or data points (xi, yi),
where F is the size of the observation window. A row input
vector xi = (x0
i , . . . , xDI−1
i ) = (an
i , sn
i ) ∈ R
1×DI has DI =
7M+5 elements that represent the action an
i = (yn
i , xn
i ) ∈ R
1×2
and state sn
i = (θn
t , lnt , at, Qn
t ) ∈ R
1×(7M+3) of peer Pn at stage
i. The output or observation target yi ∈ Y = {0, 1} represents
the payoff un
i ∈ Un = {ξn − Cn(an
t |sn
t ),−Cn(an
t |sn
t )}, such that
un
i = un(an
i , a−n
i |sn
i
)
=
{
ξn − Cn
(
an
i |sn
i
)
, Dn
(
an
i , a−n
i |sn
i
) ≤ θ
n(D)
i
−Cn
(
an
i |sn
i
)
, Dn
(
an
i , a−n
i |sn
i
)
> θ
n(D)
i
received by the player at the end of stage i, that can take only
two values, ξn − Cn(an
t |sn
t ) or − Cn(an
t |sn
t ). In particular, the
output yi is labeled as
yi =
{
1, un
(
an
t , a−n
t |sn
t
) = ξn − Cn
(
an
t |sn
t
)
0, un
(
an
t , a−n
t |sn
t
) = − C
n(
an
t |sn
t
)
.
(23a)
As such, we have a classification task where, given the
data set (Xt, Yt) at stage t, we predict the random output
f W(xt) = ŷt = (ŷ0
t , ŷ1
t ) ∈ R
1×DO , for DO = 2, being clas-
sified as 0 or 1. In a feedforward BNN with K ≥ 1 neurons
and input vector xt = (an
t , sn
t ) ∈ R
1×DI , the row output vector
is given by [41]
f W(xt) = ϕ
(
xtW(h) + b
)
WT
(v) = ŷt =
(
ŷ0
t , ŷ1
t
)
∈ R
1×DO .
(23b)
In (23b), the weights represent the tuple = (W(h), W(v), b)
∈ R
(DI+DO+1)×K , where W(h) = (wi,j
(h)) ∈ R
DI×K and W(v) =
(wi,j
(v)) ∈ R
DO×K are the weights in the hidden and visible lay-
ers, respectively; b = (bj) ∈ R
1×K are the biases; and ϕ(·)
is a logistic sigmoid function. Similar to the existing works
(e.g., [41]–[43]), we assume the standard normal prior distri-
bution over weights W, i.e., Pr{W} = N(0, I), where the prior
weight distribution Pr{W} approximates the belief Bn(a−n)
about unobservable actions a−n, i.e., Bn ∼ Pr{W}. Then, the
likelihood of the data point (xt, yt), given by
Pr
{
yt|f W(xt)
}
=
⎧
⎪⎨
⎪⎩
Softmax
(
0, f W(xt)
) = eŷ0
t
eŷ0
t +eŷ1
t
, yt = 0
Softmax
(
1, f W(xt)
) = eŷ1
t
eŷ0
t +eŷ1
t
, yt = 1
(24)
is equivalent to the probability Pr{un
t |sn
t , an
t , Bn
t } of the payoff
un
t ∈ Un received from the action an
t executed by the player Pn
in state sn
t , i.e., Pr{un
t |sn
t , an
t , Bn
t } ∼ Pr{yt|f W(xt)}. Accordingly,
the expected stage payoff Un(Bn
t , an|sn
t ) is approximated by the
function Un(xt|W), as in
Un(Bn
t , an|sn
t
) =
∑
a−n∈A−n
Bn
t
(
a−n)un(an, a−n|sn
t
)
=
∑
un
t ∈Un
un
t Pr
{
un
t |sn
t , an
t , Bn
t
}∼ Un(xt|W)
=
∑
yt∈Y
ytPr
{
yt|f W(xt)
}
. (25)
Recall that in the model-based BRL, the beliefs are updated
with the Bayesian inference, i.e., at any stage t, given the
prior Bn
t ∼ Pr{W}, we must estimate a posterior B
sn
t ,u
n
t
an
t
∼
Pr{W|xt, yt} for every observed transition (an
t , sn
t , un
t ) =
(xt, yt) with Bayes’ rule. That is, we must compute
B
sn
t ,u
n
t
an
t
= Pr
{
un
t |sn
t , an
t , a−n}Bn
t
(
a−n
)
Pr
{
un
t |sn
t , an
t
}
= Pr
{
un
t |sn
t , an
t , Bn
t
(
a−n
)}
Bn
t
(
a−n
)
∑
á−n∈A−n Pr
{
un
t |sn
t , an
t , Bn
t
(
á−n
)}
Bn
t
(
á−n
)
∼ Pr{W|xt, yt}
= Pr
{
yt|f W(xt)
}
Pr{W}
Pr{yt|xt} = Pr
{
yt|f W(xt)
}
Pr{W}
∫
Pr
{
yt|f W(xt)
}
Pr{W}dW
(26)
which is intractable, as the marginal probability Pr{yt|xt} =∫
Pr{yt|f W(xt)}Pr{W}dW cannot be evaluated analytically.
Hence, instead of direct inference, we utilize the variational
inference. Then, instead of integrating over the weights W, we
find the parameters θ of an approximating distribution qθ (W)
which minimize the KL divergence in (20b) or, equivalently,
maximize the ELBO in (20c), so that a posterior Pr{W|xt, yt}
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1987
approaches the optimal distribution qθ (W) = qθ (W) as time
tends to infinity, i.e., Pr{W|xt, yt}−→
t→∞qθ (W).
To estimate variational inference, we exploit the PDE. For
this, we reparameterize the approximating distribution qθ (W)
by parameter-free distribution q(ε) subject to Ŵt = g(θ, ε).
Then, for the Gaussian approximating distribution qθ (W) =
N(μ, σ 2), with θ = (μ, σ ), Ŵt = g(θ, ε) = μ+σε, and q(ε)
= N(0, I), the BDL problem is to find the parameters (θ, λ)
which minimize the classification loss L(θ, λ), given by
L(θ, λ) = −
t−1∑
i=t−T
(
1yi=0 log
(
Softmax
(
0, f g(θ,ε)(xi)
))
+ 1yi=1log
(
Softmax
(
1, f g(θ,ε)(xi)
)))
+ 1 − λ
2
‖θ‖2. (27)
Then, the BDL problem can be solved by stochastic gra-
dient descent [41], [42]. That is, at any stage t, given the
data set (Xt, Yt), we minimize the current loss Lt = L(θt, λt)
by updating parameters (θt, λt) in the direction of a negative
gradient until (θt, λt) converges, as in
θt+1 = θt − ηt∇θt
Lt and λt+1 = λt − ηt∇λt
Lt (28a)
where ηt is the learning rate of a stochastic gradient descent,
such that
∑∞
t=1 ηt = ∞ and
∑∞
t=1 η2
t <∞, typically set as
ηt = 1/(t + η1)
η2 ∀ η1 ∈ (0, 1) ∀η2 ∈ (0, 1). (28b)
C. Bayesian Deep Learning by the Followers
Based on the presented BNN model, in this section, we
develop a novel BDL algorithm for each peer Pn. The proposed
algorithm is unsupervised, i.e., it enables a self-organized
“online” learning which does not require a prior “off-line”
training and/or pre-existing training data sets. To employ
the PDE, we utilize the parameter θt = (θt(h), θt(v), θt(b)) ∈
R
(DI+DO+1)×K , where θt(h) = (θ
i,j
t(h)) ∈ R
DI×K , θt(v) = (θ
i,j
t(v)) ∈
R
DO×K , and θt(b) = (θ
j
t(b)
) ∈ R
1×K . The elements θt(h) and
θt(v) define the weights Ŵ(h) ∈ R
DI×K and Ŵ(v) ∈ R
DO×K in
hidden and visible layers of a BNN model, respectively. The
element θt(b) defines the biases b̂ ∈ R
1×K of the BNN. As
such, at any stage t, the Gaussian approximating distribution
qθ (Wt) =
∏
i,j
qθ
i,j
(h)
(
wi,j
t(h)
)∏
i,j
qθ
i,j
(v)
(
wi,j
t(v)
)∏
j
qθ i
(b)
(
bj
t
)
is reparameterized by the standard normal parameter-free
distribution, as
q(εt) =
∏
i,j
q
(
ε
i,j
t(h)
)∏
i,j
q
(
ε
i,j
t(v)
)∏
j
q
(
ε
j
t
)
(29a)
subject to:
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
ŵi,j
(h) = g
(
θ
i,j
t(h), ε
i,j
t(h)
)
∀i ∈ {0, . . . , DI − 1}
ŵi,j
(v) = g
(
θ
k,j
t(v), ε
k,j
t(v)
)
∀k ∈ {0, DO − 1}
b̂j = g
(
θ
j
t(b)
, ε
j
t(b)
)
(29b)
for j ∈ {0, . . . , K−1}, εt = (εt(h), εt(v), εt(b)) ∈ R
(DI+DO+1)×K ,
εt(h) = (ε
i,j
t(h)) ∈ R
DI×K , εt(v) = (ε
i,j
t(v)) ∈ R
DO×K , and εt(b) =
(ε
j
t(b)
) ∈ R
1×K .
Based on the above, we obtain the BDL algorithm for each
peer Pn where, at any stage t, we store the updated parameters
(θt, λt) and data set (Xt, Yt) = {(xi, yi)}i=t−T,...,t−1. At stage
t = 0, we initialize the parameter θ0 according to a distribution
Pr{θ0} = Pr{Ŵ} = N(0, I), i.e., for all i ∈ {0, . . . , DI −1}, k ∈
{0, . . . , DO − 1}, j ∈ {0, . . . , K − 1}, draw (DI + DO + 1)K
random samples θ
i,j
0(h), θ
k,j
0(v), θ
j
0(b)
from a standard normal dis-
tribution. At stage t = 0, . . . , Trd, we repeat the following five
steps.
1) Step 1: Generate parameter εt according to q(ε) =
N(0, I), i.e., for all i ∈ {0, . . . , DI − 1}, k ∈
{0, . . . , DO−1}, j ∈ {0, . . . , K−1}, draw (DI+DO+1)K
random samples ε
i,j
t(h), ε
k,j
t(v), ε
j
t(b)
from the standard
normal distribution.
2) Step 2: Given the stored parameters (θt, λt) and the
data set (Xt, Yt) = {(xi, yi)}i=t−T,...,t−1, estimate the
weights Ŵ = g(θt, εt) according to (29b).
3) Step 3: Observe the current state sn
t and find a pure strat-
egy αn
t (θt, εt) = argmaxan∈An Un(an, sn
t |g(θt, εt)) for
weights Ŵ = g(θt, εt), that maximizes the expected
stage payoff of the player, given by
Un(an, sn
t |g(θt, εt)
) =
∑
yt∈Y
ytPr
{
yt|f Ŵ(an, sn
t
)}
(30a)
where f Ŵ(an, sn
t ) and Pr{yt|f Ŵ(an, sn
t )} are given by
(23b) and (24), respectively;
4) Step 4: Take action an
t = αn
t (θt, εt), observe pay-
off un
t , and update the data point as (xt, yt) =
(an
t , sn
t , un
t ) and the data set as (Xt+1, Yt+1) = (Xt, Yt)∪
(xt, yt)\(xt−T , yt−T).
5) Step 5: Given weights Ŵ, update the parameters (θt, λt)
in the direction of negative gradient, i.e., for all j ∈
{0, . . . , K− 1}, compute
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
θ
i,j
t+1(h) = θ
i,j
t(h) − ηt∂Lt/∂θ
i,j
t(h) ∀i ∈ {0, . . . , DI − 1}
θ
i,k
t+1(v) = θ
i,k
t(h) − ηt∂Lt/∂θ
i,k
t(v) ∀k ∈ {0, DO − 1}
θ
j
t+1(b)
= θ
j
t(b)
− ηt∂Lt/∂θ
j
t(b)
λt+1 = λt − ηt∂Lt/∂λt
(30b)
where the learning rate ηt is defined by (28b).
Similar to the BRL, the above BDL algorithm does not need
any additional exploration, as it is implicit in the computation
of an output f Ŵ(an, sn
t ). Proposition 5 shows that this BDL
algorithm defined by the sequence of iterations {Un
t }t∈N
in
(30a) and (30b), for Un
t = Un(an, sn
t |g(θt, εt)), converges to
the optimal value U
n
that represents the expected stage payoff
of the player from the optimal strategy αn.
Proposition 5: The BDL algorithm defined by the sequence
of iterations {Un
t }t∈N
in (30a) and (30b) converges to the
optimal value U
n
, for all n ∈ N, with probability one as time
tends to infinity.
The proof of Proposition 5 is provided in Appendix G, in
the supplementary material. From Proposition 5, we obtain
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
1988 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
Corollary 2 which shows that the BDL algorithm {Un
t }t∈N
con-
verges to a stable state where the pure strategies of the players
are in the MPBE of game �.
Corollary 2: With probability one as time tends to infin-
ity, the BDL algorithm defined by the sequence of iterations
{Un
t }t∈N
in (30a) and (30b), for all n ∈ N, converges to the
stable state in which the players’ pure strategies α are in the
MPBE of game �.
The proof of Corollary 2 is given in Appendix H, in
the supplementary material. The worst-case computational
complexity and convergence rate of the BDL algorithm are
established in Proposition 6.
Proposition 6: The BDL algorithm defined by the sequence
of iterations {Un
t }t∈N
in (30a) and (30b) has the worst-case time
complexity of O(M2KT), where K is the number of neurons
and T is the number of data points in the BNN model, and the
convergence rate equal to O(1/t1−[1/(2−λ)]), where λ ∈ [0, 1)
is the average dropout rate.
The proof of Proposition 6 is presented in Appendix I, in
the supplementary material. From Proposition 6, the proposed
BDL algorithm has a sublinear convergence rate and a poly-
nomial time complexity O(M2KT) defined by the number of
leaders/BSs M, number of neurons K, and number of data
points T .
VIII. GAME OF LEADERS IN THE STACKELBERG MODEL
A. Reinforcement Learning by the Leaders
Each leader/BSm in the Stackelberg model selects its action
am
t = (cm
t , lmt , zm
t ) ∈ Am represented by stage parameters of
BSm by solving an optimization problem in (13a). To solve this
problem directly, BSm should predict the task parameters and
2-D location (θn
t , lnt ), and the best response an
t = an(Bn
t , sn
t ) =
argmaxan∈An Vn(Bn
t , an|sn
t ) of every follower/peer Pn at any
stage t, which is a function of: 1) the peer’s belief Bn
t and 2) the
peer’s state sn
t = (θn
t , lnt , am
t , a−m
t , Qn
t ) depending on actions
a−m
t ∈ A−m of other BSs. Since the values of Bn
t and a−m
t are
unobservable by BSm, the direct estimation of best responses
at is not possible. Hence, (13a) is the stochastic optimization
problem defined by a random state st = (at, θt, lpt ) ∈ S which
represents the observable, but stochastic best responses at, task
parameters θt, and locations lpt of peers, as the leader observes
the values of st only after taking its action am
t , e.g., at the end
of stage t. As such, the problem in (13a) represents an MDP
defined by the observable random state s ∈ S with unknown
transition dynamics Pr{ś|s, am}. The MDP for this problem
can be defined as follows.
Definition 6: The MDP for the problem in (13a) is the tuple
(S,Am, Pr, um, γ ) defined by the following elements: 1) fully
observable state space S = A ×�×Lp equivalent to the set of
possible actions, task parameters, and 2-D locations of peers;
2) action space Am = Cm × Lm × Zm equivalent to the set of
possible stage costs, 3-D locations, and activity assignments
of BSm; 3) transition dynamics Pr{ś|s, am}, i.e., probability
of transiting to state ś ∈ S given action am ∈ Am taken by
BSm in state s ∈ S; 4) payoff um(am|ś) received by BSm from
action am when transiting to state ś ∈ S defined in (12a) and
5) discount rate γ ∈ (0, 1].
The RL problem is to find the optimal pure strategy αm(s)
∈ Am mapping from the state space S to action space Am,
which maximizes the value, i.e., the expected long-term payoff
of BSm, given by
Vm(am|s) = E
{ ∞∑
t=0
γ tum(am
t |st
)|s0 = s, am
0 = am
}
=
∑
ś∈S
Pr
{
ś|s, am}(um(am|ś)+ γ Vm(am|ś)) (31a)
and represents a solution of the Bellman optimality equation
V
m
(s) = V
m(
αm(s)|s)
= max
am∈Am
∑
ś∈S
Pr
{
ś|s, am}(um(am|ś)+ γ V
m(
am|ś)).
(31b)
The above problem can be solved by dynamic programming.
In this case, we obtain the RL algorithm where, at every
stage t, we store a set of |Am × S| updated values Vm
t (am|s)
indexed by action am ∈ Am and state s ∈ S. In the algorithm,
to estimate unknown transition dynamics Pr{ś|s, am}, we use
a counting variable μm
t (ś, s, am) to count the number of times
that each transition (ś, s, am) has occurred and approximate
Pr{ś|s, am} as
Pr
{
ś|s, am} ∼ P̂rt
{
ś|s, am} = μm
t
(
ś, s, am
)
∑
ś∈S μm
t
(
ś, s, am
) (32a)
for any am ∈ Am, ś, s ∈ S, where
μm
t
(
ś, s, am) = μm
t−1
(
ś, s, am)+ 1ś=st,s=st−1,am=am
t
. (32b)
The algorithm is as follows. At stage t = 0, we initialize
the values Vm
0 and the counting variables μm
0 = 1. At stage
= 0, . . . , Trd, we repeat three steps below.
1) Step 1: Based on the stored values Vm
t (am|s), find the
optimal pure strategy αm(s) for current state s = st−1
by solving the Bellman equation
αm(s) = argmax
am∈Am
Vm
t+1
(
am|s) (33a)
where Vm
t+1(a
m|s) is computed and stored for each action
am ∈ Am and state s ∈ S, as in
Vm
t+1
(
am|s) =
∑
ś∈S
Pr
{
ś|s, am}(um(am|ś)+ γ Vm
t
(
am|ś)).
(33b)
2) Step 2: Take optimal action am
t = αm(s) with probability
1−ε; another action am
t ∈ Am\αm(s) with probability ε.
3) Step 3: Observe the state ś = st resulting from action
am
t taken in state s = st−1 and update the transition
dynamics P̂rt{ś|s, am} according to (32a) and (32b).
The convergence of the proposed RL algorithm defined by
the sequence of iterations {Vm
t }t∈N
in (33a) and (33b) to the
optimal value V
m
, i.e., the value of optimal pure strategy αm
which represents a solution of the Bellman optimality equa-
tion in (31b) or, equivalently, a solution of the optimization
problem in (15a) is established in Proposition 7.
Proposition 7: The RL algorithm defined by the sequence of
iterations {Vm
t }t∈N
in (33a) and (33b) converges to the optimal
value V
m
, for all m ∈ M, with probability one as time tends
to infinity.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1989
The proof of Proposition 7 is provided in Appendix J, in the
supplementary material. The worst-case computational com-
plexity and convergence rate of the RL algorithm is established
in Proposition 8.
Proposition 8: The RL algorithm defined by the sequence of
iterations {Vm
t }t∈N
in (33a) and (33b) has the worst-case time
complexity equal to O(cN), for c = maxn∈N |An × �n × Ln|,
and the asymptotic convergence rate equal to O(1/t1−γ ), for
γ > 0.5 and O(
√
log(log t)/t), otherwise.
The proof of Proposition 8 is provided in Appendix K,
in the supplementary material. As such, we obtain the hier-
archical RL (HRL) procedure for the leaders/BSs and the
followers/peers in the Stackelberg model. In this procedure,
at the beginning of stage t, each leader/BSm realizes the
action am
t = argmaxam∈Am Vm
t+1(a
m|s) that maximizes its value
according to the RL algorithm. Given the leaders’ actions
at, each follower/peer Pn selects its best-response action
an
t = argmaxan∈An Vn
t+1(B
n
t , an|sn
t ) according to the BRL algo-
rithm. Consequently, the HRL is defined by the sequence
of iterations {{Vm
t }m∈M, {Vn
t }n∈N}t∈N
. The convergence of the
HRL to the corresponding optimal values {{Vm}m∈M, {Vn}n∈N}
follows from the convergence of the RL and BRL algorithms.
B. Deep Q-Learning by the Leaders
From Proposition 8, the proposed RL algorithm has a sub-
linear convergence rate and the complexity O(cN) exponential
in the number of followers/peers N. Thus, the RL algorithm is
intractable if the number N is large and, in order to reduce its
complexity, it is essential to reduce the size of the state space S
in the MDP model. In the following, we develop a polynomial-
time DQL algorithm which enables us to decrease state space
S by approximating the value Vm with an output f ω of the
NN model. The proposed DQL algorithm is unsupervised, i.e.,
it can be implemented “online” without a prior training or
pre-existing data sets.
In the DQL algorithm, we utilize a feedforward NN model
f ω(am, s) with deterministic weights ω to approximate the
value Vm(am|s) of action am ∈ Am taken by BSm in state
s ∈ S. A row input vector (am, s) ∈ R
1×DI of the model has
DI = 4N + 6 elements to define a current action am = am
t =
(cm
t , lmt , zm
t ) ∈ R
1×4 and state s = st−1 = (at−1, θt−1, lpt−1) ∈
R
1×4N of BSm which is observable at the beginning of stage
t. The output V̂m
t+1(a
m|s,ω) = f ω(am, s) ∈ R of the NN model
with K ≥ 1 neurons approximates the future value Vm
t+1(a
m|s).
Thus
Vm
t+1
(
am|s) ∼ V̂m
t+1
(
am|s,ω) = f ω
(
am, s
)
= ϕ
((
am, s
)
ω(h) + b
)
ωT
(v) (34a)
where the tuple ω = (ω(h),ω(v), b) ∈ R
(DI+2)×K comprises
weights ω(h) = (ω
i,j
(h)) ∈ R
DI×K and ω(v) = (ω
j
(v)) ∈ R
1×K in
the hidden and visible layers of the NN model, respectively,
and biases b = (bj) ∈ R
1×K . A pure strategy αm(ωt) ∈ Am
at stage t maps from the current weights ωt to action am
t In
particular, BSm selects a strategy αm(ωt) that maximizes its
approximated value, i.e.,
αm(ωt) = argmax
am∈Am
V̂m
t+1
(
am|s,ωt
)
. (34b)
As such, at any stage t, the approximation target represents
the future value Vm
t+1(a
m|s) ∈ R. This value can be updated at
the end of stage t, after the game transits from the current state
s = st−1 and action am = am
t to the next state ś = st resulting
in the payoff um(am|ś) based on the value iteration [48]–[51]
Vm
t+1
(
am|s) = Vm
t
(
am|s)+ 1ś=st,am=am
t
χt
× (
um(am|ś)+ γ Vm
t
(
am|ś)− Vm
t
(
am|s))
∀am ∈ Am, s = st−1. (34c)
The above iteration represents a weighted average of the old
value Vm
t (am|s) and future information, where the weight χt
can be updated similar to the learning rate ηt, as in (28b). The
DQL problem is to find the weights ω which minimize the
loss function, given by [39], [40], [48]–[51]
L(ω) = 1
2
E
{(
V
m − V̂m(ω)
)2
}
= 1
2
E
⎧
⎨
⎩
⎛
⎝ max
am∈Am
∑
ś∈S
Pr
{
ś|s, am}(um(am|ś)+ γ V
m(
am|ś))
−V̂m(αm(ω)|s, ω)
⎞
⎠
2
⎫
⎪⎬
⎪⎭
(35a)
where V
m = V
m
(s) = V
m
(αm(s)|s) is an optimal value, i.e.,
a solution of the Bellman optimality equation in (31b) and the
optimization problem in (13a).
This problem can be solved by stochastic gradient
descent [39], [40], [45]–[48]. That is, at any stage t, we
minimize the current loss Lt = L(ωt), given by
Lt = L(ωt) = 1
2
(
max
am∈Am
Vm
t+1
(
am|s)− V̂m
t+1
(
αm(ωt)|s,ωt
))2
.
(35b)
After differentiating the loss Lt with respect to current weights
ωt, we obtain the gradient
∇ωtLt =
(
V̂m
t+1
(
αm(ωt)|s,ωt
)− max
am∈Am
Vm
t+1
(
am|s)
)
× ∇ωt V̂
m
t+1
(
αm(ωt)|s,ωt
)
. (36a)
Then, the loss can be minimized by updating the weights in
the direction of negative gradient, i.e.,
ωt+1 = ωt − ηt∇ωtLt (36b)
where ηt is set according to (28b), until ωt converges.
Based on the above equation, we obtain the following DQL
algorithm where, at every stage t, we store the set of updated
weights ωt and values Vm
t (am|s) indexed by action am ∈ Am
and state s ∈ S. At stage t = 0, we initialize the weights ω0
and values Vm
0 . At stage = 0, . . . , Trd, we repeat the following
four steps.
1) Step 1: Given current state s = st−1 and weights
ω = ωt, estimate the output V̂m
t+1(a
m|s,ω) of the NN
and a strategy αm(ωt) according to (34a) and (34b),
respectively.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
1990 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
2) Step 2: With probability 1 − ε, take action am
t =
αm(ωt); with probability ε, take another action am
t ∈
Am\αm(ωt).
3) Step 3: Observe the next state ś = st and payoff
um(am
t |ś) and, based on the stored values Vm
t (am|ś),
compute and store the next approximation target
Vm
t+1(a
m|s)using (34c).
4) Step 4: Given Vm
t+1(a
m|s), update gradient ∇ωtLt
in (36a), and compute and store the weights ωt+1
using (36b).
The convergence of the proposed DQL algorithm repre-
sented by the sequence of iterations {V̂m
t }t∈N
in (34a)–(34c)
and (36a) and (36b), with V̂m
t = V̂m
t (am|s,ω), to the optimal
value V
m
is established in Proposition 9.
Proposition 9: The DQL algorithm defined by the sequence
of iterations {V̂m
t }t∈N
in (34a)–(34c) and (36a)–(36b) converges
to the optimal value V
m
, for all m ∈ M, with probability one
as time tends to infinity.
The proof of Proposition 9 is provided in Appendix L,
in the supplementary material. The worst-case computational
complexity and convergence rate of the BDL algorithm is
established in Proposition 10.
Proposition 10: The DQL algorithm represented by the
sequence of iterations {V̂m
t }t∈N
in (34a)–(34c) and (36a)
and (36b) has the worst-case time complexity of O(NKT|Am|),
where K and T are the numbers of neurons and data points
in the NN model, respectively, and the convergence rate of
O(1/t1−[1/(1+β)]), for β = Kω
(v)
max max{1, ω
(v)
max}/ε, where
ω
(v)
max > 0 is the maximal weight of a visible neuron and ε > 0
is some arbitrary small number.
The proof of Proposition 10 is presented in Appendix M, in
the supplementary material. From Proposition 10, the proposed
BDL algorithm has the sublinear rate of convergence and the
polynomial time complexity O(NKT|Am|) determined by the
number of followers/peers N, number of neurons K, number
of data points T , and the size of the action space |Am|.
Accordingly, we obtain a hierarchical deep learning (HDL)
procedure for the leaders/BSs and the followers/peers in the
Stackelberg model. In this procedure, at the beginning of
each stage t, every leader/BSm adopts the DQL algorithm
to decide on the action am
t = argmaxam∈Am V̂m
t+1(a
m|s,ωt)
which maximizes its value. Each follower/peer Pn selects the
best response action an
t = argmaxan∈An Un(an, sn
t |g(θt, εt))
with the BDL algorithm. As such, the HDL is defined by
the sequence of iterations {{V̂m
t }m∈M, {Un
t }n∈N}t∈N
. The con-
vergence of the HDL to the corresponding optimal values
{{Vm}m∈M, {Un}n∈N} follows from the convergence of the
DQL and BDL algorithms.
IX. PERFORMANCE EVALUATION
A simulation model of the BaaS-MEC system is developed
with OPNET package [52]. The MEC model is realized in the
long-term evolution advanced (LTE-A) time division duplex
(TDD) [53] network. The terrestrial BSs are represented by
MT = 3 LTE evolved NodeBs (eNBs): the macrocell eNB
labeled as BSN+1, microcell eNB labeled as BSN+2, and
TABLE II
DEFAULT PARAMETERS OF THE SIMULATION MODEL
femtocell eNB labeled as BSN+3. The aerial BSs are rep-
resented by MA = 3 UAVs labeled as BSN+4, BSN+5 and
BSN+6. Each eNB is connected to the blockchain server
via a 100-km long ITU-T G.657 brand B optical fiber of the
capacity 100 wavelengths. The default number of blockchain
peers is N = 10. Each peer represents a Laptop to serve
100 IoT devices. The IoT devices are represented by the
smoke, temperature, motion detector, and image sensors. The
system presumes intercell interference and operates in a typ-
ical urban environment. The MEC network service area is
co-located with the service area of a macrocell eNB. The
peers and small-cell eNBs are positioned evenly in the network
area. All system payments are counted in units of a digi-
tal currency, i.e., bitcoin. The main default parameters of the
simulation model are summarized in Table II. The parame-
ters of the LTE-A model, e.g., path loss, antenna gain, noise,
and shadowing, are set based on Third Generation Partnership
Project (3GPP) specifications [53]. UAV-related parameters
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1991
Fig. 3. Time dynamics of the average peer’s payoff.
are based on the common settings (listed, e.g., [15]–[17]).
The parameters related to the learning models are based on
the recommendations in [38].
In the following, we evaluate the performance of the HRL
and HDL procedures for the Stackelberg game by comparing
it with the performance of the following models.
1) Stackelberg game under complete information denoted
as “CI,” where each peer Pn knows current actions
a−n
t of other peers. That is, at any stage t, the peer
takes action an
t which maximizes payoff un(an, a−n
t |sn
t ).
Each BSm selects action am
t maximizing the payoff
um(am|at, θt, lpt ) by estimating the peers’ responses at
based on the known actions a−m
t of other BSs. As
such, the actions at of the BSs form the Nash equilib-
rium (NE); the peers’ actions at are the best responses
to the BSs’ actions. Note that as the CI scenario is infea-
sible in practical blockchains [15], we use it only to
benchmark the performance of our algorithms.
2) Simultaneous play [54] Stackelberg game denoted as
“SP,” where the BSs and peers select their actions simul-
taneously and independently. Any peer Pn can observe
the past actions a−n
t−1, . . . , a−n
0 of other peers, but does
not know their current actions a−n
t . Similarly, any BSm
can observe the past actions a−m
t−1, . . . , a−m
0 of other BSs,
but does not know their current actions a−n
t . As such,
the decision making of each BS and each peer is mod-
eled as an MDP with fully observable but stochastic state
spaces. Note that although this scenario is more realistic
than CI, it is still infeasible in practical IoT blockchains
because of the massive amount of information exchange
required in this case.
3) Stackelberg game played under incomplete information
with no learning denoted as “NL.” In NL, the assump-
tions are the same as in HRL and HDL, i.e., no peer
Pn and no BSm know the current and past actions
a−n
t , . . . , a−n
0 and a−m
t , . . . , a−m
0 of other peers and BSs,
respectively, but no learning is used. Instead, each peer
Pn and each BSm operate under a fixed initial belief
Bm
0 and about unobservable states. As a result, peer
Pn and BSm select actions an
t and am
t which maximize
their expected stage payoffs Un(Bn
0, an|sn
t ) and Um(am|s)
= ∑
ś∈S Bn
0(ś)u
m(am|ś). As such, the BSs’ actions at are
in the BNE; the peers’ actions at are best responses (with
respect to their initial beliefs) to the BSs’ actions.
Fig. 4. Time dynamics of the average BS’s payoff.
Fig. 5. Convergence time depending on the number of peers.
The time dynamics of the average payoffs of a peer and
a BS are shown in Figs. 3 and 4, respectively. Observe that
the average payoffs of a peers and a BS are stable in CI and
NL. The reason is that in CI and NL, the actions of peers
and BSs’ are in the equilibrium states. In particular, in CI,
the BSs’ actions form the NE; the peers’ actions are best
responses to BSs’ actions. On the other hand, in NL, the
BSs’ actions are in a BNE; the peers’ actions are the best
responses (with respect to their initial beliefs) to BSs’ actions.
The results also show that the payoffs of peers and BSs in
SP, HRL, and HDL increase consistently with time converg-
ing to the stable close-to-optimal levels after around 65, 70,
and 75 iterations, respectively. We observe that SP converges
faster than HRL and HDL. The results follow from the fact
that in SP, the stochastic state sn of peer Pn has an observ-
able state transition Pr{śn|sn, an}. On the contrary, in HRL and
HDL, in addition to state sn, there is also an unobservable state
a−n that is estimated based on the observable state transitions
by updating peers’ beliefs with the Bayes’ rule (in HRL) or
its BNN approximation (in HDL). This means that the peers’
beliefs (in HRL) and the weights of the BNN (in HDL) must
converge before convergence of the best responses. Thus, SP
converges faster than HRL and HDL. We also observe that
HRL converges faster than HDL. The reason is that in HDL,
loss functions are nonconvex with respect to weights of NN
or BNN. Thus, in general, a stochastic gradient descent goes
through several local minima prior to reach its global opti-
mum. In Fig. 5, convergence times in SP, HRL, and HDL are
shown as the functions of the number of peers N. Observe
that the convergence time increases with N, because when N
is large, it is difficult to accurately estimate the peers’ best
responses and the algorithms need more time to converge.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
1992 IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 3, MARCH 2020
Fig. 6. Average peer’s payoff depending on the number of peers.
Fig. 7. Average BS’s payoff depending on the number of peers.
Fig. 8. Average peer’s payoff depending on the transaction fee per task.
Figs. 6 and 7 show the average payoffs of a peer and a BS
in stable states as functions of the number of peers N. Observe
that the payoffs of peers decrease, whereas the payoffs of BSs
increase with N. The reason is that when the number of peers
is big, more tasks are recorded, forwarded, and offloaded in the
system. Hence, some tasks could experience delays exceeding
their completion deadlines, in which case the transaction fees
for the tasks are nullified leading to a reduced average payoff
of a peer. On the contrary, BSs receive payments for MEC
services provided to a task regardless of the task completion
deadlines. As a result, their payoffs increase when the number
of peers and, thus, the number of forwarded and offloaded
tasks in the system is big. Figs. 8 and 9 show the average
payoffs of a peer and a BS in the stable states as functions of
the task transaction fee. Observe that the payoffs of both the
peers and BSs increase with the transaction fee, but the growth
rates are different—exponential for peers and logarithmic for
BSs. The reason is that the peer’s payoff is affected directly
by the task transaction fee [see (8a)], while the BS’s payoff
is affected indirectly [see (12a)]. In particular, given the same
offloading costs, the peer prefers to process the task locally if
Fig. 9. Average BS’s payoff depending on the transaction fee per task.
the task transaction fee is small (as energy costs are lower than
offloading costs) and offload the task to a faster server if the
fee is large, in order to increase its payoffs in (8a). Thus, if the
task transaction fee is small, more tasks are processed locally
by peers leading to reduced BSs’ payoffs. On the other hand,
if the task transaction fee is high, more tasks are forwarded
and offloaded leading to increased BSs’ payoffs.
X. CONCLUSION
We have proposed a novel unsupervised hierarchical RL and
deep learning framework for a stochastic Stackelberg game
with multiple leaders under incomplete information. The game
models the interactions between the BSs as leaders and peers
as followers in the IoT system with BaaS-MEC support. We
have developed a hierarchical RL algorithm based on the MDP
and POMDP models of the decisions of BSs and peers. We
have formulated an HDL algorithm that combines: 1) DQL,
where the value of a BS is approximated by the NN and
2) BDL, where uncertainties about unobservable states of the
POMDP model for peers are modeled by the BNN. We have
shown that the proposed algorithms converge to the stable
states in which the peers’ actions are the best responses to
optimal actions of BSs.
REFERENCES
[1] J. Cao, D. Zhang, H. Zhou, and P.-J. Wan, “Guest editorial emerging
computing offloading for IoTs: Architectures, technologies, and applica-
tions,” IEEE Internet Things J., vol. 6, no. 3, pp. 3987–3993, Jun. 2019.
[2] S. Fu, Q. Fan, Y. Tang, H. Zhang, X. Jian, and X. Zeng, “Cooperative
computing in integrated blockchain based Internet of Things,” IEEE
Internet Things J., to be published.
[3] O. Novo, “Scalable access management in IoT using blockchain:
A performance evaluation,” IEEE Internet Things J., vol. 6, no. 3,
pp. 4694–4701, Jun. 2019.
[4] K. Yeow, A. Gani, R. W. Ahmad, J. J. P. C. Rodrigues, and K. Ko,
“Decentralized consensus for edge-centric Internet of Things: A review,
taxonomy, and research issues,” IEEE Access, vol. 6, pp. 1513–1524,
2018.
[5] W. Wang et al., “A survey on consensus mechanisms and mining
strategy management in blockchain networks,” IEEE Access, vol. 7,
pp. 22328–22370, 2019.
[6] J. Kang, R. Yu, X. Huang, S. Maharjan, Y. Zhang, and E. Hossain,
“Enabling localized peer-to-peer electricity trading among plug-in hybrid
electric vehicles using consortium blockchains,” IEEE Trans. Ind.
Informat., vol. 13, no. 6, pp. 3154–3164, Dec. 2017.
[7] J. Kang, Z. Xiong, D. Niyato, D. Ye, D. I. Kim, and J. Zhao, “Toward
secure blockchain-enabled Internet of vehicles: Optimizing consensus
management using reputation and contract theory,” IEEE Trans. Veh.
Technol., vol. 68, no. 3, pp. 2906–2920, Mar. 2019.
[8] N. Herbaut and N. Negru, “A model for collaborative blockchain-based
video delivery relying on advanced network services chains,” IEEE
Commun. Mag., vol. 55, no. 9, pp. 70–76, Sep. 2017.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: DISTRIBUTED DYNAMIC RESOURCE MANAGEMENT AND PRICING IN IoT SYSTEMS 1993
[9] W. Chen et al., “Cooperative and distributed computation offloading
for blockchain-empowered industrial Internet of Things,” IEEE Internet
Things J., vol. 6, no. 5, pp. 8433–8446, Oct. 2019.
[10] Amazon Blockchain . Accessed: Dec. 1, 2019. [Online]. Available:
https://docs.aws.amazon.com/managed-blockchain/?id=docs_gateway
[11] Ali-Baba Cloud BaaS. Accessed: Dec. 1, 2019. [Online]. Available:
https://www.alibabacloud.com/help/doc-detail/85263.htm?spm=
a2c63.l28256.a3.1.3abc14a4QOeVRL
[12] B. Omoniwa, R. Hussain, M. A. Javed, S. H. Bouk, and S. A. Malik,
“Fog/edge computing-based IoT (FECIoT): Architecture, applica-
tions, and research issues,” IEEE Internet Things J., vol. 6, no. 3,
pp. 4118–4149, Jun. 2019.
[13] R. Yang, F. R. Yu, P. Si, Z. Yang, and Y. Zhang, “Integrated blockchain
and edge computing systems: A survey, some research issues and chal-
lenges,” IEEE Commun. Surveys Tuts., vol. 21, no. 2, pp. 1508–1532,
2nd Quart., 2019.
[14] A. Asheralieva, “Optimal computational offloading and content caching
in wireless heterogeneous mobile edge computing systems with Hopfield
neural networks,” IEEE Trans. Emerg. Topics Comput. Intell., to be
published.
[15] A. Asheralieva and D. Niyato, “Hierarchical game-theoretic and
reinforcement learning framework for computational offloading in
UAV-enabled mobile edge computing networks with multiple service
providers,” IEEE Internet Things J., vol. 6, no. 5, pp. 8753–8769,
Oct. 2019.
[16] A. Asheralieva and D. Niyato, “Game theory and Lyapunov optimization
for cloud-based content delivery networks with device-to-device and
UAV-enabled caching,” IEEE Trans. Veh. Technol., vol. 68, no. 10,
pp. 10094–10110, Oct. 2019.
[17] M. Mozaffari, W. Saad, M. Bennis, Y.-H. Nam, and M. Debbah, “A
tutorial on UAVs for wireless networks: Applications, challenges, and
open problems,” arXiv preprint arXiv:1803.00680, Aug. 2018.
[18] T. Jin, X. Zhang, Y. Liu, and K. Lei, “BlockNDN: A bitcoin blockchain
decentralized system over named data networking,” in Proc. 9th Int.
Conf. Ubiquitous Future Netw. (ICUFN), Milan, Italy, Jul. 2017,
pp. 75–80.
[19] W. Wang, D. Niyato, P. Wang, and A. Leshem, “Decentralized caching
for content delivery based on blockchain: A game theoretic perspective,”
in Proc. IEEE Int. Conf. Commun. (ICC), Kansas City, MO, USA, May
2018, pp. 1–6.
[20] K. Kotobi and S. G. Bilén, “Blockchain-enabled spectrum access in
cognitive radio networks,” in Proc. Wireless Telecommun. Symp. (WTS),
Chicago, IL, USA, Apr. 2017, pp. 1–6.
[21] A. Lei, H. Cruickshank, Y. Cao, P. Asuquo, C. P. A. Ogah, and Z. Sun,
“Blockchain-based dynamic key management for heterogeneous intel-
ligent transportation systems,” IEEE Internet Things J., vol. 4, no. 6,
pp. 1832–1843, Dec. 2017.
[22] Z. Xiong, S. Feng, W. Wang, D. Niyato, P. Wang, and Z. Han,
“Cloud/fog computing resource management and pricing for blockchain
networks,” IEEE Internet Things J., vol. 6, no. 3, pp. 4585–4600, Jun.
2019.
[23] Z. Xiong, Y. Zhang, D. Niyato, P. Wang, and Z. Han, “When mobile
blockchain meets edge computing,” IEEE Commun. Mag., vol. 56, no. 8,
pp. 33–39, Aug. 2018.
[24] Y. Jiao, P. Wang, D. Niyato, and Z. Xiong, “Social welfare maximization
auction in edge computing resource allocation for mobile blockchain,”
in Proc. IEEE Int. Conf. Commun. (ICC), Kansas City, MO, USA,
May 2018, pp. 1–6.
[25] N. C. Luong, Z. Xiong, P. Wang, and D. Niyato, “Optimal auction for
edge computing resource management in mobile blockchain networks:
A deep learning approach,” in Proc. IEEE Int. Conf. Commun. (ICC),
Kansas City, MO, USA, May 2018, pp. 1–6.
[26] M. Liu, F. R. Yu, Y. Teng, V. C. M. Leung, and M. Song, “Distributed
resource allocation in blockchain-based video streaming systems with
mobile edge computing,” IEEE Trans. Wireless Commun., vol. 18, no. 1,
pp. 695–708, Jan. 2019.
[27] M. Liu, F. R. Yu, Y. Teng, V. C. M. Leung, and M. Song, “Computation
offloading and content caching in wireless blockchain networks with
mobile edge computing,” IEEE Trans. Veh. Technol., vol. 67, no. 11,
pp. 11008–11021, Nov. 2018.
[28] S. Nakamoto. (2008). Bitcoin: A Peer-to-Peer Electronic Cash System.
[Online]. Available: http://bitcoin.org/bitcoin.pdf
[29] S. Li, M. Yu, S. Avestimehr, S. Kannan, and P. Viswanath, “PolyShard:
Coded sharding achieves linearly scaling efficiency and security simul-
taneously,” arXiv preprint arXiv:1809.10361, Sep. 2018.
[30] L. Yang, J. Cao, Y. Yuan, Y. Li, A. Han, and A. Chan, “A framework for
partitioning and execution of data stream applications in mobile cloud
computing,” Perform. Eval. Rev., vol. 40, no. 4, pp. 23–32, Aug. 2013.
[31] A. Asheralieva, T. Q. S. Quek, and D. Niyato, “An asymmetric evolution-
ary Bayesian coalition formation game for distributed resource sharing
in a multi-cell device-to-device enabled cellular network,” IEEE Trans.
Wireless Commun., vol. 17, no. 6, pp. 3752–3767, Jun. 2018.
[32] A. Asheralieva, “Bayesian reinforcement learning-based coalition for-
mation for distributed resource sharing by device-to-device users in
heterogeneous cellular networks,” IEEE Trans. Wireless Commun.,
vol. 16, no. 8, pp. 5016–5032, Aug. 2017.
[33] M. J. Osborne and A. Rubenstein. A Course in Game Theory.
Cambridge, MA, USA: MIT Press, 1994.
[34] S. Sorin, “Stochastic games with incomplete information,” in Stochastic
Games and Applications. Dordrecht, The Netherlands: Springer, 2003,
pp. 375–395.
[35] D. Rosenberg, E. Solan, and N. Vieille, “Stochastic games with a single
controller and incomplete information,” SIAM J. Control Optim., vol. 43,
no. 1, pp. 86–110, 2004.
[36] M. Ghavamzadeh, S. Mannor, J. Pineau, and A. Tamar, “Bayesian rein-
forcement learning: A survey,” Found. Trends Mach. Learn., vol. 8,
nos. 5–6, pp. 359–483, 2016.
[37] P. Poupart, N. Vlassis, J. Hoey, and K. Regan, “An analytic solution
to discrete Bayesian reinforcement learning,” in Proc. ACM Int. Conf.
Mach. Learn.(ICML), Jun. 2006, pp. 697–704.
[38] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.
Cambridge, MA, USA: MIT Press, 1998.
[39] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Artificial neu-
ral networks-based machine learning for wireless networks: A tutorial,”
IEEE Commun. Surveys Tuts., vol. 24, no. 4, pp. 3039–3071, 4th Quart.,
2019.
[40] Y. Sun, M. Peng, and S. Mao, “Deep reinforcement learning-based mode
selection and resource management for green fog radio access networks,”
IEEE Internet Things J., vol. 6, no. 2, pp. 1960–1971, Apr. 2019.
[41] Y. Gal, “Uncertainty in deep learning,” Ph.D. dissertation, Dept. Eng.,
Univ. Cambridge, Cambridge, U.K., 2016.
[42] S. Depeweg, J. M. Hernández-Lobato, F. Doshi-Velez, and S. Udluft,
“Learning and policy search in stochastic dynamical systems with
Bayesian neural networks,” arXiv preprint arXiv:1605.07127, May 2016.
[43] A. Kendall and Y. Gal, “What uncertainties do we need in Bayesian
deep learning for computer vision?” in Proc. Adv. Neural Inf. Process.
Syst., 2017, pp. 5574–5584.
[44] M. D. Hofman, D. M. Blei, C. Wang, and J. Paisley, “Stochastic vari-
ational inference,” J. Mach. Learn. Res., vol. 14, pp. 1303–1347, May
2013.
[45] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, “Dropout: A simple way to prevent neural networks
from overfitting,” J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958,
2014.
[46] D. Krueger et al., “Zoneout: Regularizing RNNs by randomly preserving
hidden activations,” arXiv preprint arXiv:1606.01305, Jun. 2016.
[47] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger, “Deep networks
with stochastic depth,” in Proc. Eur. Conf. Comput. Vis., Oct. 2016, pp.
646–661.
[48] T. Hester et al., “Deep Q-learning from demonstrations,” in Proc. AAAI
Conf. Artif. Intell., 2018, pp. 3223–3230.
[49] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep
q-learning with model-based acceleration,” in Proc. ACM ICML,
Jun. 2011, pp. 2829–2838.
[50] J. Schmidhuber, “Deep learning in neural networks: An overview,”
Neural Netw., vol. 61, pp. 85–117, Jan. 2015.
[51] L. Deng and D. Yu, “Deep learning: Methods and applications,” Found.
Trends Signal Process., vol. 7, nos. 3–4, pp. 197–387, 2014.
[52] OPNET Simulation and Development Tool. Accessed: Dec. 1, 2019.
[Online]. Available: http://www.opnet.com
[53] Evolved Universal Terrestrial Radio Access (E-UTRA) and Evolved
Universal Terrestrial Radio Access Network (E-UTRAN); Overall
Description; Stage 2, Release 13, 3GPP Standard TS 36.300, 2016.
[54] T. Imai, “Essays in revealed preference theory and behavioral eco-
nomics,” Ph.D. dissertation, Humanities Soc. Sci., California Inst.
Technol., Pasadena, CA, USA, 2016.
[55] C. Szepesvári, “The asymptotic convergence-rate of Q-learning,” in
Proc. Adv. Neural Inf. Process. Syst., 1998, pp. 1064–1070.
[56] M. Hardt, B. Recht, and Y. Singer, “Train faster, generalize
better: Stability of stochastic gradient descent,” arXiv preprint
arXiv:1509.01240, Sep. 2015.
[57] X. Xu, Y. Zeng, Y. L. Guan, and R. Zhang, “Overcoming endurance
issue: UAV-enabled communications with proactive caching,” IEEE J.
Sel. Areas Commun., vol. 36, no. 6, pp. 1231–1244, Jun. 2018.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 07:41:38 UTC from IEEE Xplore.  Restrictions apply. 
<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles false
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.4
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /LeaveColorUnchanged
  /DoThumbnails false
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo false
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts false
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
    /Arial-Black
    /Arial-BoldItalicMT
    /Arial-BoldMT
    /Arial-ItalicMT
    /ArialMT
    /ArialNarrow
    /ArialNarrow-Bold
    /ArialNarrow-BoldItalic
    /ArialNarrow-Italic
    /ArialUnicodeMS
    /BookAntiqua
    /BookAntiqua-Bold
    /BookAntiqua-BoldItalic
    /BookAntiqua-Italic
    /BookmanOldStyle
    /BookmanOldStyle-Bold
    /BookmanOldStyle-BoldItalic
    /BookmanOldStyle-Italic
    /BookshelfSymbolSeven
    /Century
    /CenturyGothic
    /CenturyGothic-Bold
    /CenturyGothic-BoldItalic
    /CenturyGothic-Italic
    /CenturySchoolbook
    /CenturySchoolbook-Bold
    /CenturySchoolbook-BoldItalic
    /CenturySchoolbook-Italic
    /ComicSansMS
    /ComicSansMS-Bold
    /CourierNewPS-BoldItalicMT
    /CourierNewPS-BoldMT
    /CourierNewPS-ItalicMT
    /CourierNewPSMT
    /EstrangeloEdessa
    /FranklinGothic-Medium
    /FranklinGothic-MediumItalic
    /Garamond
    /Garamond-Bold
    /Garamond-Italic
    /Gautami
    /Georgia
    /Georgia-Bold
    /Georgia-BoldItalic
    /Georgia-Italic
    /Haettenschweiler
    /Helvetica
    /Helvetica-Bold
    /HelveticaBolditalic-BoldOblique
    /Helvetica-BoldOblique
    /Impact
    /Kartika
    /Latha
    /LetterGothicMT
    /LetterGothicMT-Bold
    /LetterGothicMT-BoldOblique
    /LetterGothicMT-Oblique
    /LucidaConsole
    /LucidaSans
    /LucidaSans-Demi
    /LucidaSans-DemiItalic
    /LucidaSans-Italic
    /LucidaSansUnicode
    /Mangal-Regular
    /MicrosoftSansSerif
    /MonotypeCorsiva
    /MSReferenceSansSerif
    /MSReferenceSpecialty
    /MVBoli
    /PalatinoLinotype-Bold
    /PalatinoLinotype-BoldItalic
    /PalatinoLinotype-Italic
    /PalatinoLinotype-Roman
    /Raavi
    /Shruti
    /Sylfaen
    /SymbolMT
    /Tahoma
    /Tahoma-Bold
    /Times-Bold
    /Times-BoldItalic
    /Times-Italic
    /TimesNewRomanMT-ExtraBold
    /TimesNewRomanPS-BoldItalicMT
    /TimesNewRomanPS-BoldMT
    /TimesNewRomanPS-ItalicMT
    /TimesNewRomanPSMT
    /Times-Roman
    /Trebuchet-BoldItalic
    /TrebuchetMS
    /TrebuchetMS-Bold
    /TrebuchetMS-Italic
    /Tunga-Regular
    /Verdana
    /Verdana-Bold
    /Verdana-BoldItalic
    /Verdana-Italic
    /Vrinda
    /Webdings
    /Wingdings2
    /Wingdings3
    /Wingdings-Regular
    /ZapfChanceryITCbyBT-MediumItal
    /ZWAdobeF
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 200
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages false
  /ColorImageDownsampleType /Average
  /ColorImageResolution 300
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 200
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages false
  /GrayImageDownsampleType /Average
  /GrayImageResolution 300
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 400
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages false
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False
  /CreateJDFFile false
  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Recommended"  settings for PDF Specification 4.01)
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice