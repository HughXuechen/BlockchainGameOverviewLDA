Distributed and Secure Federated Learning for Wireless Computing Power Networks
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
Distributed and Secure Federated Learning for
Wireless Computing Power Networks
Peng Wang, Student Member, IEEE, Wen Sun, Senior Member, IEEE, Haibin Zhang, Member, IEEE,
Wenqiang Ma, Student Member, IEEE, and Yan Zhang, Fellow, IEEE
Abstract—The explosively growth of mobile applications im-
poses much burden on the current computing networks. Wireless
Computing Power Network (WCPN), as an emerging computing
architecture, can sense and coordinate computing resources
through agile wireless communications, and realize distributed
intelligence based on federated learning. However, the mobility
and heterogeneity of WCPN nodes typically impact the security
(e.g., malicious node disturbance) and efficiency of federated
learning in WCPN. In light of this, this paper proposes a
provable secure and decentralized federated learning based on
blockchain for WCPN, where nodes can freely participate or
leave the WCPN federated training without authorization and
security threats. Particularly, we design a blockchain with proof-
of-accuracy (PoAcc) consensus scheme to deeply integrate with
the federated learning procedure, in which high-accuracy local
models have the priority of aggregation, thus accelerating the
convergence of federated learning and improving the efficiency
of WCPN. The proposed PoAcc is proved to be secure as long as
the ratio of honest to malicious nodes is above a lower bound. To
further meet the security requirement of PoAcc, we then propose
an evolutionary game-based incentive scheme that incentivizes
honest nodes to participate the WCPN federated learning under
malicious node disturbance. Numerical results show that the pro-
posed scheme ensures the consistency and security of federated
learning in WCPN, while outperforming the benchmarks in terms
of model accuracy and resource consumption.
Index Terms—Wireless Computing Power Network, Federated
Learning, Blockchain, Asynchronous Learning, Security of arti-
ficial intelligence.
I. INTRODUCTION
NOWADAYS, wireless services such as cloud-based ex-
tended reality, holographic communication and intelli-
gent interaction are usually data-intensive and computing-
intensive [1], posing unprecedented challenges to current
computing networks. To deliver real-time immersive user
experience, researchers have turned their attention to designing
new computing architectures. As a highly integrated intelligent
framework, Wireless Computing Power Network (WCPN)
supports to deliver intense and adaptive computing services
for consumers, through the deep integration and utilization
Copyright (c) 2015 IEEE. Personal use of this material is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending a request to pubs-permissions@ieee.org.
P. Wang and H. Zhang are with the School of Cyber Engineering, Xidian
University, No.2 South Taibai Road, Xi’an Shaanxi, 710071, China (e-mail:
pengw@stu.xidian.edu.cn, hbzhang@mail.xidian.edu.cn).
W. Sun and W. Ma are with the School of Cybersecurity, Northwestern
Polytechnical University, 127 West Youyi Road, Xi’an Shaanxi, 710072, China
(e-mail: sunwen@nwpu.edu.cn, mawenqiang520@mail.nwpu.edu.cn
Y. Zhang is with University of Oslo, Norway; and also with
Simula Metropolitan Center for Digital Engineering, Norway (e-mail:
yanzhang@ieee.org).
of reachable computing resources distributed over diverse
devices. Different from Mobile Edge Computing that relies
on the service capability of a single edge node, WCPN
can well cope with the load imbalance challenge through
the decomposition of computing tasks and the cooperation
between computing nodes [2], [3].
WCPN refers to a network, through the deep integration
of communication, computing, and networking over various
edge nodes, clouds, and terminal nodes, enables on-demand
and comprehensive computing services for users. WCPN can
be aware of the distributed computing power and resource
location through edge-to-edge interaction. Then, WCPN can
automatically find optimal resources and schedule services
to multiple cooperative computing nodes. With the help of
WCPN, consumers do not need to know the specific deploy-
ment of intelligence services, but only need to give service
requests, which can be distributed to the appropriate node for
service through agile wireless communication. Furthermore,
according to the requirements and complexity of requests,
WCPN can adaptively adjust the network topology to further
achieve a flexible balance between service requirements and
computing resources. It is foreseeable that the deep integration
of different entities in WCPN will form a distributed and
intelligent wireless network environment.
Federated learning has great potential to enable distributed
intelligence of WCPN. In WCPN, computing devices are
allowed to freely undertake intelligent tasks through federated
learning, resulting in the collaboration among distributed com-
puting power. Federated learning allows multiple computing
nodes in WCPN to build intelligent model collaboratively
without collecting their raw data, which guarantees the security
of computing nodes and the privacy of computing devices.
In addition, the WCPN implemented by federated learning
is robust and can still provide stable and reliable intelligent
services even if the network topology changes dynamically.
However, due to the openness nature of WCPN, computing
devices can join and exit without limitation. It is difficult to
orchestrate the entire process of federated learning in such
network dynamics. In addition, the aggregator of federated
learning may be vulnerable to a single point of failure.
Meanwhile, unreliable updates uploaded by malicious devices
such as data poisoning attacks severely disrupt the federated
learning process [4], users are reluctant to participate without
incentives and unwilling to cooperate due to the dishonest
partners [5], [6].
Blockchain, as a shared, immutable distributed ledger, pro-
vides a distributed security solution for federated learning
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
[7].The peer-to-peer distributed architecture and decentral-
ized nature of blockchain enable blockchain-based federated
learning to safely store training model in the blockchain
and update it, which avoids the single point of failure.
However, blockchain-based federated learning presents a set
of challenges that have so far been rarely studied. Firstly,
most existing blockchain-based federated learning schemes
lack strict mathematical analysis about the security of their
schemes. Second, clients in federated learning are often as-
sumed to be candidates rather than active participants, which
greatly weakens the openness of federated learning. In additon,
existing research on blockchain-based federated learning is
processed in a synchronous manner or a semi-asynchronous
method, where stragglers substantially retard the efficiency of
training. For instance, Lu et al. [8] proposed a blockchain
scheme for federated learning based on Delegated Proof of
Stakes protocol to enhance the security and reliability of the
global model. Korkmaz et al. [7] proposed a federated learning
system named ChainFL is proposed to alleviate the laggards’
drag through a combination of synchronous and asynchronous
training. Therefore, how to improve the training efficiency
while ensuring the security of the federated learning model
considering the openness of WCPN deserves further research.
In response to these challenges, we propose a provable
secure federated learning based on blockchain for WCPN. The
contributions of this paper can be summarized as follows.
1) We design a blockchain architecture for decentralized
federated learning in WCPN. The proposed scheme al-
lows participating devices to freely connect and leave the
blockchain for model training without any authorization.
In addition, we analyze and prove the security of the
scheme.
2) To improve the security and efficiency of federated learn-
ing in WCPN, we propose a proof-of-accuracy consensus
mechanism in which high-quality local models have the
priority of aggregation in a cost-efficient manner. We
prove a minimum requirement of the consensus mech-
anism on security.
3) To achieve the minimum requirement of the security of
federated learning in WCPN, we propose an evolutionary
game-based incentive scheme considering that each node
can make irrational decisions without prior knowledge.
Numerical results show that this scheme outper forms
other schemes in both accuracy and energy consumption.
The remainder of this paper is organized as below. The
related work is reviewed in the following section. The
blockchain-based asynchronous federated learning, its security
proof and threat model are described in Section III. Section
IV details and analyzes the security of blockchain-based
federated learning. Section V introduces and demonstrates an
evolutionary game-based incentive mechanism. In section VI,
we conducted a simulation. Finally, the conclusion is given in
Section VII.
II. RELATED WORK
In the 6G era, the proliferation of data poses unprecedented
challenges on the current computing networks. In order to
unleash the full potential of massive data and support compute-
intensive applications on industrial devices, researchers have
turned their attention to designing new computing architec-
tures. Azad et al. [9] studied the convergence of edge and
cloud computing to effectively meet the multi-level latency
needs of users. Through the integration of edge and cloud
computing, the proposed framework can maximize the com-
pletion of computing tasks with different latency requirements.
Wu et al. [10] proposed a hybrid task offloading framework
for heterogeneous clouds. The proposed method can efficiently
schedule eligible compute nodes to serve requested devices
and coordinate network resources (e.g., network cost, band-
width). Furthermore, Kr´ol et al. [11] proposed a computing
first network, which was a computing platform composed of
computing nodes, such as terminals, edge nodes, and cloud
nodes. The proposed framework greatly simplifies the underlay
of service, on which computing tasks can be dynamically
dispatched to the corresponding computing nodes. However,
these existing studies lack openness for available entities
providing computing power. For example, they do not have the
robustness to the dynamic join and exit of computing nodes,
and cannot ensure the security of computing services.
Federated learning as a promising solution for the collabo-
ration among distributed computing power has attracted more
and more attention. Federated learning can train distributed
models on devices and protect data privacy, which provide a
platform to sense and utilize reachable computing resources
[12]. Balasubramanian et al. [13] explored federated learning
in edge computing, which trains model using the data and
computational capabilities at end-user devices and provides
a high-level Quality of Service. Qu et al. [14] proposed a
framework called air-ground integrated federated learning to
provide ubiquitous intelligent services. Through integrating
air-ground networks and federated learning, all the nodes can
collaboratively train an effective learning model by FL in order
to empower edge intelligence. Yang et al. [15] proposed an
efficient federated learning scheme for wireless communica-
tion networks, which achieved energy efficient transmission
and computation resource allocation for each user participation
in federated learning. However, due to the unreliable factors
in the network, mobile devices may be offline. In addition,
the centralized architecture is vulnerable to a single point of
failure, and the server may encounter problems such as high
network bandwidth and communications overhead in a model
training.
To further address these issues, some studies investigate
semi-centralized federated learning, which normalizes the
communication and computing time differences by clustering
and layering mobile devices. Ghosh et al. [16] proposed the
Iterative Federated Clustering Algorithm, which alternately es-
timates the cluster identities of the users and optimizes model
parameters for the user clusters via gradient descent. Sattler
et al. [17] proposed a clustered federated learning framework,
which does not explicitly calculate the distance (similarity)
between participants, but determines which category each fed-
erated participant belongs to by evaluating the accuracy of the
local model improved by different clustering. To mitigate the
impact of stragglers on the convergence and training efficiency
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
Wireless computing  power layer
Applications layer
Manufacturing Smart city Smart home
ons layerrlayyyerr
M mart hoS i
Blockchain-based federated 
learning layer
Local model Local model Local model 
Updating Model
SupportSupport
g laaaaaaaaaaaaaayyyyyyer
puting  poooooooooooooooooooooooowwwwwwer layer
Dispatching 
computing power
Dispatching 
computing power
Fig. 1: Architecture of WCPN consists of computing power layer, blockchain-based federated learning layer, and application
layer.
of the central model, hierarchical classification of mobile
devices has become a potentially feasible solution. Zheng et
al. [18] proposed a novel distributed hierarchical tensor deep
computation model by condensing the model parameters from
a high-dimensional tensor space into a set of low-dimensional
subspaces to reduce the bandwidth consumption and improve
training efficiency. Chai et al. [19] proposed a layer-based
federated learning system, which divides participants into
layers based on training performance and selects participants
according to the level to which they belong, which improves
the convergence speed of federated learning in heterogeneous
scenarios. However, these efforts still rely on the clustering
center and the central server to collaborate to complete the
aggregation update of the model, which can lead to problems
such as single point of failure, performance bottleneck, and
privacy disclosure.
Several existing studies combine federated learning with
blockchain and design a decentralized model aggregation
method to overcome the problems caused by the traditional
centralized architecture. Kim et al. [20] proposed a decen-
tralized federated learning method applied to the device side
based on the blockchain framework with PoW, in which the
local gradient of each iteration is stored in the block after
verification and consensus, and the end-to-end delay and the
optimal block generation rate are analyzed. Lu et al. [21] used
the lightweight delegated proof of stake (DPoS) consensus
algorithm and developed a hybrid blockchain architecture
that consists of the permissioned blockchain and the local
Directed Acyclic Graph (DAG). However, the adoption of
DAG makes the consistency of the model uncontrollable.
Warnat-Herresthal et al. [22] introduced Swarm Learning,
a decentralized machine, learning approach that unites edge
computing, blockchain-based peer-to-peer networking and co-
ordination while maintaining confidentiality without the need
for a central coordinator, thereby going beyond federated
learning. However, this scheme predefines the permissions
of federal participants, and only participants with transaction
permissions can jointly train the federation model. In addition,
these consensus mechanism of above schemes either consumes
excessive computing resources or lacks mathematical analysis
in security guarantee of federated learning model.
Compared with the exist studies, the advantage of our
scheme is that it proposes an asynchronous federation learning
scheme based on blockchain, which ensures the security of
the model without predefining the participants’ permissions.
In addition, we also design a new consensus scheme, which
can reduce energy consumption for blockchain in maintain-
ing federated learning security, and demonstrate a minimum
requirement of the consensus mechanism on security. Finally,
based on the security analysis, we give an incentive mechanism
based on the evolutionary game to ensure that the minimum
requirement can always be satisfied.
III. SYSTEM MODEL
A. Architecture of WCPN
WCPN architecture, through the deep integration of commu-
nication, computing, and networking over various edge nodes,
clouds, and terminal nodes, enables on-demand and compre-
hensive computing services for intelligent services. Federated
learning enables WCPN to achieve collaboration among dis-
tributed computing power in a privacy-preserving manner. In
particular, federated learning supports to decompose intelligent
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
services to available computing nodes, and then leverages
their data and computing power to train machine learning
models for intelligent services. In WCPN, federated learning
can greatly reduce the communication overhead and further
improve resource utilization and user experience.
Fig. 1 shows a WCPN architecture consisting of comput-
ing power layer, blockchain-based federated learning layer,
and application layer. The computing power layer refers to
physical entities such as phones, unmanned vehicles, edge
servers and cloud servers, provides computing power and
personalized datasets for WCPN. Specifically, these devices
train local models for intelligent tasks of WCPN using their
own datasets, then interact with blockchain-based federated
learning layer to update the progress of intelligent tasks.
In the blockchain-based federated learning layer, federated
learning is used to integrate computing power over various
entities, enables comprehensive computing services for diverse
applications. Blockchain provides a traceable and verifiable
platform for model updates of intelligent tasks, ensuring the
openness and security of intelligence models in WCPN.
In particular, the blockchain is maintained by any devices
with sufficient computing and storage capacity such as edge
servers, and handle the interactions related to the model
update. The application layer includes diversified federated
learning-enabling applications in WCPN, such as smart cities,
autonomous vehicles, and industrial automation.
During the proposed blockchain-based federated learning,
any devices can connect to blockchain for the submission of
the local model, and then continue to train based on the newest
global model in blockchain without waiting for other devices.
The performance of the global model steadily improves with
each update of the model parameters. Thus, the proposed
WCPN guarantees some properties as follows:
• Decentralization: Though distributed blockchain plat-
form, the intelligence models in WCPN are protected
from a single point of failure issue and service paral-
ysis attacks. In addition, each device deployed with
blockchain can support to perform these intelligence
models, resulting in distributed services for consumers.
• Openness and Asynchronization: The proposed WCPN
allows devices providing computing power to freely con-
nect and leave the blockchain for model training without
prior approval and waiting for other devices.
• Provable Security: With the help of consensus mecha-
nism in blockchain, federated learning is not affected by
unreliable updates such as the data poisoning attack and
low-quality updates, guaranteeing the consistency and
security of the model.
B. Blockchain-based Federated Learning Process in WCPN
In the proposed federated learning, we use set N =
{1, 2, ..., N} to mobile devices and denote blockchain consists
of the following ordered blocks in the current time t as
B(t) = {B1, B2, ..., Bt}. The latest global model is stored
in the latest block of blockchain, all devices attempt to extend
blockchain with a new block including a new global update.
The federated learning process in Proof-of-Accuracy baesd
blockchain shown in Fig. 2.
• Step 1: Train locally intelligent model for WCPN.
These devices obtain the latest global model from the
blockchain node associated with the device, such as ωt
included the block Bt in t-th slot. Then the device n uses
its data to train the local model ωt
n, that is expressed as
ωt+1
n = ωt − λ▽ L(ωt
n), (1)
where λ > 0 is the learning step and ▽L(ωt
n) is the
gradient of the loss function. Then the local model ωt+1
n
is uploaded to blockchain to complete federal learning
aggregation.
• Step 2: Deliver Trained Model to WCPN. The
blockchain node updates the global model after receiving
local updates ωt
n from the device n, without waiting
for the other devices to finish. The global model update
process is defined as follows:
ωt+1 = (1− α(t))ωt + α(t)ωt+1
n , (2)
where α(t) is the function of staleness for adaptive
weight of the new model, which is related to the cur-
rent time t. After that, the blockchain node writes the
aggregated global model to its own new block generated
by blockchain consensus mechanism.
• Step 3: Reach a Consensus about Model over WCPN.
We term the consensus mechanism as Proof of Accuracy
(PoAcc), where each node wins the the global model
update with probability proportional its model accuracy
on test data. This replaces the compute-intensive mining
process of PoW, while ensuring the high-quality models
have the higher priority of aggregation and increasing
the barrier of attacks initiated by malicious devices. In
particular, each blockchain node participates in a leader
election with a winning probability proportional to the
accuracy of model received by the node. The winner of
this election has the right to propose a new block includ-
ing new global model. Thus, new blocks (or new global
models) can be effectively protected from adversely affect
of malicious devices. Mathematically, the leader election
is to find a valid puzzle solution nonce that make the
new block Bt+1 satisfy following hash inequality:
H(Bt+1) < ρ
(
ϕ(wk
n)− ϕ(wt)
)
, (3)
where ρ is a constant, and ϕ(wk
n) represents the accuracy
of model ωk
n on test set stored in the blockchain, wt is the
latest global model contained in block Bt. After finding
the new block, the new block would be broadcasted to
the network and accepted by the remaining blockchain
nodes. Meanwhile, the blockchain fork choice rule of our
protocol remains the longest chain. That is mean when
there are multiple versions of blockchain in the system,
all honest nodes will continue to train on the longest
blockchain.
• Step 4: Download Latest Model from WCPN. The
device n gets the latest global model from the associated
blockchain node, and repeats these steps until the global
loss function converge, that is
ω∗ = argmin
ω
L(ω), (4)
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
Blockchain
Computing
nodes
= 1 ++
Blockchain
Train locally intelligent model 
for WCPN
Deliver Trained Model to WCPN
Reach a Consensus about Model 
over WCPN
Download Latest Model from WCPN
Consensus
Fig. 2: Operations of Blockchain-based Federated Learning in
WCPN, where devices providing computing power to freely
connect and leave the blockchain for model training without
prior approval and waiting for other devices.
where L(ω) is the global loss function.
C. Security Analysis for Federated Learning in WCPN
The ability to resist global model tampering and distributed
model poisoning is critical for the security and stability of
federated learning. In blockchain-based asynchronous feder-
ated learning, the above attacks are implemented through
publishing a private chain locally forged by malicious nodes,
which is longer than the longest public chain. In this case the
malicious model on the private chain would be accepted by all
nodes in place of the existing honest global model. In order to
analyze the security of blockchain-based asynchronous feder-
ated learning, we define three stochastic processes, including
honest chain process, adversarial tree process and longest
chain process to capture all possible changes in blockchain
federated learning over time. We denote the rate of generating
blocks for adversary nodes and honest nodes on a given block
as λa, λh. Thus, we can define three random processes as
follows:
• Honesty chain process denoted by C(t) reflects the
resources owned by honest nodes to maintain the security
of blockchain. It is a chain growth process participating
by honest nodes, where the block growth rate is λh. λh
represents the rate of generating blocks for honest nodes
on a given block. Intuitively, due to the longest chain
protocol in blockchain, honest nodes always propose on
the lasted block of the longest chain.
• Adversary Tree Process denoted by A(t) reflects the
resources the adversary has at its disposal to attack at
time t. It is a random tree process generated by adversary
nodes. Because of malicious nodes can attack from any
TABLE I: New Main Symbols
Notation Definition
N = {1, 2, ..., N} The set of computing nodes
ωk The global model in k-th slot
▽L(ωk
n) The gradient of the loss function
B(t) = {B1, ..., Bi} Blockchain at time t
H() Hash function
(SKm, PKm) Signing-Verication key pair
ϕ(wk
n) Accuracy of the model on the test set
C(t) Honesty chain process
A(t) Adversary tree process
Eij Block i Security event
Ei Blockchain security event
φ(σ) (log-)Laplace transform of the point process
L(∗) Length of the blockchain
v Lable for every block in adversary tree
tv The appearance time of lable v
P () Probability of event occurrence
Θ Random event
G Evolutionary game
K = {K1,K2} Populations
x = [x1, x2]⊤ Population distribution of populations
Ui Payoff for every node
block, there is independent Poisson processes of rate λa
at each block. We use Ai(t) to express the adversarial
tree consisting of all blocks generated by the adversary
on i-th honest block from ti time to t time, where ti is
the generation time of block Bi. Thus, the random tree
process generated by the adversary is denoted by
A(t) = {A0(t),A1(t), ...,Ai(t)}. (5)
• Longest Chain Process denote as B(t), which increases
in the following two ways: 1) an additional honest block
mined by honest nodes is added to B(t); 2) it receives
a blockchain longer than the current longest chain from
A(t) of an adversary.
Note that different from the longest chain, the honesty chain
and adversary tree is fictitious to facilitate analysis. Based on
the above processes, we define block security event as follows,
Eij =
{
1, if L(Aj(t)) < L(C(t)− L(C(tj)), for t > ti,
0, other situations,
(6)
where L()is a function that measures the depth of the tree.
Eij = 1 means that the length of the adversary tree built on
block Bj before block Bi cannot catch up with the honest
chain. In short, adversary nodes do not have enough resources
for block Bj to tamper with the global model on block Bi.
Furthermore, we define the chain security event as follows,
Ei =
i−1∏
j=0
Eij . (7)
If Ei = 1, then the blockchain B(ti) = {B1, B2, ..., Bi} is
safe against any adversary, since there is not enough resources
to let any previous adversary tree attack the chain.
IV. SAFETY ANALYSIS FOR BLOCKCHAIN-BASED
FEDERATED LEARNING IN WCPN
In this section, we first prove the equivalence between the
proposed “block security event” and the non-tamperability of
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
Fig. 3: The adversarial tree in terms of a Branching Random
Walk
the federated learning model. we further analyze the correla-
tion between block security event and adversary capabilities
to demonstrate a minimum requirement of the proposed con-
sensus mechanism on security.
First, the equivalence between “block security event” and
the non tamperability of the federated learning model is
difficult to directly analyze. We simplify this process by using
the following inequality,
L
(
C(t)− L(C(ti)
)
< L
(
B(t)− L(B(ti)
)
. (8)
This inequality means the length of the longest blockchain is
greater than the number of blocks produced by honest nodes,
because the longest chain will increase if the adversary forges
a longer blockchain. Based on Eqn. (7) and Eqn. (8), we
further propose and prove the following lemma,
Theorem 4.1: For blockchain B(ti), if Ei = 1, then B(ti)
will be a prefix of any future longest chain. Equivalently, ωk
n
in Bi will not be tampered with by any adversary.
Proof 1: We suppose Ei = 1, and the minimum generation
time of the longest chain which do not include the prefix B(ti)
is tmin. Let Bh is the last block generated by honest nodes
on B(tmin). If ti < th, then the th−1 is a shorter time than
tmin, and B(ti) is not a prefix of B(th−1). This is because the
arrival of a honest block does not replace other blocks on the
longest chain according to longest chain process. Thus, the Bh
must be generated before time ti. In the case, all blocks during
th to tmin must be generated by adversary nodes. Hence, the
following inequality holds,
L
(
Ah(tmin)
)
≥ L
(
B(tmin)− L(B(th)
)
.
However, Ei = 1 means the following inequality holds,
L(Ah(tmin)) < L(C(tmin)− L(C(th)).
This is a contradiction.
Next, we analyze the growth rate of the adversary tree,
because “block security event” depends on whether the ad-
versary tree can catch up with the honesty chain. First, we
descries the adversarial tree in terms of a Branching Random
Walk (BRW) [23]. As show as Fig. 2, a Genesis Block is
located at the origin of random walk. Then, each block in
the each generation produces its own children according to
the exponential random variables with parameter λa indepen-
dently. We use a label v = (i1, .., ik−1, ik) to represent every
block of the adversary tree, where ik represents v is the ik-th
child of vk−1 = (i1, .., ik−1). ξv is the exponential random
variable that represents the interval between the reproduction
of v by the ancestor nodes of vk−1. Then, inf
|v|=k
tv represents
the appearance time of the first block in the k-th generation,
where tv denote the appearance time of the block v and |v|
denotes the generation (blockchain length) of v.
Lemma 4.1: The appearance time of the first block in the
k-th generation has a linear asymptotic velocity,that is,
lim
k→∞
1
k
inf
|v|=k
tv =
1
eλa
. (9)
Proof 2: According to [23], inf
|v|=k
tv is the minimum of
a standard BRW, and then we introduce the (log-)Laplace
transform of the point process.
φ(σ) = lnE(
∑
v:|v|=1
e−σtv ) = ln
∞∑
j=1
E(e
∑j
i=1 σξ(i))
= ln
∞∑
j=1
(E(eσξ(1)))j = − ln(
λa
σ
), σ ∈ R.
(10)
where ξv is an exponential random variable with parameter λa,
indicating the time interval when vk−1 generates v. Based on
Theorem 1.3 in [23], when φ(σ) < ∞ for some σ > 0, we can
get the following equation surely on the set of non-extinction,
lim
k→∞
1
k
inf
|v|=k
tv = − inf
σ>0
φ(σ)
σ
(11)
By solving the first derivative on the right side of the equation,
we can get
∂
(
inf
σ>0
φ(σ)
σ
)
∂(σ) =
1−ln(λa
σ )
σ2 . Obviously, the key point
is σ = eλa. Combining Eqn. (11) and Eqn. (10), we have
lim
k→∞
1
k
inf
|v|=k
tv = inf
σ>0
ln(λa
σ )
σ
=
1
eλa
.
Based on lemma 4.1, we can further get the following lemma,
its reasoning process is shown in the appendix.
Lemma 4.2: The probability that the length of adversary
tree is greater than λat within t time satisfies the following
inequality, when eλat+ δ is an integer:
P (L((A(t)) ≥ eλat+ δ) ≤ e−δ. (12)
After getting the relationship between the length of the
adversary tree and time, we can prove that block security event
always occurs when certain conditions are met, That is, the
following theorem.
Theorem 4.2: If eλa ≤ λh, then Ei has a non-zero
probability of occurrence, that is P (Ei = 1) > ϵ for all i,
where ϵ is a strictly positive constant. This implies that the
event Ei occurs for infinitely many i with probability 1.
Proof 3: We assume there are k blocks generated in time
t, and use Rm = tm − tm−1 represent the generation time of
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
adjacent blocks. Combining with Eqn. (7) and Eqn. (13), we
have as follows:
P (Eij = 1) ≤ P
(
L
(
Aj(t)
)
≤ L
(
B(t)− L(B(tj)
))
, for t > ti
= P
(
L
(
Aj(
k−1∑
m=j
Rm)
)
< k − j − 1
)
, for k > i.
(13)
Let’s set a constant θ, and define two random events as follows:
Eij = Event L(Aj(
k−1∑
m=j
Rm)) ≥ k − j − 1, for t > ti.
Θ = Event Rm <
1
θ
, for m = 0, 1, ..., θ − 1.
Then, we can deduce the following inequality
P (Ei = 1) ≥ P (Ei = 1|Θ)P (Θ)
= P (
i−1∏
j=0
Eij = 1|Θ)P (Θ)
≥ (1−
∑
0<j<i<k
(P (Eij |Θ)))P (Θ).
(14)
Consider three cases:
First, for 0 < j < k < θ,
P (Eij |Θ) = P
(
L(Aj(
k−1∑
m=j
Rm)) ≥ k − j − 1|Θ
)
≤ P
(
L
(
Aj(
k − j
θ
)
)
≥ θ − 1
)
≤ P
(
L
(
Aj(1)
)
≥ θ − 1
)
Based on Lemma 4.2,∑
0<j<k<θ
P (Eij |Gθ) ≤
∑
0<j<k<θ
eλa−1−θ, (15)
where θ → ∞,
∑
0<j<k<θ P (Eij |Θ) → 0.
Second, for 0 < j < θ < k
P (Eij |Θ) ≤ P
(
Eij |Θ, 1 +
k−1∑
m=θ
Rm) < (k − j − 1)
1 + λae
2λae
)
+ P
(
1 +
k−1∑
m=θ
Rm ≥ (k − j − 1)
1 + λae
2λae
)
Based on Lemma 4.2, we can get∑
0<j<θ<k
P (Eij |Θ) ≤ e(k−j−1)λae−1
2 +A1e
−α(k−i−1), (16)
where it is obvious that λae < 1, thus θ → ∞,∑
0<j<θ<k P (Eij |Θ) → 0.
Third, 0 < θ < j < k, similar to previous case, we can get∑
0<θ<j<k
P (Eij |Θ) ≤ e(k−j−1)λae−1
2 +A2e
−α(k−i−1), (17)
where θ → ∞,
∑
0<j<θ<k P (Eij |Θ) → 0.
Combining with Eqn(18), Eqn.(19), Eqn(20), and Eqn(21),
if θ sufficiently large, P (Ei) ≥ P (Θ) > 0. This implies that
the event Ei occurs infinitely often.
V. EVOLUTIONARY INCENTIVE MECHANISM FOR SECURE
FEDERATED LEARNING IN WCPN
After security analysis, we introduce evolutionary game the-
ory to address the issue that how to incentive honest nodes to
ensure security under malicious node bribing. The evolutionary
game does not require participants to be completely rational,
nor does it require the limitation of complete information. It
reveals players in the game with bounded rationality can reach
equilibrium through repeated games. Because of the above
characteristics, we construct an evolutionary game including
an adversary population and an honest population, then give
an incentive strategy to ensure that the security in Theorem
4.2 can always be satisfied.
The evolutionary game model generally consists of four
parts: player set, population, strategy set and payoff func-
tion. Players are the set N participating in blockchain-based
federated learning. We denote K = {K1,K2} as different
populations, where K1 is honest populations where nodes are
regarded as honest nodes, and K2 is adversary populations
composed of adversary nodes. For strategy set, we denote
x = [x1, x2]
⊤ as the vector of population fraction for pop-
ulations. Because each node is bounded rational, it chooses
which population to join to maximize its utility. Furthermore,
a node’s expected payoff in population Mi can be expressed
as:
Ui(x,λ, Ri) =
λixi∑M
j=1 λjxj
Ri
Nxi
− βλi, (18)
where λ = [λ1, λ2]
⊤ represent the block generation rate that
the populations require participating devices to provide at
least. Ri is the reward for each global aggregation, and β
denote the energy cost of device for maintaining a unit block
generation rate.
As we defined before, the evolutionary game for pop-
ulation selection can be model as a 4-tuple G =
{N ,M,x, {Ui(x,λ, Ri)}i∈M}. In order to describe the long-
term behaviour of the dynamics, we use imitation dynamics
to yield replicator equation as follows,
xi(t)
′ = xi(t)(Ui(x,λ, Ri)−
|K|∑
j=1
Uj(x,λ, Rj)xj) (19)
where xi(t)
′ is the growth rate of the size of population Mi
over time. Thus, the replicator equation can be used to estimate
the portion of migration among honest nodes and adversary
nodes to reach an evolutionarily stable strategy (ESS).
Definition 1: A population state x∗ is an ESS of G, if there
exists a neighborhood B ∈ X , for ∀x ∈ B−x∗, the condition
(x − x∗)
T
U(x∗) = 0 implies that (x∗ − x)
T
U(x) ≥ 0.
This definition means no other minority can invade and
confuse the original resident. Thus, ESS can describe the
long-term dynamics behaviors of adversary population and an
honest population. In this state, according to [24], a player
has no other strategy that is more profitable than the currently
selected one assuming that the other players’ strategies are
fixed.
Lemma 5.1: Consider the game G with two populations
modeled by the payoff function Eqn. (18). When N is suffi-
ciently large, the evolutionary game exists a unique ESS, that
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
is the rest point x∗ = λ1R1−λ2R2
Nβ(λ1−λ2)2
− λ2
λ1−λ2
, if the following
conditions hold:{
R1λ1 −R2λ2 < 0,
(R1 −R2)(λ2 − λ1) > 0.
(20)
Proof 4: First, we need to find the Nash Equilibrium (NE)
of our proposed evolutionary game. According to [24], the
NE is a set of rest points of the replicator equation, that is
xi(t)
′ = 0. Combining with Eqn. (19), we can obtain three
rest points of the replicator equation
x∗ = {0, 1, λ1R1 − λ2R2
Nβ(λ1 − λ2)2
− λ2
λ1 − λ2
}.
Then, we need to further investigate the stability of NE.
According to [10], if the Jacobian matrix
J =
[
J11 J12
J21 J22
]
=
[
∂x1(t)
∂x1
∂x1(t)
∂x2
∂x2(t)
∂x1
∂x2(t)
∂x2
]
is negative definite at the equilibrium, then the replicator
dynamics is asymptotically stable.
∂x1(t)
∂x1
=(1− 2x1)
( λ1R1
N(λ1x1 + λ2x2)
− βλ1
)
+ βλ2x2
− R2λ
2
2x
2
2
N(λ1x1 + λ2x2)2
− R1λ
2
1(x1 − x2
1)
N(λ1x1 + λ2x2)2
,
∂x1(t)
∂x2
=βλ2x1 −
R1λ1λ2(1− x1)x1 −R2λ
2
2x1x2
N(λ1x1 + λ2x2)2
− R2λ2x1
N(λ1x1 + λ2x2)
,
∂x2(t)
∂x1
=βλ1x2 +
R1λ
2
1x1x2 −R2λ1λ2(1− x2)x2
N(λ1x1 + λ2x2)2
− R2λ2x1
N(λ1x1 + λ2x2)
,
∂x2(t)
∂x2
=(1− 2x2)
( λ2R2
N(λ1x1 + λ2x2)
− βλ2
)
+ βλ1x1
− R1λ
2
1x
2
1
N(λ1x1 + λ2x2)2
− R2λ
2
2(x2 − x2
2)
N(λ1x1 + λ2x2)2
.
Based on above equation, we consider the following cases:
1) if x∗ = 0 is ESS, then the following two conditions need
satisfy:
det(J11) =
R1λ1 −R2λ2
Nλ2
− β(λ1 − λ2) < 0,
det(J) =
(R1λ1 −R2λ2
Nλ2
− β(λ1 − λ2)
)
(βλ2 −
R2
N
) > 0.
When N is sufficiently large, the above two inequalities are
contradictory. Thus, the rest point with x∗ = 0 is not an ESS.
Similarly, we can also conclude that the rest point with x∗ = 1
is also not an ESS.
2) if x∗ = λ1R1−λ2R2
β(λ1−λ2)2
− λ2
λ1−λ2
is ESS, then the following
two conditions satisfy:
det(J11)=
A
(
R1λ1(λ1−λ2)−2R2λ1λ2+Nβλ2
1(λ1−λ2)
)
N(R1λ1 −R2λ2)(λ1 − λ2)2
<0,
det(J)=
βAλ1λ2(R1−R2)(R1λ1−R2λ2+Nβλ1(λ2−λ1))
N(R1λ1−R2λ2)(λ1−λ2)2
<0,
where A = λ1 − λ2 + Nβλ2(λ2 − λ1). Based on these two
inequalities, if N → ∞ and the Jacobian matrix is negative
definite, we can get
lim
N→∞
det(J11) = lim
N→∞
Nβ2λ1λ2
R1λ1 −R2λ2
< 0
lim
N→∞
det(J) = lim
N→∞
Nβ3λ2
1λ
2
2(R1 −R2)(λ2 − λ1)
(R1λ1 −R2λ2)2
> 0
According to these inequality, the rest point with x∗ =
λ1R1−λ2R2
β(λ1−λ2)2
− λ2
λ1−λ2
is a ESS if the conditions R1λ1−R2λ2 < 0
and (R1 −R2)(λ2 − λ1) > 0 holds.
Next, we analyze what strategy (R1) the honest population
adopts to obtain a stable ESS that meets the safety factor (λh
λa
)
in Lemma 4. Obviously, λh and λa in section III respectively
are equal to the total block generation rate of the honest pool
and the total block generation rate of the adversary pool. Based
on Lemma 4 and Lemma 5, we can get all inequalities to solve
the R1 that satisfies the condition:
R1λ1 −R2λ2 < 0,
(R1 −R2)(λ2 − λ1) > 0,
0 < x∗ = λ1R1−λ2R2
Nβ(λ1−λ2)2
− λ2
λ1−λ2
< 1,
λh
λa
= λ1Nx1
λ2Nx2
= λ1x
∗
λ2(1−x∗) > e.
(21)
After some standard mathematical manipulations (see Ap-
pendix for detailed steps), we can get the lemma as follows:
Theorem 5.1: The minimum requirement of security in
Theorem 4.2 can be satisfied, if the reward value of honest
population satisfies the following inequality.
max
{
(λ2Nβ(λ1−λ2)+λ2R2)
λ1
,
B(1+e)+eλ2
2R2+λ1λ2R2
λ2
1−eλ1λ2
}
< R1 < min
{
R2,
λ1Nβ(λ1 − λ2) + λ2R2
λ1
}
,
where B=λ1λ2Nβ(λ1−λ2). It is noted that the safety factor
increases with the increase of the R1.
The evolutionary population selection game process based
on Pairwise Proportional Imitation Protocol is as shown in
algorithm 1. Honest population set its reward R1 and the
required block generation rate λ1 meeting Eqn. (5.1) according
to the predetermined strategy of the adversary population (that
is R2, λ2). Then, the nodes can choose with a P j
i probability
a population for higher revenue, which reveals the bounded
rationality of the nodes in the repetitive and dynamic game.
Based on the imitation protocol, we can predict the population
dynamics in federated learning. After a period of time, the
evolutionary stable strategy of block generation rate dynamics
of the two populations must meet the Lemma 4.
VI. NUMERICAL RESULTS
In this section, we first evaluate the performance of the pro-
posed blockchain-based asynchronous free federated learning
compared with the three federated learning schemes. Then, we
test the robustness and energy consumption of the proposed
scheme under malicious node attacks.
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
Algorithm 1: Population Selection Game
Input: R2, λ2
Output: R1, λ1
1 For honest population, set R1, λ1 meets:
2 R1 = max
{
(λ2Nβ(λ1−λ2)+λ2R2)
λ1
,
B(1+e)+eλ2
2R2+λ1λ2R2
λ2
1−eλ1λ2
}
3 ∀i ∈ N , node i randomly selects a mining pool to start
with.
4 while x has not converged do
5 foreach ∀i ∈ N do
6 Determine whether to switch to pool Kj ∈ K
according to the switching probability:
7 P j
i = xj max
(
Uj(x,λ, Rj)− Ui(x,λ, Ri), 0
)
8 end
9 end
10 return x that meets λae < λh.
Fig. 4: Accuracy varying with number of gradients in different
schemes.
A. Setup
Simulation settings: We simulate a blockchain-based asyn-
chronous federated learning in a WCPN consisting of 20 com-
puting nodes (each node configured with 6 CPU cores, 32GB
memory, and running Ubuntu 18.04 system). For blockchain,
the maximum size of each block is 2 Mb, and for ease of calcu-
lation, the total delay required for block propagation between
devices is limited to 1 ms. in addition, we randomly selected
5000 pictures of classification data set MNIST for each node
in the MNIST dataset as the node dataset. Specifically, we
use TensorFlow to perform Multilayer Perceptron (MLP) to
TABLE II: Comparison of model accuracy in different
schemes under attacking of malicious nodes.
Malicious
Nodes
Scheme (Number of gradients = 10000)
FedAvg AsynFL BAFL SBAFL
0% 0.907800 0.910732 0.913441 0.932921
10% 0.901874 0.900269 0.902976 0.932092
20% 0.881499 0.878323 0.878070 0.934139
30% 0.831825 0.819501 0.836616 0.934906
40% 0.773249 0.705999 0.805475 0.932627
perform the learning tasks based on data set MNIST in each
node, in which a gradient descent method with a learning rate
of 0.01 is adopted. The network structure is 256 × 64× 10,
in which first two layer is activated by a ReLU function and
the third layer uses tanh as the activation function. In order to
explore the impact of malicious nodes for the proposed scheme
in this paper, we respectively select 2, 4, 6 and 8 nodes from
20 nodes as malicious nodes. For the data sets of malicious
nodes, we exchange the labels of the training sets to launch a
poison attack.
Comparative Methods: We compare the proposed SBAFL
with the Synchronous Federated Learning (FedAvg) and Asyn-
chronous Federated Learning (AsynFL), Blockchain-based
Asynchronous Federated Learning (BAFL) and Centralized
learning (BaseLine). In particular, FedAvg proposed in [25]
represents a classical federated learning where uses a weighted
average to update the server model in every global syn-
chronous aggregation. In AsynFL [26], the aggregator im-
mediately uses staleness coefficient updates the global model
whenever it receives a local model. BAFL [27] adopts PoW-
based blockchain to achieve an asynchronous model update,
which uses a long-term trust index considering the training
time, training sample size and so on to update the global
model. BaseLine represents a machine learning combining the
data of all clients and processed on a single machine, which is
to reflect the upper limit of the accuracy of the neural network
on the training data set.
B. Results
Figure 4 depicts model accuracy varying with number of
gradients under five schemes. The number of gradients refers
to the number of gradient updates applied to the global model,
which fairly measures the performance difference among five
schemes. Note that the BaseLine scheme is a centralized
learning combining the data of all clients. The Baseline
scheme reflects the upper limit of the accuracy of the structural
neural network model on the data set. As can be seen, model
accuracy of all schemes begins to increase as the amount of
gradients number continues to grow. However, other schemes
are significantly lower than baseline because of distributed
training of federated learning. However, it is noted that our
scheme SBAFL is closest to the performance of the baseline
scheme. This is because PoAcc consensus mechanism gives
high-performance models more probability to participate in
global aggregation, rather than egalitarianism (Fedavg) or only
computing power correlation (AsynFL and BAFL).
Figure 5 depicts the model accuracy over varying number
of gradients under different proportion of malicious nodes in
different schemes. As can be seen from Fig. 5(a) - Fig. 5(d),
regardless of the proportion of malicious nodes, the model
accuracy in SBAFL alway increases steadily with the increase
of gradients number and keeps the highest value among four
schemes. However, the proportion of malicious nodes is high
as shown in Fig. 5(c) and Fig. 5(d), the model accuracy in
AsynFL and BAFL keeps fluctuating with the increase of
gradients number. This is because malicious nodes upload
their models frequently in AsynFL and BAFL, which makes
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
(a) 10% malicious nodes (b) 20% malicious nodes
(c) 30% malicious nodes (d) 40% malicious nodes
Fig. 5: Accuracy varying with number of gradients under malicious nodes attacking in different schemes.
the global model unable to converge well. PoAcc in SBAFL
reduces the probability of malicious models participating in
aggregation. Meanwhile, it is noted that all other schemes
except SBAFL have significant decline in model accuracy
under the same gradient number with the increase of the
proportion of malicious nodes. This shows the effectiveness
of our scheme.
Table II depicts the average model accuracy over the varying
proportion of malicious nodes under gradients number be
10000 among four schemes. Note that model accuracy in
Fedavg, AsynFL, and BAFL begin to declined significantly
as the proportion of malicious nodes increases. Among them,
BAFL has better model accuracy because of its proposed
weighted method considers the device’s data size, local update
correlation, global model cheating times, and past scores for
the global aggregation. However, its effect is limited, because
malicious devices can use new identities to upload malicious
parameters every time in our scenario. In contrast, SBAFL can
maintaine the model accuracy at about 93%. This is because
PoAcc makes the probability of malicious nodes participating
in global aggregation much lower than that of honest nodes,
because the accuracy of the local model trained by malicious
nodes is lower than that of honest nodes.
To evaluate the performance of the proposed PoAcc con-
sensus, we compare the energy consumption over the varying
proportion of malicious nodes among four schemes when
model accuracy reaches 85%. As shown in Figure 6, when the
proportion of malicious nodes is 0%, the energy consumption
of BAFL is the highest, while the energy consumption of the
other three schemes is generally the same. This is because
PoW consensus of BAFL requires a lot of additional com-
puting resources. In addition, with the increase of proportion
of malicious nodes, the energy consumption also increases in
four schemes. However, the increased energy consumption of
AsynFL and BAFL is remarkable, that of FedAvg increased
linearly, while that of SBAFL increased slightly. This is
because the asynchronous aggregation of AsynFL and BAFL
gives malicious nodes more opportunities to upload malicious
models, thus interfering with federated learning and wasting a
lot of resources. Meanwhile, PoAcc in SBAFL considers the
process of calculating a high-performance local model as part
of the proof of working compared with PoW in BAFL. This
reduces the computing energy consumption, while alleviating
poisoning attacks for malicious nodes.
Figure 7 depicts the ratio of block generate rate between
adversary nodes and honest nodes varying with the round
number of the evolutionary game under different rewards.
As we have seen, with the progress of the evolutionary
game, these evolutionarily stable strategies (ESS) reached by
different R1 satisfying Theorem 5.1 can meet the requirements
of the minimum requirement of security in Theorem 4.2.
This reflects the decision-making process of bounded rational
nodes and proves the effectiveness of our calculated ESSs.
In addition, it is obvious that the higher the reward given
by the honest population, the higher the ratio of the block
generation rate. This means that under the condition of high
return, more nodes are more inclined to contribute their models
and computing power to the honest population.
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
Fig. 6: Accuracy varying with energy consumption under
malicious nodes attacking between BAFL and SBAFL.
Fig. 7: Ratio of block generate rate between adversary nodes
and honest nodes varying with round number of evolutionary
game.
VII. CONCLUSION
In this paper, we proposed a provable secure blockchain-
based federated learning framework for WCPN. The proposed
scheme used a PoAcc consensus mechanism to ensure the
security and learning efficiency of federated learning where
the participating devices did not need any authorization regis-
tration and could perform the federated learning in an asyn-
chronous and decentralized manner. Furthermore, the proposed
PoAcc was proved to be secure as long as the ratio of
honest to malicious nodes was above a lower bound, and we
proposed an incentive mechanism based on the evolutionary
game to ensure that the minimum requirement can always be
satisfied. Numerical results showed that the proposed scheme
outperformed the existing work (such as FedAvg, AsynFL,
and BAFL) in learning accuracy, convergence rate, and energy
saving. In the future, we aim to overcome the storage overhead
and scalability limitations of current blockchain-based feder-
ated learning by sharding technology. Moreover, we strive to
eliminate the impact of the heterogeneity of the device on the
accuracy of federated learning.
VIII. ACKNOWLEDGMENT
This work was supported by the National Natural Sci-
ence Foundation of China under Grants 62272391, in part
by the Key Industry Innovation Chain of Shaanxi un-
der Grant 2021ZDLGY05-08, and in part by the De-
fense Industrial Technology Development Program un-
der Grant JCKY2020204B027, JCKY2021110B143, and
JCKY2021608B001.
APPENDIX
A. Proof of Lemma 4.2
Note that the following two events are equivalent: the length
of the adversary tree in t time is less than or equal k, that is
equivalent with the appearance time of the first block in the
k-th generation of the adversary tree is greater than or equal
to t. Thus, the equation is as follows.
{L((A(t)) ≤ k} = { inf
|v|=k
tv ≥ t}, (22)
According to Eqn. (22), set m = eλa + δ, we have the
following equation
P
(
L(A(t) ≥ m)
)
= P ( inf
|v|=m
tv ≥ t). (23)
Obviously, the following inequality holds in the adversary tree,
P ( inf
|v|=m
tv ≥ t) ≤
∑
v,|v|=m
P (tv ≤ t). (24)
for any v, set Z(v) =
∑|v|
j=1 ij . Obviously, v has the same
law as
∑Z(v)
j=1 ξj . Thus, by Chebycheff’s inequality, for any
v, |v| = m, we have
P (tv ≤ t) = P (e−eλatv ≥ e−eλat) ≤ E(e−eλatv )
e−eλat
= (
λa
λa +−eλa
)Z(v)eeλat = (
1
1 + e
)Z(v)eeλat.
Note that ∑
v,|v|=m
P (tv ≤ t) =
∑
v,|v|=m
(
1
1 + e
)Z(v)eeλat
= (
∞∑
i>1
(
1
1 + e
)i)meeλat
= e−meeλat = e−δ.
(25)
Combining Eqn.(23), Eqn.(24) and Eqn. (25), we can yield
Eqn. (12) .
B. Proof of Theorem 5.1
According to the definition of x∗ (proportion of population),
its value range should be greater than or equal to 0 and less
than or equal to 1. Based on Lemma 5.1, we can get 0 and 1 are
not the ESS of constructed evolutionary game when N is large
enough, and any x∗ that satisfies must satisfy R1λ1−R2λ2 <
0 and (R1 −R2)(λ2 − λ1) > 0. Thus, we have
0 < x∗ =
λ1R1 − λ2R2
Nβ(λ1 − λ2)2
− λ2
λ1 − λ2
< 1 (A.1)
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
If x∗ = λ1R1−λ2R2
Nβ(λ1−λ2)2
− λ2
λ1−λ2
> 0 holds, then we can derive the
inequality λ1−λ2 < 0 must be hold. It is because that R1λ1−
R2λ2 < 0 according to Lemma 5.1 and Nβ(λ1 − λ2)
2 > 0
must be hold. Combining with (R1 − R2)(λ2 − λ1) > 0 in
Lemma 5.1, we can get the following inequality holds
R1 −R2 < 0. (A.2)
Furthermore, we can solve the inequality in Eqn. (A.1) to get
the following results
(λ2Nβ(λ1−λ2)+λ2R2)
λ1
< R1 <
λ1Nβ(λ1 − λ2) + λ2R2
λ1
.
(A.3)
Finally, the ESS x∗ should also meet the minimum require-
ment of security in Theorem 4.2, that is λ1x
∗
λ2(1−x∗) > e. After
deduction, we can get
λ1λ2Nβ(λ1−λ2)(1+e)+eλ2
2R2+λ1λ2R2
λ2
1−eλ1λ2
< R1. (A.4)
Thus, considering (A.2), (A.3) and (A.4), R1 satisfied the
inequality:
max
{
(λ2Nβ(λ1−λ2)+λ2R2)
λ1
,
B(1+e)+eλ2
2R2+λ1λ2R2
λ2
1−eλ1λ2
}
< R1 < min
{
R2,
λ1Nβ(λ1 − λ2) + λ2R2
λ1
}
,
where B=λ1λ2Nβ(λ1−λ2).
REFERENCES
[1] Y. Lu and X. Zheng, “6g: A survey on technologies, scenarios,
challenges, and the related issues,” Journal of Industrial Information
Integration, vol. 19, p. 100158, 2020.
[2] W. Sun, S. Lei, L. Wang, Z. Liu, and Y. Zhang, “Adaptive federated
learning and digital twin for industrial internet of things,” IEEE Trans-
actions on Industrial Informatics, vol. 17, no. 8, pp. 5605–5614, 2020.
[3] Y. Qu, C. Dong, J. Zheng, H. Dai, F. Wu, S. Guo, and A. Anpala-
gan, “Empowering edge intelligence by air-ground integrated federated
learning,” IEEE Network, vol. 35, no. 5, pp. 34–41, 2021.
[4] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning
attacks against federated learning systems,” in European Symposium on
Research in Computer Security. Springer, 2020, pp. 480–501.
[5] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in
Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security, 2015, pp. 1310–1321.
[6] G. Heigold, V. Vanhoucke, A. Senior, P. Nguyen, M. Ranzato, M. Devin,
and J. Dean, “Multilingual acoustic models using distributed deep neural
networks,” in 2013 IEEE international conference on acoustics, speech
and signal processing. IEEE, 2013, pp. 8619–8623.
[7] C. Korkmaz, H. E. Kocas, A. Uysal, A. Masry, O. Ozkasap, and
B. Akgun, “Chain fl: decentralized federated machine learning via
blockchain,” in 2020 Second international conference on blockchain
computing and applications (BCCA). IEEE, 2020, pp. 140–146.
[8] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, “Low-latency
federated learning and blockchain for edge association in digital twin
empowered 6g networks,” IEEE Transactions on Industrial Informatics,
vol. 17, no. 7, pp. 5098–5107, 2021.
[9] M. W. Al Azad, S. Shannigrahi, N. Stergiou, F. R. Ortega, and
S. Mastorakis, “Cledge: A hybrid cloud-edge computing framework
over information centric networking,” in 2021 IEEE 46th Conference
on Local Computer Networks (LCN), 2021, pp. 589–596.
[10] H. Wu, Z. Zhang, C. Guan, K. Wolter, and M. Xu, “Collaborate edge and
cloud computing with distributed deep learning for smart city internet of
things,” IEEE Internet of Things Journal, vol. 7, no. 9, pp. 8099–8110,
2020.
[11] M. Król, S. Mastorakis, D. Oran, and D. Kutscher, “Compute first
networking: Distributed computing meets icn,” in Proceedings of the 6th
ACM Conference on Information-Centric Networking, 2019, pp. 67–77.
[12] Y. Xiao, G. Shi, and M. Krunz, “Towards ubiquitous AI in 6g
with federated learning,” CoRR, vol. abs/2004.13563, 2020. [Online].
Available: https://arxiv.org/abs/2004.13563
[13] V. Balasubramanian, M. Aloqaily, M. Reisslein, and A. Scaglione,
“Intelligent resource management at the edge for ubiquitous iot: An
sdn-based federated learning approach,” IEEE Network, vol. 35, no. 5,
pp. 114–121, 2021.
[14] Y. Qu, C. Dong, J. Zheng, H. Dai, F. Wu, S. Guo, and A. Anpala-
gan, “Empowering edge intelligence by air-ground integrated federated
learning,” IEEE Network, vol. 35, no. 5, pp. 34–41, 2021.
[15] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, “Energy
efficient federated learning over wireless communication networks,”
IEEE Transactions on Wireless Communications, vol. 20, no. 3, pp.
1935–1949, 2021.
[16] A. Ghosh, J. Chung, D. Yin, and K. Ramchandran, “An Efficient
Framework for Clustered Federated Learning,” arXiv e-prints, p.
arXiv:2006.04088, 2020.
[17] F. Sattler, K.-R. Müller, and W. Samek, “Clustered federated learning:
Model-agnostic distributed multitask optimization under privacy con-
straints,” IEEE Transactions on Neural Networks and Learning Systems,
vol. 32, no. 8, pp. 3710– 3722, 2021.
[18] H. Zheng, M. Gao, Z. Chen, and X. Feng, “A distributed hierarchical
deep computation model for federated learning in edge computing,”
IEEE Transactions on Industrial Informatics, vol. 17, no. 12, pp. 7946–
7956, 2021.
[19] Z. Chai, A. Ali, S. Zawad, S. Truex, A. Anwar, N. Baracaldo,
Y. Zhou, H. Ludwig, F. Yan, and Y. Cheng, “Tifl: A tier-based
federated learning system,” CoRR, vol. abs/2001.09249, 2020. [Online].
Available: https://arxiv.org/abs/2001.09249
[20] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, vol. 24, no. 6, pp.
1279–1283, 2020.
[21] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, “Blockchain
empowered asynchronous federated learning for secure data sharing
in internet of vehicles,” IEEE Transactions on Vehicular Technology,
vol. 69, no. 4, pp. 4298–4311, 2020.
[22] S. Warnat-Herresthal, H. Schultze, K. Shastry, S. Manamohan,
S. Mukherjee, V. Garg, R. Sarveswara, K. Händler, P. Pickkers, N. A.
Aziz, S. Ktena, F. Tran, M. Bitzer, S. Ossowski, N. Casadei, C. Herr,
D. Petersheim, U. Behrends, F. Kern, and T. Velavan, “Swarm learning
for decentralized and confidential clinical machine learning,” Nature,
vol. 594, no. 7862, pp. 265–270, 2021.
[23] Z. Shi, Branching random walks. Springer, 2015.
[24] X. Liu, W. Wang, D. Niyato, N. Zhao, and P. Wang, “Evolutionary
game for mining pool selection in blockchain networks,” IEEE Wireless
Communications Letters, vol. 7, no. 5, pp. 760–763, 2018.
[25] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in Artificial intelligence and statistics. PMLR, 2017, pp. 1273–
1282.
[26] C. Xie, S. Koyejo, and I. Gupta, “Asynchronous federated optimization,”
arXiv preprint arXiv:1903.03934, 2019.
[27] L. Feng, Y. Zhao, S. Guo, X. Qiu, W. Li, and P. Yu, “Bafl: A blockchain-
based asynchronous federated learning framework,” IEEE Transactions
on Computers, vol. 71, no. 5, pp. 1092–1103, 2022.
Peng Wang (Student Member, IEEE) received the
B.Eng. degree in software engineering from Harbin
Institute of Technology, Harbin, China in 2017. He is
currently working toward the Ph.D. degree in cyber
engineering with Xidian University, Xi’an, China.
His research interests include blockchain, access
control, Internet of things.
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply. 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
Wen Sun (Senior Member, IEEE) is currently a
full professor with the School of Cybersecurity,
Northwestern Polytechnical University. She received
her Ph.D. degree in Electrical and Computer En-
gineering from National University of Singapore
in 2014, and B.E. degree from Harbin Institute of
Technology in 2009. Her research interests cover
a wide range of areas including wireless mobile
communications, IoT, 5G, and blockchain. She has
published more than 50 peer-reviewed papers in
various prestigious IEEE journals and conferences,
including IEEE Transactions on Industrial Informatics, IEEE Transactions on
Wireless Communications, IEEE Network, IEEE Wireless Communications.
She was the recipient of the best paper award of GlobeCom2019. She serves
as publicity chair for WiMob2019 and CNS2020, and TPC member for ICC
and GlobeCom in 2018 and 2019.
Haibin Zhang (Member, IEEE) received his B.S.
degree in applied mathematics from Ocean Univer-
sity of China in 2003, and Ph.D. degrees in computer
science and technology from Xidian University in
2007. He is currently a professor with the School
of Cyber Security, Xidian University. His research
interests concentrate on formal verification, artificial
intelligence, IoT. He has published more than 30
peer-reviewed papers in various prestigious journals
and conferences, including IEEE Internet of Things
Journal and IEEE Systems Journal.
Wenqiang Ma (Graduate Student Member, IEEE)
received the B.E. degree from the School of
Computer, Northwestern Polytechnical University
in 2021. He is currently pursuing a M.S. in the
School of Cybersecurity, Northwestern Polytechni-
cal University. His research interests include feder-
ated learning, mobile edge computing, reinforcement
learning.
Yan Zhang (Fellow, IEEE) received the Ph.D. de-
gree from the School of Electrical and Electron-
ics Engineering, Nanyang Technological University,
Singapore, in 2005. He is currently a Full Professor
with the Department of Informatics, University of
Oslo, Oslo, Norway. His research interests include
nextgeneration wireless networks leading to 6G, and
green and secure cyber-physical systems, such as
smart grid and transport. He is a fellow of IET and an
Elected Member of the Academia Europaea (MAE),
the Royal Norwegian Society of Sciences and Letters
(DKNVS), and the Norwegian Academy of Technological Sciences (NTVA).
In 2018, he was a recipient of the global Highly Cited Researcher Award
(Web of Science Top 1% most cited worldwide). He is also the Sympo-
sium/Track Chair of a number of conferences, including IEEE ICC 2021,
IEEE SmartGridComm 2021, and IEEE GLOBECOM 2017. He is also the
Chair of the IEEE Communications Society Technical Committee on Green
Communications and Computing (TCGCC). He is also an Editor (an Area
Editor, a Senior Editor, or an Associate Editor) for several IEEE transac-
tions/magazines, including IEEE Network Magazine, IEEE TRANSACTIONS
ON NETWORK SCIENCE AND ENGINEERING, IEEE TRANSACTIONS
ON VEHICULAR TECHNOLOGY, IEEE TRANSACTIONS ON INDUS-
TRIAL INFORMATICS, IEEE TRANSACTIONS ON GREEN COMMUNI-
CATIONS AND NETWORKING, IEEE COMMUNICATIONS SURVEYS &
TUTORIALS, IEEE INTERNET OF THINGS JOURNAL, IEEE SYSTEMS
JOURNAL, IEEE Vehicular Technology Magazine, and IEEE BLOCKCHAIN
TECHNICAL BRIEFS. He was also a Distinguished Lecturer of the IEEE
Vehicular Technology Society from 2016 to 2020. He is also a Distinguished
Lecturer of the IEEE Communications Society and a Distinguished Speaker
of the IEEE Vehicular Technology Society
This article has been accepted for publication in IEEE Transactions on Vehicular Technology. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TVT.2023.3247859
© 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:15:03 UTC from IEEE Xplore.  Restrictions apply.