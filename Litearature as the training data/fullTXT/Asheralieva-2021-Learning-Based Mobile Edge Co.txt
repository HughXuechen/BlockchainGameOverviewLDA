Learning-Based Mobile Edge Computing Resource Management to Support Public Blockchain Networks
Learning-Based Mobile Edge Computing
Resource Management to Support
Public Blockchain Networks
Alia Asheralieva and Dusit Niyato , Fellow, IEEE
Abstract—Weconsider a public blockchain realized in themobile edge computing (MEC) network, where the blockchainminers compete
against each other to solve the proof-of-work puzzle and win amining reward. Due to limited computing capabilities of their mobile
terminals, miners offload computations to theMEC servers. TheMEC servers aremaintained by the service provider (SP) that sells its
computing resources to theminers. The SP aims atmaximizing its long-term profit subject tominers’ budget constraints. Theminers
decide on their hash rates, i.e., computing powers, simultaneously and independently, tomaximize their payoffs without revealing their
decisions to other miners. As such, the interactions between the SP andminers aremodeled as a stochastic Stackelberg game under
private information, where the SP assigns the price per unit hash rate, andminers select their actions, i.e., hash rate decisions, without
observing actions of other miners.We develop a hierarchical learning framework for this game based on fully- and partially-observable
Markov decisionmodels of the decision processes of the SP andminers.We show that the proposed learning algorithms converge to
stable states in whichminers’ actions are the best responses to the optimal price assigned by the SP.
Index Terms—Blockchain, deep learning, game theory, incomplete information, Markov decision process, mining, mobile edge computing,
partially-observable Markov decision process, reinforcement learning, resource management
Ç
1 INTRODUCTION
DEVELOPED originally as an accounting method for the
cryptocurrency Bitcoin [1], blockchains have been
adopted in numerous applications, e.g., FinTech, and as a
backbone of the distributed open-access virtual machines
(VMs) for decentralized token-driven resource management
in autonomous communication networks and cyber-physical
systems [2], [3], [4]. Blockchain is a distributed ledger technol-
ogy (DLT) that enables to store data with in-build robustness,
since it cannot be controlled by a single entity and able to
avoid a single point of failure. Public blockchains are also
characterized by their transparency, disintermediation and
open access. In a blockchain, data is collected in the form of
blocks, e.g., records of transactions which form a linked list
data structure to preserve logical relations in the appended
information. The blocks are copied and distributed across the
entire blockchain network comprising all blockchain users.
This provides improved data integrity and security when
compared to the centralized ledger approaches [2], [3]. The
process by which the transactions are confirmed and added
to the blockchain is referred to as a mining [1]. During
mining, a blockchain user, i.e., a miner, must solve a compu-
tationally intensive proof-of-work (PoW) puzzle and broad-
cast its solution to other users. The transaction is verified, and
a new block is added to the current blockchain only if the
respective solution reaches a consensus. Most consensus pro-
tocols are based onNakamoto consensus [4], [5] that provides
a certain reward to the first miner successfully solving the
PoW puzzle. The probability of winning a reward is propor-
tional to the miner’s relative hash rate, i.e., computing power
[1], [2], [3], [4], [5]. This leads to the competition among min-
ers and incentivizes themining process.
Unfortunately, although blockchains are already utilized
in several distributed system scenarios, e.g., content delivery
networks [6] and smart-grids [7], they are not yet widely
adopted in the Internet of Things (IoT) and other mobile sys-
tems. The main reason is that the IoT devices and users’
mobile terminals have limited computing capabilities. Hence,
they cannot run blockchain applications directly and inde-
pendently. To address this issue, edge computing has been
proposed as a promising solution to facilitate future IoT and
mobile blockchain applications [8], [9]. InMECnetworks, vir-
tual cloud computing capabilities are extended at the net-
work “edges” throughMEC servers attached to base stations
(BSs) in proximity to IoT and mobile terminals [10], [11]. As
such, the devices running blockchain applications can offload
their PoW puzzle computations to nearest BSs equipped
withMEC servers. Compared to traditional cloud computing
where computations are offloaded to centralized clouds
located in the network core (far from end-users) resulting in
the high propagation delays and heavy backhaul load, MEC
allows to reduce the network congestion and backhaul traffic,
� A. Asheralieva is with the Department of Computer Science and Engineering,
Southern University of Science and Technology, 1088 Xueyuan Avenue,
NanshanDistrict, Shenzhen, Guangdong 518055, China.
E-mail: aasheralieva@gmail.com.
� D. Niyato is with the School of Computer Science and Engineering,
Nanyang Technological University, 50 Nanyang Ave, 639798, Singapore.
E-mail: dniyato@ntu.edu.sg.
Manuscript received 3 Oct. 2018; revised 6 Oct. 2019; accepted 10 Dec. 2019.
Date of publication 16 Dec. 2019; date of current version 3 Feb. 2021.
(Corresponding author: Alia Asheralieva.)
Digital Object Identifier no. 10.1109/TMC.2019.2959772
1092 IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 20, NO. 3, MARCH 2021
1536-1233� 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
https://orcid.org/0000-0002-4430-5928
https://orcid.org/0000-0002-4430-5928
https://orcid.org/0000-0002-4430-5928
https://orcid.org/0000-0002-4430-5928
https://orcid.org/0000-0002-4430-5928
https://orcid.org/0000-0002-7442-7416
https://orcid.org/0000-0002-7442-7416
https://orcid.org/0000-0002-7442-7416
https://orcid.org/0000-0002-7442-7416
https://orcid.org/0000-0002-7442-7416
mailto:aasheralieva@gmail.com
mailto:dniyato@ntu.edu.sg
and also decreases latency and power consumption of IoT
and mobile terminals. All advantages of integrating block-
chainwithMEC systems are detailed in [12].
In this paper, we consider a public blockchain application
implemented in the MEC network, where the set of block-
chain users or miners offload their PoW puzzle com-puta-
tions to the associated BSs operated by theMEC SP. In such a
scenario, the cost of computing power in the MEC network
becomes a critical issue. In particular, the SP assigns the price
per unit of hash rate to maximize its long-term payoff from
the MEC services while taking into account the constraints
on miners’ budgets, i.e., the highest price that miners can
afford to pay during the entire mining process. In response
to the assigned price, every miner decides on its hash rate
to maximize its long-term payoff defined as a difference
between the miner’s long-term mining reward and the total
cost of computing power payable to the SP.
Two critical issues must be addressed when modeling
the interactions between the SP and the miners:
– First, in the blockchain network, miners make their
hash rate decisions simultaneously and indepen-
dently [1], [13]. Besides, as miners compete against
each other to earn the reward, they will not reveal
their decisions to each other. As such, a hash rate deci-
sion of any miner is the private information which is
unknown and unobservable by otherminers.
– Second, the stochasticity of the mobile environment
(e.g., time-varying wireless channel quality and min-
ers’ locations) must be taken into account in the
model. That is, the SP and miners should be able to
dynamically adjust their respective pricing and hash
rate decisions in response to random environmental
changes.
The main contributions of this paper are as follows:
1) Given unobservable information about miners’
actions (i.e., hash rate decisions), we demonstrate that
the interactions between the SP and the miners can be
described by a stochastic Stackelberg game under pri-
vate information about the miner’s actions. The game
is stochastic because the long-term payoffs of the SP
(or a leader) and miners (i.e., followers) are affected
by the random mobile environment with unknown
state transitions.
2) We prove the existence and uniqueness of Nash equi-
librium (NE) for the game played by theminers. Since
miners cannot observe the actions of other miners,
we model their decision making by a partially-
observable Markov decision process (POMDP) and
show that its solution determines the miners’ best
response strategies which maximize their expected
long-term payoffs.
3) We formulate a stochastic optimization problem of the
SP where both the objective (expected long-term pay-
off of the SP) and the constraints (on miners’ budgets)
are the functions of a stochastic state determined by
the miners’ actions and rewards. To solve this prob-
lem, we represent the SP’s decision making as a Mar-
kov decision process (MDP) and show that its solution
is equivalent to the solution of the SP’s optimization
problem.
4) Based on the presented MDP and POMDP models,
we propose a hierarchical reinforcement learning
(RL) fra-mework for the SPs and miners. The frame-
work allows the SP to maximize its long-term payoff
by dynamically adjusting the price per unit hash rate.
It also enables the miners to select the best response
strategies and update beliefs about the unobservable
actions of other miners through repeated interactions
with each other and themobile environment.
The rest of the paper is organized as follows. In Section 2,
we review the related work on blockchain applications in
wireless networks. The system model of blockchain applica-
tions in the MEC system is developed in Section 3. The
game modeling the interactions of blockchain miners in the
blockchain network, the POMDP model of the miners’ deci-
sion process, and RL algorithms based on this model are
proposed in Section 4. The stochastic optimization problem
for the SP, the MDP model of the SP’s decision process, RL
algorithms based on this model, and a hierarchical learning
procedure for the SP and the miners are formulated in
Section 5. The numerical results are provided in Section 6.
The conclusions are given in Section 7.
2 RELATED WORK
Existing research in the area of blockchain applications in
wireless networks can be arbitrarily divided into two catego-
ries. The first category ([14], [15], [16], [17], [18], [19], [20],
[21]) analyzes the design and implementation of the various
blockchain-based protocols for secure and decentralized
data communication in wireless networks. For example, in
[14], the authors present a distributed mechanism which
exploits a secure public key encryption scheme, namely, a
hierarchical identity-based encryption (HIBE), for content
distribution in information centric networks. The mecha-
nism deploys a Bitcoin-like blockchain network, Namecoin,
for data registration and transfer in the system, where the
content name is utilized as HIBE public keys. In [15],
the authors propose to directly map the account addresses in
the blockchain networks to the addresses in the named data
networking (NDN) and show that such blockchain system
over NDN can overcome some problems, such as eavesdrop-
ping and traffic analysis, of Internet protocol (IP) networks.
The studies in [16] and [17] explore feasibility of distributed
consensus powered smart contracts in the content delivery
networks. The proposed content delivery model utilizes a
blockchain-based brokering mechanism with a series of
smart contracts. It adopts the blockchain network as a
backbone for executing and auditing transactions among
different network entities to enable self-organization and
decentralization. Blockchain-based spectrum sharing proto-
cols for cognitive radio networks are analyzed in [18]. Here,
the blockchain network serves as a medium access protocol,
bywhich the competing cognitive radios can lease and access
available spectrum resources in a secure and decentralized
manner. The key management scheme for secure group
communication in vehicular communication systems is pre-
sented in [19]. The proposed framework uses a blockchain-
based network topology to decentralize and simplify the key
transfer transactions which are recorded, shared and copied
in blocks to preserve the data integrity. In [20], the authors
ASHERALIEVA AND NIYATO: LEARNING-BASED MOBILE EDGE COMPUTING RESOURCE MANAGEMENT TO SUPPORT PUBLIC BLOCKCHAIN ... 1093
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
develop a security enhancement solution for a blockchain-
enabled Internet of Vehicles (BIoV) system that comprises
two stages: i) miner selection and ii) block verification. The
first stage is a voting scheme where the candidate vehicle is
evaluated based on its reputation which depends on the his-
torical data and opinions of other vehicles (the candidate
with the high reputation is selected to be an active miner). In
the second stage, a newly generated block is verified and
audited by standby (i.e., inactive) miners to prevent internal
collusions among activeminers. The participation of standby
miners in the block verification process is incentivized by
adopting the contract theory. A comprehensive survey on
the state-of-the-art decentralized consensus mechanisms in
the blockchain network is provided in [21]. The survey
focuses on the perspectives of the design of distributed con-
sensus mechanisms, its impact on the blockchain network
architecture, self-organization by individual blockchain nodes,
and emerging blockchain applications.
The second category ([8], [9], [22], [23]) investigates the
pricing and resourcemanagement for supporting blockchain
applications in wireless networks. The focus here is on the
mining under Nakamoto consensus [4], [5] which results in
the competition among miners to receive a mining reward.
Due to limited computing resources of their mobile termi-
nals, miners offload their PoW computations to local MEC
servers. For example, in [8] and [9], the interplay between the
MEC SP and miners is modeled as a two-stage Stackelberg
game. At stage one, the SP (aka leader) assigns the price per
unit hash rate with the aim to maximize its one-stage profit.
At stage two, given the unit price, the miners (followers)
select hash rates to maximize their stage payoffs. The auction
mechanisms for resource management in the MEC system
with blockchain applications are analyzed in [22] and [23].
The objective of the auction in [22] is to maximize the social
welfare while ensuring its incentive compatibility (i.e., trust-
fulness for the miners). In [23], the performance of auction to
maximize the revenues of the MEC SPs is improved by
applying deep learning. To summarize, the above studies
provide the possibility of enhancing and extending the com-
puting abilities of mobile and IoT terminals through MEC
services offered by some external SP. Nevertheless, they still
have several limitations which are listed below:
– Themain assumption of the works in [8], [9], [22], [23]
is that the hash rate decision of each miner is known
to other miners. This is unrealistic in the blockchain
networks, since miners choose their actions simulta-
neously and independently, without announcing
their decisions to otherminers [1], [13].
– It is considered that the miners’ budgets are uncon-
strained, i.e., the miners can pay any price for MEC
services, but in real-world scenarios, the miners’
budgets are finite.
– The dynamics and randomness of the wireless envi-
ronment (e.g., channel quality and miners’ locations)
are ignored in the models in [8], [9], [22], [23]. That
is, the games and auctions are played in a static and
deterministic environment, and their solutions are
for a short term (one-stage).
Similar to above works, this paper studies pricing and
computational offloading for public blockchainmining under
the Nakamoto consensus. However, unlike these works, it is
based onmore realistic assumptions that are applicable to the
practicalMEC networks. In particularly, we assume that:
1) The information about theminers’ hash rate decisions
is private, i.e., unknown by other miners. Hence, the
miners must form and update their beliefs about the
actions of other miners through RL.
2) The miners’ budgets are limited. That is, when
deciding on the price, the SP must also consider pos-
sible expenditures of miners, which makes the prob-
lem much more complicated.
3) The proposed Stackelberg model is stochastic, i.e., it
is played in a dynamic and random environment,
and re-turns the long-term (multi-stage) solutions.
3 BLOCKCHAIN OVER THE MEC NETWORK
3.1 System Model
Consider a public blockchain application realized over awire-
less MEC network, e.g., as in [8], [9]. As shown in Fig. 1, the
MEC network is formed by the setM ¼ f1; . . . ;Mg of macro-
and small-cell BSs, labeled as BS1; . . . ;BSM , each of which is
equipped with the MEC server. In the network, the set N ¼
f1; . . . ; Ng of users or miners, labelled as U1; . . . ;UN , offload
the PoW puzzle computations to MEC servers of their associ-
ated BSs. We denote byNm � N,m 2 M, the subset of miners
associated with BSm, such that
S
m2MNm ¼ N and
T
m2M
Nm ¼ �. Note that since the locations of miners in the MEC
network may change over time, the corresponding miner-BS
associations and, hence, the subsets Nm, m 2 M, may also
change. The network operates on the orthogonal frequency-
division multiplexing (OFDM) spectrum. All BSs are main-
tained by the SP that sells its computing resources counted in
terms of edge computing units (ECUs) to miners. The com-
puting resources of each BS are limited by the maximal num-
ber of ECUs supported by its MEC server. We denote by Am
2 Nþ,m 2 M, whereNþ ¼ Nnf0g is the set of positive natural
numbers, the maximal computing power of BSm (in ECUs).
The miners compete against each other to solve a chain of
blocks representing the PoW puzzle through a try-and-guess
strategy. The occurrence of mining a block is modeled as a
random variable following a Poisson distribution with the
mean of 1=Tb, where Tb is the expected block interval time,
typically equal to 600 seconds [9], [13], [24]. The process of
mining a blockchain consists of the number of stages, denoted
as t ¼ 0; . . . ; T , during which every subsequent block of PoW
puzzle is solved. The SP and all miners remain in the system
Fig. 1. Blockchain application in the MEC network withM ¼ 11 BSs.
1094 IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 20, NO. 3, MARCH 2021
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
for an indefinitely-long time. Thus, even if the duration of
their stay in the system may be finite, it is unknown. Hence,
the total number of mining stages is T !/. It is assumed that
during one mining stage, all the network parameters (e.g.,
locations of miners, miner-BS associations, wireless channel
quality and channel gains) remain constant.
At the beginning of stage t, the SP assigns the current price
pt 2 P per ECU and announces it to all miners. To be compat-
ible with the analytical framework of this paper, we assume
that all payments in the blockchain can take any value from
the finite discrete set P ¼ f1; . . . ; Pg, where P is a maximal
possible payment. Similarly, the budgets of miners can take
any value from from the finite discrete set B ¼ f1; . . . ; Bg,
whereB is a maximal miner’s budget. Given the ECU’s price
pt, every miner Un, n 2 N, decides on its hash rate, i.e., com-
puting power ant 2 An (in ECUs), where a finite discrete set
An ¼ f1; . . . ; ang of possible miner’s hash rate decisions is
limited by the maximal hash rate an that can be selected by
miner Un. As such, an is contrained by the maximal comput-
ing power of the BS associated with miner Un. That is, for
any BSm, we must have
P
n2Nm
an � Am. For example, the
SP can restrict themaximal hash rate of eachminerUn associ-
atedwith someBSm to staywithin an � bAm=Nmc, where bxc
is the floor of x; Nm ¼ jNmj is the number of miners associ-
ated with BSm. After deciding on their hash rates, miners
start (simultaneously) solving a new block of puzzle. Once
the solution is found, it is propagated across the entire block-
chain network to reach a consensus. Any miner which is the
first to find a solution that reaches consensus receives a min-
ing reward. At this moment, the current stage t of the mining
process concludes and the next stage tþ 1 begins.
3.2 Mining Rewards and Payoffs of the Miners
Let rnt 2 Rn be the reward of miner Un resulting from its
hash rate decision ant at stage t, where Rn ¼ f0; Rþ rxng is
a finite discrete set of possible miner’s rewards. Specifically,
rnt ¼ Rþ rxn > 0, if the miner’s solution is the first to
reach consensus, and rnt ¼ 0 otherwise. A positive reward
includes a fixed part R 2 P and a variable part rxn. A vari-
able part is equal to the product of a given reward factor
r 2 P and a block size defined as the number of transactions
xn 2 Nþ chosen by miner Un. The block size does not affect
the complexity or cost of its mining. However, the chances
of winning a mining reward are decreasing when the block
propagates more slowly. That is, if the size xn is too large,
the block is likely to be discarded due to long latency which
is called orphaning [9], [13], [24].
Let ðant ; a�n
t Þ 2 A ¼ �i2NAi be the miners’ hash rate pro-
file, where a�n
t ¼ faitgi2Nnfng 2 A�n ¼ �i2NnfngAi denotes
the hash rates of all miners except Un. Then, for any miner
Un, the probability of reward rn 2 Rn given the hash rate
profile ðant ; a�n
t Þ is defined by
Pr rnt ¼ rnjant ; a�n
t
� �
¼ Pn
s ant ; a
�n
t
� �
1� Pn
o
� �
; rn ¼ Rþ rxn;
1� Pn
s ant ; a
�n
t
� �
1� Pn
o
� �
; rn ¼ 0
:
(
(1)
In (1), Pn
s ðant ; a�n
t Þ is the probability that miner Un solves
the block that is proportional to a relative hash rate of the
miner, i.e.,
Pn
s ant ; a
�n
t
� � ¼ antP
i2N ait
; (2)
Pn
o is the probability of orphaning which depends on the
transmission delay in the wireless channel between miner
Un and its associated BS and can approximated as [9], [24]
Pn
o ¼ 1� e�znxn=Tb ; (3)
where zn > 0 is the delay factor, i.e., transmission time per
transaction of miner Un. Note that Pn
s ðant ; a�n
t Þ 2 ð0; 1� and
Pn
o 2 ð0; 1Þ, since ant > 0 and znxn > 0, for all n 2 N.
Apparently, zn can be computed from the current data
rate of a wireless channel between the miner and its associ-
ated BS, as in
zn / 1
bn log 2 1þPm2M
1n2Nm
pnGn;mP
i2Nn nf g b
i;npiGi;mþs2
� � ; (4)
where bn is the bandwidth of wireless channel of miner Un;
bi;n 2 f0; 1g is the spectrum overlap indicator, such that
bi;n ¼ 1, if the bandwidth bn of miner Un overlaps with the
bandwidth bi of another miner Ui; p
i and Gi;m are the trans-
mit power of miner Ui and the channel gain of wireless link
between miner Ui and BSm, respectively; s
2 is the variance
of a zero-mean additive white Gaussian noise (AWGN)
power; 1n2Nm ¼ 1, if n 2 Nm and 1n2Nm ¼ 0, otherwise.
Note that the expression in (4) presumes the possibility of
interference between transmissions of miners, although the
interference-free scenario is also possible. For example, the
bandwidth can be allocated so that
P
n2N
P
i2N bi;n ¼ 0 andP
n2N bn � B, where B is the operating bandwidth of the
network. Note that at any mining stage, miner Un knows
the current exact values of bn, pn, Gn;m, and s2, for its wire-
less channels (between the miner and its associated BSm).
All other parameters, i.e., the values of pi and Gi;m for any
miner Ui transmitting over the overlapping band (i.e., for
all i 2 fj 2 Nnfng j bj;n ¼ 1g, can be reported to Un by its
associated BS at the end or beginning of each mining stage.
Hence, at any mining stage, miner Un can accurately esti-
mate its current delay factor zn that reflects the time- and
location-dependent characteristics of the wireless channel
between miner Un and its associated BS.
Given the ECU’s price pt assigned by the SP, the instanta-
neous payoff that miner Un receives at the end of stage t
from its hash rate decision ant 2 An resulting in the reward
rnt 2 Rn is expressed by
un ant ; r
n
t jpt
� � ¼ rnt � pta
n
t : (5)
The payoff of miner Un in (5) is a function of:
– ECU’s price pt observable by miner Un;
– hash rate ant decided by miner Un;
– a random mining reward rnt which (as follows from
the probability expression in (1)) depends on the
hash rates ðant ; a�n
t Þ selected by all miners, where a�n
t
is unobservable by miner Un.
As nominerUn is certain about the hash rate profile a�n
t of
otherminers, it cannot know its payoff in (5) whilemaking its
hash rate decision. Nevertheless, any miner Un can construct
ASHERALIEVA AND NIYATO: LEARNING-BASED MOBILE EDGE COMPUTING RESOURCE MANAGEMENT TO SUPPORT PUBLIC BLOCKCHAIN ... 1095
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
its own estimate of the profile a�n
t . This estimate, also called a
“belief”, represents a joint probability distribution Bn
t ¼
fBn
t ða�nÞga�n2A�n , where each Bn
t ða�nÞ ¼Qi2Nnfng B
n
t ðaiÞ is
the probability Prfa�n
t ¼ a�ng that miner Un assigns to other
miners having the hash rate profile a�n at stage t and Bn
t ðaiÞ
is the marginal probability. The initial miner’s belief Bn
0 ða�nÞ
is estimated (as in [25], [26], [27]) based on, e.g., historical
data on a�n. If no data are available, the initial distributionBn
0
is uniform, i.e., all hash rates are assumed equiprobable. After
this, the beliefsBn
t , t ¼ 1; . . . ; T , are updated using Bayes’ the-
orem [25], [26], [27], as described in Section 4.
3.3 Stochastic Optimization Problem for the SP
The SP aims at maximizing the long-term payoff from its
services. Since the SP remains in the system for an indefi-
nitely-long time, its long-term payoff is defined by the infi-
nite discounted sum of its stage payoffs [26], [27], [28], i.e.,
V ¼
X/
t¼0
gtu ptjatð Þ; (6)
where g 2 ð0; 1� is a discount factor; uðptjatÞ is the payoff
from the ECU’s price pt at stage t given the miners’ hash
rates at ¼ fant gn2N 2 A ¼ �n2NAn, expressed by
u ptjatð Þ ¼ pt � �Tbð Þ
X
n2N
ant : (7)
In (7), �Tb is the expected service cost per ECU equal to the
product of the cost of power � and expected block interval
time Tb.
Theoretically, the SP can set any price per ECU, including
the maximal possible price (i.e., pt ¼ P ). In reality, however,
the miners have limited budget, i.e., the amount of currency
that they can afford to spend during an entire mining pro-
cess. Let sn
t 2 SS
n � B be the budget of minerUn at the begin-
ning of stage t, where the set of miner’s budgets SSn is finite
and discrete, as it represents the subset of the finite and dis-
crete set B. Before the start of a mining process (i.e., at stage
t ¼ 0), the information about the initial budget sn
0 of every
miner Un is passed to the SP, which must ensure that no
miner leaves the blockchain network because its budged
is depleted. This requirement is common to many pricing
models (e.g., [29], [30]). Formally, it can be expressed by the
following budget constraint:
sn
tþ1 ¼ sn
t � pta
n
t þ rnt and lim
t!1
sn
t � 0; 8n 2 N: (8)
Then, the optimization problem for the SP is defined by
maximize
pt2Pf gt2N
X/
t¼0
gtu ptjatð Þ; (9)
subject to (8).
Note that the SP assigns the ECU’s price pt at the begin-
ning of stage t, i.e., before knowing the miners’ hash rate
decisions at. That is, at represents a random variable vector,
although the SP can observe the value of at at the end of each
stage t, i.e., after assigning the price pt. Moreover, the reward
rnt of each miner Un is also a random variable that depends
on the probabilities Pn
s ðant ; a�n
t Þ and Pn
o in (2) and (3), respec-
tively. Therefore, (9) is a stochastic optimization problem.
3.4 Stackelberg Model of the Blockchain Over MEC
Based on the above description of the mining process, the
interactions between the miners and the SP can be modeled
as a stochastic Stackelberg game with private or incomplete
information. In this game, at the beginning of mining stage t,
the leader, i.e., the SP, assigns the ECU’s price pt by solving
the stochastic optimization problem in (9). Every follower,
i.e., miner Un, responds to the current price assignment by
selecting the hash rate ant thatmaximizes its long-term payoff
defined by the discounted sum of its stage payoffs over an
infinite period in the future, i.e.,
P/
t¼0 g
tunðant ; rnt jptÞ. At any
stage t, the payoff unðant ; rnt jptÞ is affected not only by the
miner’s hash rate ant and ECU’s price pt, but also by the
reward rnt resulting from its hash rate decision. The reward
rnt is a random variable which depends on the hash rates
at ¼ ðant ; a�n
t Þ selected by all miners. Since no miner Un is
able to observe and know the hash rate profile a�n
t of other
miners, it must form its belief Bn
t ða�nÞ about a�n
t based on
the available observations rnt . The data flow between the SP
and the miners at every stage of a stochastic Stackelberg
game under private information is shown in Fig. 2.
Note that the above game model is stochastic because at
every stage of the mining process, the payoffs of the leader
and its followers are determined by the set of stochastic
parameters with unknown state transitions. Some of these
parameters, in particular, the hash rate decisions ant of each
miner Un, are also private, i.e., known only by miner Un and
not by other miners. Therefore, the followers operate under
incomplete information. Furthermore, recall that in the con-
ventional Stackelberg game, the leader assigns the price by
estimating the best responses of its followers by using back-
ward induction [31]. In our game, however, the best
responses of the followers depend on their beliefs about the
unobservable hash rate profiles. These beliefs are unknown
to a leader and, thus, it cannot estimate the best response
strategies directly. Instead, it can learn these strategies via
direct observations performed during repeated interactions
with the followers. In the following, we formulate the game
describing the interactions among the miners in the block-
chain network and develop the POMDP model of miners’
decision process to find the game solution. Based on the
POMDP model, we propose the RL framework which will
be used by the miners to update their beliefs about the hash
rates of other miners and select their best response strate-
gies in response to the price set by the SP. Next, we model
the interactions between the miners and the SP as the MDP.
Fig. 2. Data flow between the SP and the miners in the game model.
1096 IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 20, NO. 3, MARCH 2021
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
Based on the MDP model, we develop learning algorithms
that will allow the SP to dynamically adjust the ECU’s price
in order to maximize its long-term expected payoff from the
MEC services subject to miners’ budget constraints.
4 INTERACTIONS AMONG MINERS IN
BLOCKCHAIN NETWORK
4.1 Mining as a Stochastic Non-Cooperative Game
With Unobservable Actions
The interactions among miners can be described by the sto-
chastic non-cooperative game with unobservable actions
played repeatedly at every stage of the mining process. In
the game, in addition to external environmental states which
have fully-observable but stochastic transitions, no player
can observe the past or current actions of other players, i.e.,
the information about players’ actions is private. That is, the
game generalizes both the repeated games and POMDPs
with partially-observable random state and unknown state
transitions [32], [33]. In particular, in our game, denoted as GG,
at any stage t, the private information of each player, i.e.,
minerUn, represents its hash rate decision or action ant 2 An.
A fully-observable environmental state snt 2 Sn of miner Un
is represented by the reward-cost pair ðrnt ; ptÞ 2 Rn � P and
has the state transitions which depend on the partially-
observable action profile ðant ; a�n
t Þ 2 A.
The game is defined by the following elements:
– the set of players or miners N and, for each player
Un, n 2 N,
– the set of actions or hash rate decisions An;
– a partially-observable state space VVn ¼ Sn �A�n
which includes an observable state space Sn ¼ Rn � P
equivalent to a set of all possible reward-cost pairs of
playerUn and the unobservable state spaceA�n equiv-
alent to a set of possible actionprofiles of other players;
– the transition dynamics Prf�sn; �a�njan; sn; a�ng, i.e.,
the probability of transitioning to state ð�sn; �a�nÞ 2 VVn
given the action an 2 An executed in state ðsn; a�nÞ
2 VVn, which can be factored into two conditional dis-
tributions, Prf�snjan; sn; a�ng and Prf�a�nja�ng;
– the payoff unðan; snÞ ¼ unðan; rnjpÞ received from the
action an 2 An in state sn ¼ ðrn; pÞ 2 Sn.
Note that in conventional stochastic games (with observ-
able actions), a strategy pn : VVn ! An of each player Un is a
mapping from the state space VVn to the action spaceAn [28].
In our case, however, the state space VVn of each player Un
comprises two subsets, Sn and A�n. Since the latter subset is
not directly observable, the player forms its belief Bnða�nÞ
about unobservable actions a�n 2 A�n. As a result, the play-
er’s strategy pnðBn; snÞ in game GG is a mapping from its belief
Bn and observable state sn 2 Sn to the action an 2 An.
The game proceeds as follows. At the beginning of stage t,
the game is in some state snt 2 Sn. After observing the state snt ,
the player Un selects a strategy pnðBn
t ; s
n
t Þ ¼ ant that maps
from its current belief Bn
t and state snt 2 Sn to the action
ant 2 An. As a result of the action ant 2 An executed in state
snt 2 Sn, the gamemoves to a new random state sntþ1 2 Sn and
the player Un receives a payoff unðant ; sntþ1Þ. The distribution
of state sntþ1 depends on the past state snt and action ant . The
procedure is repeated in new state and the game continues
for an infinite number of mining stages T !/. Thus, the
value, i.e., the long-term payoff of playerUn is defined by the
infinite discounted sum of its stage payoffs [26], [27], [28], i.e.,
V n ¼
X1
t¼0
gtun pn Bn
t ; s
n
t
� �
; snt
� �
: (10)
Consequently, at any stage t, given belief Bn
t and state snt ,
player Un will select a strategy pn	ðBn
t ; s
n
t Þ ¼ pn	
t that maxi-
mizes its current expected long-term payoff, i.e.,
pn	
t ¼ argmax
an2An
V n
tþ1 Bn
t ; s
n
t jan
� �
: (11)
In (11), V nðBn
t ; s
n
t janÞ is the current expected long-term
payoff of player Un given belief Bn
t and state snt , defined as
V n Bn
t ; s
n
t jan
� � ¼ E
X1
t¼t
gt�tun pn Bn
t ; s
n
t
� �
; snt
� �j Bn
t ; s
n
t ; a
n
( )
¼
X
a�n2A�n
Bn
t a�nð ÞV n snt jan; a�n
� �
;
(12)
where
V n snt jan; a�n
� � ¼ X
sn2Sn
Pr snjan; snt ; a�n
� �
un an; snð Þð
þ gV n snjan; a�nð ÞÞ;
(13)
is the expected long-term payoff of player Un given state snt
and action profile of other players a�n:
A commonly-used solution notion for non-cooperative
games is the NE, i.e., an action profile a	 ¼ ðan	; a�n	Þ that
satisfies
V n snjan	; a�n	ð Þ � V n snjan; a�n	ð Þ; (14)
for all an 2 An, sn 2 Sn, n 2 N. The following lemmas estab-
lish the existence of NE in game GG and the condition for the
uniqueness of this equilibrium.
Lemma 1. Game GG admits at least one Nash equilibrium.
Lemma 2. The uniqueness of the Nash equilibrium in game GG is
guaranteed, provided that the following condition
1� Pn
o
� �
Rþ rxnð Þ � 4p
X
i2Nn nf g
ai; 8n 2 N; (15)
is satisfied.
The proofs of Lemma 1 and 2 are provided in Appendices
A and B, respectively. Based on these lemmas, game GG
admits the NE which is unique given the condition in (15).
Note, however, that in our model, no player Un can directly
compute its best-response action an	 satisfying (14), as it can-
not observe the actions of other players. Instead, any player
Un can estimate a strategy pn	ðBn; snÞ, which is optimal with
respect to its current beliefBn in state sn, i.e.,
V n Bn; snjpn	 Bn; snð Þð Þ � V n Bn; snjpn Bn; snð Þð Þ: (16)
In the repeated game settings, player Un can update its
belief Bn about unobservable actions of other players by
adopting stochastic dynamic programming and RL. Thus, in
the following, we develop the RL framework that allows the
ASHERALIEVA AND NIYATO: LEARNING-BASED MOBILE EDGE COMPUTING RESOURCE MANAGEMENT TO SUPPORT PUBLIC BLOCKCHAIN ... 1097
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
players to select the optimal strategies and learn the distribu-
tions of unobservable actions and observable states through
repeated interactions with each other and the stochastic
environment.
4.2 POMDP Model of the Miners’ Decision Making
We start with the brief description of POMDPs which will
be further used to develop our RL model. Formally, the
POMDP is defined by the tuple ðVV;S;A;Pr; u; gÞ compris-
ing the following elements [26], [27], [28]:
– partially-observable state space VV ¼ S�QQ that con-
sists of the observation space, i.e., the set of fully-
observable states S � VV, and the unknown state
space QQ;
– set of actions A;
– transition dynamics Prf�s;�uja; s; ug ¼ Prf�sja; s; ug Pr
f�ujug, i.e., the probability of transiting to state
ð�s;�uÞ 2 VV given the action a 2 A taken in state
ðs; uÞ 2 VV;
– payoff function uða; sÞ;
– discount factor g 2 ð0; 1�.
Since the transitions Prf�ujug of unknown state u 2 QQ are
not directly observable, the learning agent must rely on
every observed transition ðs; a; �sÞ to infer the underlying
probability distribution B ¼ fBðuÞgu2QQ, for BðuÞ ¼ Prfug.
This distribution, also called a belief, can be updated using
Bayes’ theorem [25], [26], [27], [28], as
Bs;�s
a uð Þ ¼ Pr �sja; s; uf gB uð ÞP
�u2QQ Pr �sja; s;�u� �
B �u
� � ; (17)
where BðuÞ is the prior belief, Bs;�s
a ðuÞ is an updated belief.
The RL problem consists of finding a strategy pðBÞ 2 A
(mapping from the beliefs to actions) that achieves a maxi-
mal value. In RL, the value V pðBÞ of strategy p represents
the expected discounted sum of the payoffs received while
executing it. That is,
V p Bð Þ ¼
X/
t¼0
gtu p Btð Þ; stð Þ; (18)
whereBt and st are, respectively, the belief and state at stage
t of the learning process. A strategy p	 is optimal if its value
is maximal in all belief states, i.e., V p	ðBÞ � V pðBÞ, for all p
andB, and satisfies the Bellman optimality equation [28]:
V p	 Bð Þ ¼ max
a2A
X
�s2S
Pr �sja; s;Bf g u a; �sð Þ þ gV p	 Bs;�s
a
� �� 	
;
(19)
where Bs;�s
a is an updated belief computed using (17).
Apparently, the problem of determining the players’ best
response strategies in a stochastic game under incomplete or
private information can be cast as a POMDP [31], [32], [33].
More specifically, when applied to game GG, the POMDP
for player/miner Un, n 2 N, is represented by the tuple
ðVVn;Sn;An;Pr; un; gÞ, with elements described in the previ-
ous subsection. In our case, the strategy of the player
pnðBn; snÞ 2 An maps from its belief Bn and state sn to the
action an. Hence, the RL problem is to find an optimal strategy
pn	 maximizing the value in (12). We can solve this problem
by stochastic dynamic programming [33], [34]. In this case,
we obtain a dynamic programming algorithm comprising
four steps repeated at each stage of the mining/learning pro-
cess. At any stage t, we store a set of jSn �Aj updated values
V n
tþ1ðsnjan; a�nÞ indexed by state sn 2 Sn and action profile
ðan; a�nÞ 2 A. The algorithm terminates when the value V n
t
reaches a stable state, i.e., when
V n
tþ1 � V n
t
 
¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiX
sn2Sn
X
an2An V n
tþ1 Bn
tþ1; s
njan� �� V n
t Bn
t ; s
njanð Þ� �2q
� d;
(20)
where d 2 ½0; 1Þ is some infinitesimal number.
The algorithm proceeds as follows:
1) At the beginning of stage t, for the current state snt
and each possible transition ðsnt ; an; �snÞ, update the
belief according to Bayes’ theorem, as
B
snt ;�s
n
an a�nð Þ ¼ Pr �snjan; snt ; a�n
� �
Bn
t a�nð ÞP
�a�n2A�n Pr �snjan; snt ; �a�nf gBn
t �a�nð Þ ;
(21)
for all a�n 2 A�n.
2) Compute the value of an updated belief for the current
state snt from the Bayesian exploration equation [26],
[33]:
V n
t B
snt ;�s
n
an ; �snjan
� 	
¼
X
a�n2A�n
B
snt ;�s
n
an a�nð ÞV n
t �anjan; a�nð Þ;
(22)
for all �sn 2 Sn, an 2 An.
3) Find the optimal strategy pn	ðBn
t ; s
n
t Þ ¼ pn	
t at stage t
by solving the Bellman equation:
pn	
t ¼ argmax
an2An
V n
tþ1 Bn
t ; s
n
t jan
� �
; (23)
where
V n
tþ1ðBn
t ; s
njanÞ ¼
X
�sn2Sn
Prf�snjan; sn; Bn
t gðunðan; �snÞ
þ gV n
t ðBsn;�sn;�snjan
an Þ ¼
X
a�n2A�n
Bn
t ða�nÞV n
tþ1ðsnjan; a�nÞ;
(24)
V n
tþ1 snjan; a�nð Þ ¼
X
�sn2Sn
Prf�snjan; sn; a�ngðunðan; �snÞ
þ gV n
t ð�snjan; a�nÞÞ;
(25)
for all sn 2 Sn, an 2 An.
4) After executing the action ant ¼ pn	
t and observing
the state �sn ¼ sntþ1 resulting from this action at the
end of stage t, set the next belief Bn
tþ1ða�nÞ based on
the last realized transition ðsnt ; ant ; sntþ1Þ, as
Bn
tþ1 a�nð Þ ¼ B
snt ;s
n
tþ1
ant
a�nð Þ; 8a�n 2 A�n: (26)
1098 IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 20, NO. 3, MARCH 2021
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
Lemma 3 below establishes that the proposed dynamic
programming algorithm which represents the sequence of
iterations fV n
t gt2N, converges to a fixed optimal value V n	,
i.e., the value of an optimal strategy pn	 ¼ pn	ðBn	; snÞ. In
other words, this lemma shows that limt!/ V n
t ¼ V n	 and,
hence, limt!/ pn	
t ¼ pn	.
Lemma 3. The learning process defined by the sequence of itera-
tions fV n
t gt2N in (21), (22), (23), (24), (25), (26), converges to a
fixed optimal value, i.e., the solution V n	 of an exact POMDP,
for each n 2 NN , with probability one as time tends to infinity.
The proof of Lemma 3 is given in Appendix C. Based on
Lemma 3, we obtain the following corollary that estab-
lishes that the learning process fV n
t gt2N converges to a
stable state in which the players’ strategies form the NE
defined by (14).
Corollary 1. With probability one as time tends to infinity, the
learning process represented by the sequence of iterations
fV n
t gt2N in (21) – (26), converges to a stable state in which the
players’ strategies are in the NE.
The proof of Corollary 1 is provided in Appendix D.
4.3 Exact Model-Based RL by the Miners
Wenowdiscuss the implementation of the proposed dynamic
programming algorithm in (21) – (26). Note that in game GG,
the state sntþ1 ¼ �sn 2 Sn resulting from the action an ¼ ant 2 A
taken at stage t in state snt 2 Sn is represented by the pair
ðrnt ; ptÞwhich comprises:
– reward rnt 2 Rn received by player/miner Un at
the end of stage t as a result of players’ actions
ðant ; a�n
t Þ 2 A;
– ECU’s cost pt 2 P assigned by the SP at the beginning
of stage t.
Clearly, the random variables rnt and pt are independent
of each other. Furthermore, the action profile ðant ; a�n
t Þ has no
impact on the ECU’s cost pt, since this cost is assigned by the
SP before the corresponding actions are realized by players.
Thus, the transition probability Prf�snjan; sn; a�ng in (21)
takes the form
Pr �snjan; sn; a�nf g ¼ Pr �rnjan; rn; a�nf gPr �pjpf g; (27)
where the probability Prf�rnjan; rn; a�ng is equivalent to the
probability Prfrnt ¼ rnjan; a�ng in (1).
Combining (1), (21), and (27), we arrive at the following
expression for the belief update rule in (21):
B
snt ;�s
n
an a�nð Þ ¼ Brn
an a�nð Þ
¼
Pn
s an;a�nð ÞBn
t a�nð Þ
c1
; rn ¼ Rþ rxn;
1þPn
o �Pn
s an;a�nð Þð ÞBn
t a�nð Þ
c2
; rn ¼ 0;
8<
:
(28)
which does not depend on Prf�pjpg. In (28), the probabilities
Pn
s ðan; a�nÞ and Pn
o are defined in (2) and (3), respectively;
c1 and c2 are normalizing factors, given by
c1 ¼
X
�a�n2A�n
Pn
s an; �a�nð ÞBn
t �a�nð Þ; (29)
c2 ¼
X
�a�n2A�n
1� Pn
s ant ; �a
�n
� �
1� Pn
o
� �� �
Bn
t �a�nð Þ: (30)
The transition probability Prf�snjan; sn; Bn
t g in the Bell-
man equation (24) is equal to the product
Pr �snjan; sn; Bn
t
� � ¼ Pr �rnjan; rn; Bn
t
� �
Pr �pjpf g; (31)
where
Pr �rnjan; rn; Bn
t
� � ¼
X
a�n2A�n
Bn
t a�nð ÞPr �rnjan; rn; a�nf g:
(32)
Hence, we have
Pr �snjan; sn; Bn
t
� � ¼ 1� Pn
0
� �
Pr �pjpf gc1; rn ¼ Rþ rxn;
Pr �pjpf gc2; rn ¼ 0:
(
(33)
To estimate the transition probability Prf�pjpg in (33), we
utilize a counting variable mn
t ð�p; pÞ to count the number of
times that each transition ð�p; pÞ has occurred and, then,
approximate Prf�pjpg as
P̂rt �pjpf g ¼ mn
t �p; pð ÞP
~p2P m
n
t ~p; pð Þ ; 8�p; p 2 P; (34)
where
mn
t �p; pð Þ ¼ mn
t�1 �p; pð Þ þ 1�p¼pt;p¼pt�1
(35)
at the beginning of stage t. Clearly, the approximation in
(34) and (35) becomes more accurate with time [35], i.e.,
limt!/ P̂rtf�pjpg ¼ Prf�pjpg.
Based on the above, we present the following RL algo-
rithm.At stage t ¼ 0, for anyminerUn, we initialize the belief
Bn
0 , value V n
0 , and counting variable mn
0 . Then, at stage t ¼
0; . . . ; T (where T is a recursion depth [35] which, in general,
can be infinite, i.e., T ! 1), we repeat the steps below until
there are no further changes in the value V n
t , i.e., until the
condition in (20) is satisfied:
1) Estimate the transition dynamics P̂rf�pjpg according
to (34) and (35);
2) Given current state snt , update the belief B
snt ;�s
n
an ða�nÞ
for each transition ðsnt ; an; �snÞ by using the Bayes’
rule in (28);
3) Determine the value V n
t ðB
snt ;�s
n
an ; �snjanÞ, for all �sn 2 Sn,
an 2 An, using the Bayesian exploration equation
in (22);
4) Find the optimal strategy pn	
t by solving the Bellman
equation in (23) – (25).
5) Execute the action ant ¼ pn	
t , observe the resulting
state �sn ¼ sntþ1, and set the next beliefBn
tþ1ða�nÞ based
on the last realized transition ðsnt ; ant ; sntþ1Þ using (26).
It is worth noting that the above model-based RL algorithm
does not require any additional exploration, since it is implicit
in the computation of V n
t ðB
snt ;�s
n
an ; �snjanÞ in (22). Nevertheless,
when the exploration ability is limited, e.g., because of the
finite recursion depth T , it may be useful to deploy some
explicit explorationmethod [35]. For example, we can adopt a
ASHERALIEVA AND NIYATO: LEARNING-BASED MOBILE EDGE COMPUTING RESOURCE MANAGEMENT TO SUPPORT PUBLIC BLOCKCHAIN ... 1099
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
simple "-greedy exploration. In thismethod, for a certain arbi-
trary small " 2 ½0; 1Þ, the optimal strategy pn	
t 2 An is exe-
cuted with probability 1� ", whereas, all other non-optimal
strategies pn
t 2 Annpn	
t are selected uniformly at randomwith
probability ". Furthermore, note that the size of a partially-
observable state space in the game is exponential in the num-
ber of players N , because we have jA�nj ¼ j�i2NnfngAij ¼
ðM þ 1ÞN�1 and jSnj ¼ jRn � Pj ¼ 2ðK þ 1Þ. This gives
jVVnj ¼ jSn �A�nj ¼ 2ðK þ 1ÞðM þ 1ÞN�1. Hence, the pro-
posed RL algorithm based on the exact POMDP is intrac-
table due to its exponential time complexity and, to be
practically-realizable, we introduce its approximation in
the following.
4.4 Approximate Model-Based RL by the Miners
A heuristic technique to approximate the solution of exact
POMDP and reduce the size of the unobservable state
space A�n is a maximum a posteriori estimation (MAPE)
[26]. In MAPE, player/miner Un assumes that the unob-
servable actions a�n of other players coincide with the
most probable, with respect to the current player’s belief
Bn
t , actions â�n ¼ fâigi2Nnfng. Thus, MAPE is equivalent
to exact RL where the belief Bn
t of player Un is approxi-
mated as
B̂n
t a�nð Þ ¼ 1; a�n ¼ â�n ¼ argmaxa�n2A�n Bn
t a�nð Þ;
0; otherwise:
�
(36)
Therefore, the estimation of values V n
tþ1ðBn
t ; s
njanÞ for the
actual player’s belief Bn
t reduces to the estimation of values
V̂ n
tþ1ðB̂n
t ; s
n
t janÞ ¼ V̂ n
tþ1ðsnjan; â�nÞ givenmost probable, from
the player’s point of view, action profile â�n.
As a result, we obtain the following algorithm where, at
every stage t, we store the set of jSn �Anj updated value-
functions V̂ n
tþ1ðsnjan; â�nÞ indexed by the state sn 2 Sn and
action an 2 An. At stage t ¼ 0, for every minerUn, we initial-
ize the belief Bn
0 , value V̂ n
0 , and counting variable mn
0 . Then,
at stage t ¼ 0; . . . ; T , we repeat the steps below until there
are no further changes in the value V̂ n
t , i.e., the condition in
(20) is satisfied:
1) Update the transition dynamics P̂rf�pjpg according to
(34) and (35);
2) Given the current beliefBn
t , determine the most prob-
able action profile â�n ¼ fâigi2Nnfng of other players
from
âi ¼ argmax
ai2Ai
Bn
t ai
� �
; 8i 2 Nn nf g; (37)
If there are two or more most probable action profiles,
one of them is chosen uniformly at random.
3) Determine a near-optimal strategy p̂n	
t by solving the
approximate Bellman equation
p̂n	
t ¼ argmax
an2An
V̂ n
tþ1 B̂n
t ; s
n
t jan
� �
¼ argmax
an2An
V̂ n
tþ1 snt jan; â�n
� �
;
(38)
where
V̂ n
tþ1 B̂n
t ; s
njan� �
¼ max
an2An
X
a�n2A�n
B̂n
t a�nð Þ
X
�sn2Sn
Pr �snjan; sn; a�nf g un an; �snð Þð
þ gV̂ n
t B̂sn;�sn
an ; �snjan
� 		
¼ V̂ n
tþ1 snjan; â�nð Þ
¼ max an2An
X
�sn2Sn
Pr �snjan; sn; â�nf g un an; �snð Þð
þ gV̂ n
t �snjan; â�nð Þ�; 8sn 2 Sn; an 2 An;
(39)
4) With probability 1� ", execute the near-optimal action
ant ¼ p̂n	
t . With probability ", execute another action
ant 2 Annp̂n	
t . Observe the resulting state �sn ¼ sntþ1
and, for each i 2 Nnfng, set the next belief Bn
tþ1ðaiÞ
based on the last realized transition ðsnt ; ant ; sntþ1Þ
according to
Bn
tþ1 ai
� � ¼ B
snt ;s
n
tþ1
ant
ai
� �
¼ Pr sntþ1jant ; snt ; ai
� �
Bn
t aið ÞP
�ai2Ai Pr sntþ1jant ; snt ; �ai
� �
Bn
t �aið Þ ; 8ai 2 Ai:
(40)
The above approximate RL algorithm has a polynomial
time complexity, since the most complex operation in the
algorithm is the search of an optimal strategy p̂n	
t according
to the Bellman equation in (38) and (39) that has the complex-
ity of OðjAnj2 � jSnj2Þ ¼ OðM2K2Þ. Note that although the
algorithm converges to a certain fixed value V̂ n	, this value
is not necessarily the optimal one. That is, in general,
V̂ n	 6¼ V n	. Lemma 4 below derives the error bound between
the fixed value V̂ n	 reached by the approximate RL and the
optimal value V n	, i.e., a solution of the POMDP, reached by
the exact algorithm.
Lemma 4. The approximate learning process defined by the
sequence of iterations fV̂ n
t gt2N in (37), (38), (39), (40) con-
verges to a fixed value V̂ n	 with probability one as time tends to
infinity, for all n 2 NN . Moreover, if the payoff function is
bounded, i.e., junj � un
max < 1, and there exists � 2 ½0; 1�,
such that Bn � B̂n
1 � �, where B̂n is given by (36), then the
error between the value V̂ n	 and the optimal value, i.e., the solu-
tion V n	 of an exact POMDP, is bounded by
V n	 � V̂
n	  � �un
max
1� g
; 8n 2 N: (41)
The proof of Lemma 2 is given in Appendix E. Note that
based on (5), the payoff of player Un is bounded as un 2
½�PaM; Rþ rxn�, which yields unmax ¼ maxfPaM; Rþ rxng.
Furthermore, we have � ¼ 1, since Bn � B̂n
1 � 1. Hence, the
error bound in (41) takes the form:
V n	 � V̂
n	  � max PaM; Rþ rxnf g
1� g
; 8n 2 N: (42)
1100 IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 20, NO. 3, MARCH 2021
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
5 INTERACTIONS BETWEEN THE SERVICE
PROVIDER AND MINERS
5.1 Exact Model-Based RL by the SP
Recall that the stochastic optimization problem of the SP in (9)
depends on the random, i.e., unknown before the price per
ECU is assigned, but observable after the price is assigned,
environmental state. This state is determined by the miners’
best response strategies and rewards received by playing
these strategies. To estimate the miners’ best responses before
setting its price, the SP requires at least the following informa-
tion from each miner Un: i) initial belief B
n
0 , ii) initial values
V n
0 , iii) initial counting variables mn
0 . Since this information is
inaccessible to the SP, the direct estimation of the miners’
best responses is infeasible. Instead, the SP can learn these
strategies via direct observations performed during repeated
interactions with the miners. Hence, in the following, we for-
mulate the optimization problem of the SP as an MDP with
fully-observable random state and unknown state transitions.
Based on the MDP model, we derive the RL process that will
allow the SP to dynamically adjust the ECU’s price in order
to maximize the long-term expected payoff from its MEC
services.
In particular, the MDP for the problem in (9) is defined
by the tuple ðVV;P;Pr; u; gÞ comprising the elements below:
– VV ¼ SS�A� R is a fully-observable finite discrete
state space of the SP represented by the Cartesian
product of the set SS ¼ �n2NSS
n of possible miners’
budgets, set A of miners’ action profiles, and set
R ¼ �n2NRn of miners’ rewards, i.e., the state at
stage t is vvt ¼ ðsst; at; rtÞ, where sst ¼ fsn
t gn2N and
rt ¼ frnt gn2N;
– P is the SP’s action space;
– Prf�v�vjp;vvg is the transition dynamics, i.e., the proba-
bility of transiting to state �v�v 2 VV given the action
p 2 P taken in state vv 2 VV;
– uðp;vvÞ ¼ uðpjaÞ is the SP’s payoff function;
– g 2 ð0; 1� is a discount factor.
The RL problem is to determine an optimal strategy
pðvvÞ 2 Pmapping from the state space VV to the action space
P, which maximizes the value-function, given by
V vvð Þ ¼
X/
t¼0
gtu p vvtð Þ;vvtð Þ; (43)
subject to the constraint in (8). Apparently, this problem can
be solved by dynamic programming [34]. The Bellman opti-
mality equation for the problem takes the form
V 	 vvð Þ ¼ max
p2P
X
�v�v2VV
Pr �v�vjp;vvf g u p; �v�vð Þ þ gV 	 �v�vð Þð Þ
¼ max
p2P
X
�s�s2SS
X
�a2A
X
�r2R
Pr �s�s; �a;�rjp; a; rf g u p; �að Þð
þ gV 	 �s�s; �a;�rð ÞÞ ¼ max
p2P
X
�a2A
X
�r2R
Pr �a;�rjp; a; rf g u p; �að Þð
þ gV 	 ss � paþ r; �a;�rð ÞÞ:
(44)
To estimate the transition dynamics Prf�a;�rjp; a; rg in (44),
note that according to (1), at any stage t, the miners’ rewards
rt 2 R depend only on their action profile at 2 A. Hence, the
probability Prf�a;�rjp; a; rg can be expressed as
Pr �a;�rjp; a; rf g ¼ Pr �ajp; a; rf gPr �rj�af g; (45)
In (45), the joint probability Prf�rj�ag ¼ Prf�r1; . . . ; �rN j�ag is
represented by the product of marginal probabilities, i.e.,Q
n2N Prf�rnj�ag. Clearly, at any stage t, a marginal probabil-
ity Prf�rnj�ag is equivalent to the probability Prfrnt ¼ rnjatg
in (1). On the other hand, the probability Prf�ajp; a; rg is cal-
culated similarly to the probability Prf�pjpg. That is, at the
beginning of stage t, we compute
P̂rt �ajp; a; rf g ¼ mt �a; p; a; rð ÞP
~a2AA mt ~a; p; a; rð Þ ; 8�a; a 2 A; p 2 P;
(46)
where
mt �a; p; a; rð Þ ¼ mt�1 �a; p; a; rð Þ þ 1�a¼at;p¼pt;a¼at�1;r¼rt�1
:
(47)
Based on the above MDPmodel, we obtain the algorithm,
i.e., the sequence of iterations fVtgt2N, where at every stage t,
given the current statevvt ¼ ðsst; at; rtÞ, we determine the opti-
mal strategy p	ðvvtÞ ¼ p	
t by solving the Bellman equation:
p	
t ¼ argmax
p2P
Vtþ1 sst; at; rt; pð Þ; (48)
where
Vtþ1 ss; a; r; pð Þ ¼
X
�a2A
X
�r2R
Pr �a;�rjp; a; rf g u p; �að Þð
þ gVt ss � paþ r; �a;�r; pð ÞÞ ¼
X
�r2R
Pr �rj�af g u p; �að Þð
þ g
X
�a2A
Pr �ajp; a; rf gVt ss � paþ r; �a;�r; pð ÞÞ;
(49)
for all ss 2 SS, a 2 A, r 2 R, p 2 P. In the algorithm, at any
stage t, we store a set of jVV� Pj updated values Vtþ1ðvv; pÞ
indexed by state vv 2 VV and action p 2 P. The algorithm com-
prises three steps which are repeated until there are no fur-
ther changes in the value Vt, i.e., until:
Vtþ1 � Vtk k ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiX
vv2VV
X
p2P Vtþ1 vv; pð Þ � Vt vv; pð Þð Þ2
q
� d:
(50)
The algorithm proceeds as follows. At stage t ¼ 0, we ini-
tialize the values V0 and counting variablesm0. Then, at stage
t ¼ 0; . . . ; T , we repeat the steps below:
1) Observe the current state vvt ¼ ðsst; at; rtÞ 2 VV and
update the transition dynamics P̂rtf�ajp; a; rg using
(46) and (47);
2) Find the current optimal strategy p	
t according to the
Bellman equation in (48) and (49).
3) With probability 1� ", execute the action pt ¼ p	
t .
With probability ", execute another action pt 2 Pnp	
t .
The convergence of the above exact model-based RL
algorithm to a fixed optimal value V 	 that represents the
solution of an exact MDP which is equivalent to the solution
of the optimization problem in (9) is established in Lemma 5
below.
ASHERALIEVA AND NIYATO: LEARNING-BASED MOBILE EDGE COMPUTING RESOURCE MANAGEMENT TO SUPPORT PUBLIC BLOCKCHAIN ... 1101
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
Lemma 5. The learning process defined by the sequence of itera-
tions fVtgt2N in (48) and (49), converges to a fixed optimal
value, i.e., the solution V 	 of an exact MDP, with probability
one as time tends to infinity.
The proof of Lemma 5 is provided in Appendix F.
5.2 RL With Function Approximation by the SP
Note that the size of state spaceVV in our MDPmodel is expo-
nential in the number of miners N . In particular, jVVj ¼
j�n2NðSSn �An � RnÞj ¼ 2NðP þ 1ÞNðM þ 1ÞN . Thus, the
exact RL algorithm becomes intractable if the number of min-
ers is large, as it requires the computation and storage of
jVV� Pj ¼ 2NðP þ 1ÞNþ1ðM þ 1ÞN value-functions at every
stage of learning process. A common approach to deal with
large or continuous state spaces ofMDPmodel is the function
approximation [27], [35], [36], [37], [38], where the value-func-
tion Vtðvv; pÞ is approximated as
V̂ vv; p; uuð Þ ¼
XF
i¼1
ui’i vv; pð Þ; (51)
where uu 2 RF is the real-value vector of weights; ’i : VV�
P ! R is monotonically-increasing, non-constant, continu-
ous, and bounded function, sometimes called an activation
function. Some common activation functions are the thresh-
old function, piece-wise linear function, or sigmoid function
[38], [39]. As such, an approximation in (51) represents a
feed-forward neural network [38] with the output V̂ ðvv; p; uuÞ
and a single hidden layer that comprises F neurons, num-
bered 1; . . . ; F . A hidden neuron i 2 f1; . . . ; Fg has the input
p, the output ’iðvv; pÞ, and two synaptic weights, vv. and ui,
where vv is the weight of the synapse that goes from input p
and ui is the weight of the synapse that connects neuron i
with the output V̂ ðvv; p; uuÞ.
At any stage t of the learning process, the SP’s strategy
pðuutÞ 2 P maps from the current weights uut to the action pt.
An optimal strategy p	ðuutÞ ¼ p	
t maximizes the approximated
value, i.e.,
p	
t ¼ argmax
p2P
V̂ vvt; p; uutð Þ: (52)
The RL problem is to find the weights uu	 which minimize
the loss function, given by [35], [40], [41]
L uuð Þ ¼ E V̂
	 � V̂
� 	2� �
¼ E max
p2P
X
�v�v2VV
Pr �v�vjp;vvf g u p; �v�vð Þð
 (
þ gV̂ �v�v; p; uuð Þ�� V̂ vv;p uuð Þ; uuð Þ
!2
9=
;;
(53)
where a fixed optimal value V̂ 	 is such that jV̂ 	 � V 	j < �,
for some infinitesimal � 2 ½0; 1Þ.
Apparently, the above problem can be solved by stochas-
tic gradient descent [37], [40], [41]. That is, at every stage t,
we minimize the current loss Lt, given by
Lt ¼ yt � V̂ vvt�1; p; uutð Þ� �2
; (54)
where
yt ¼ max
p2P
u p;vvtð Þ þ gV̂ vvt; p; uut�1ð Þ� �
(55)
is an approximation target at stage t. After differentiating the
lossLt with respect to weights uut, we obtain the gradient:
ruutLt ¼ V̂ vvt�1; p; uutð Þ � yt
� �ruut V̂ vvt�1; p; uutð Þ: (56)
Then, the loss can be minimized by updating the weights
in the direction of the negative gradient, i.e.,
uutþ1 ¼ uut � atruutLt; (57)
for X1
t¼1
at ¼ 1 and
X1
t¼1
a2
t < 1: (58)
Based on the above, we arrive at the following RL algo-
rithm where, at every stage t, we store the past weights uut�1
and state vvt�1. At stage t ¼ 0, for each miner Un, we set the
value of F and initialize the weights uu0. At stage t ¼ 1; . . . ; T ,
we repeat the steps below until the loss is minimized, i.e.,
untilLðuuÞ < d:
1) Given the past weights uut�1, observe the current state
vvt 2 VV and calculate the approximation target yt
using (55);
2) Given the current state vvt and weights uut, determine
the optimal strategy p	
t according to (52).
3) With probability 1� ", execute the action pt ¼ p	
t .
With probability ", execute another action pt 2 Pnp	
t .
4) Given the target yt and past state vvt�1, compute the
gradient ruutLt and next weights uutþ1 based on (56)
and (57), respectively.
The proposed algorithm has a polynomial time complex-
ity, because the most complex operations in the algorithm
are related to the gradient estimation in (56) and weight
updates in (57) of the complexity OðF jP� 1jÞ ¼ OðFP Þ. The
convergence of the proposed algorithm to the fixed optimal
value V̂ 	 defined in (53), is established in Lemma 6 below.
Lemma 6. The learning process defined by the sequence of
approximations fV̂ gt2N in (52) – (57), converges to a fixed opti-
mal value V̂ 	, for all n 2 NN , with probability one as time tends
to infinity.
The proof of Lemma 6 is given in Appendix G. Note that
the proposed algorithm belongs to the family of unsuper-
vised deep Q-learning [40], [41], [42], [43] techniques. Similar
to most modern deep learning models [39], it uses an artifi-
cial neural network to approximate the solution of exact
MDP and, at each learning stage, updates the weights of the
neural network to minimize the loss function. Unlike super-
vised learning methods based on feature engineering [39],
the algorithm does not require any labeled data for training,
which means that it can quickly adjust to dynamic environ-
mental changes [39], [40]. The “deepness” of learning in the
algorithm is determined by the number of hidden neuron
layers (in our case, one layer) through which the input is
mapped to the output. That is, the larger is the number of
hidden layers, the “deeper” and the more accurate is the RL
1102 IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 20, NO. 3, MARCH 2021
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
model. However, the improved accuracy comes with the
tradeoff in the complexity [38], [39].
Now, if we combine the proposed exact RL algorithms for
the SP and miners, we will obtain the exact “hierarchical”
learning procedure. In this procedure, at any stage t ¼ 0; . . . ;
T , the SP assigns the ECU’s price pt and updates the values
Vtþ1 by applying the exact RL algorithm presented in this sec-
tion. Then, given the ECU’s price pt assigned by the SP, every
miner Un takes the action ant and updates the values V n
tþ1
using the exact RL algorithm in Section 4. As such, the exact
hierarchical learning procedures can be represented by the
sequence of iterations fV; fV ngn2Ngt2N. The convergence of
this sequence to fixed optimal values fV 	; fV n	gn2Ngt2N is
based on the convergence of the exact RL algorithms for the
SP and miners established in Lemmas 3 and 5, respectively.
Similarly, by combining the approximate RL algorithms, we
obtain the approximate “hierarchical” learning procedure
represented by the sequence of iterations fV̂ ; fV̂ ngn2Ngt2N.
The convergence of this procedure to fixed sub-optimal val-
ues fV̂ 	
; fV̂ n	gn2Ngt2N follows directly from the convergence
of approximate RL algorithms for the SP and miners proved
in Lemmas 4 and 6, respectively.
6 PERFORMANCE EVALUATION
6.1 Simulation Settings
We simulate a public blockchain application in the long-term
evolution advanced (LTE-A) time division duplex (TDD)
based MEC network. The simulation model of the network is
implemented with the OPNET development package [44]. In
the model, the BSs are represented by one macro-cell LTE
evolved NodeB (eNB) labeled as BS1, one micro-eNB labeled
as BS2, and M � 2 pico-eNBs labeled as BS3; . . . ;BSM . The
number of pico-eNBs varies from 1 (by default) to 5, i.e., the
number of BSs varies from M ¼ 3 (by default) toM ¼ 7. The
network area coincides with the macro-cell area, i.e., BS1 is
placed in the center of the network. The small-cell BSs are
placed randomly in the network area. Cell radiuses of the
macro-, micro and pico-eNBs are equal to 2000, 500 and 50m,
respectively. The model considers inter-cell interference
(ICI). In particular, the bandwidth of macro-cell BS1, set as
B1 ¼ B ¼ 20 MHz, is the same as the network bandwidth B.
The spectrum bands of small-cell BSs, set asB2 ¼ 10 andB3 ¼
. . . ¼ BM ¼ 5 MHz, overlap fully with the spectrum band of
BS1 and partially with each other. Every BSm allocates band-
width bn ¼ xnBm=ð
P
i2Nm
xiÞ to its associated miner Un,
n 2 Nm, in proportion to its selected number of transactions
xn. The transmit power of every miner Un, n 2 N, is equal to
pn ¼ 23 dBm. The computing resources of the macro-, micro
and pico-eNBs are limited to A1 ¼ 500, A2 ¼ 100 and A3 ¼
. . . ¼ AM ¼ 10 ECUs, respectively. The maximal hash rate
of miner Un, n 2 Nm, associated with BSm is set as
an � Am=Nm. By default, the numbers of miners associated
with the macro-, micro and pico-eNBs are given by N1 ¼ 50,
N2 ¼ 20 and N3 ¼ . . . ¼ NM ¼ 10, respectively. As such, the
total number of miners varies from N ¼ 80 (by default) to
N ¼ 120. All other parameters of LTE-Amodel (e.g., antenna
gains, noise, shadowing, path loss, etc.) are set based on the
3rdGeneration Partnership Project (3GPP) specifications [45].
We also consider a dynamic and random scenario in
which the numbers of miners can change at any mining
stage. In this scenario, the arrivals of miners at every BS fol-
low the Poisson distribution with the mean arrival rate �
miners/hour. The time during which the miners remain in
the system is distributed exponentially with the expected
value of 1=m hours, where m is the mean departure rate in
miners/hour. Theminers’ mobility is simulated according to
a random waypoint model [46]. In the model, at the end of
stage t, each miner Un randomly selects its target lntþ1 within
a distance of 10 m from the current location lnt (i.e.,
klntþ1 � lnt k 2 [0, 10]). If lntþ1 6¼ lnt , the miner moves towards
its target with a random velocity vntþ1 2 ð0; vmax�, where
vmax ¼ 10 m/s is a typical moving vehicle speed. Otherwise,
if lntþ1 ¼ lnt , the miner remains in its current location. The next
stage tþ 1 starts after all miners arrive at their target loca-
tions where they will remain stationary until the end of stage
tþ 1 (recall that miners’ locations are fixed during one
mining stage). All miners operate in a typical urban environ-
ment. The miners’ budgets and all payments in the block-
chain are counted in the units of a digital currency, i.e.,
bitcoin. Similar to [9], [13], [24], the block size xn follows a
normal distribution with the mean of 200 and a variance of 5;
a fixed reward part is R ¼ 300; the reward factor is r ¼ 1;
�Tb ¼ 0:5 and P ¼ 10R. The initial miners’ budgets follow a
Poisson distributionwith a default expected value of 7R.
According to [28], [35], a discount factor is set as g ¼ 0:9;
the learning rate of stochastic gradient descent is set as
at ¼ 1=ðtþ 0:5Þ0:7; the initial values of counting variables
for the SP and miner Un are equal to m0ð�a; p; a; rÞ ¼ 1
and mn
0 ð�p; pÞ ¼ 1, respectively; the initial weights are set as
uu0 ¼ fui0gi2f1;...;Fg, for F ¼ 100 and ui0 ¼ 0:5; the initial value-
functions for the SP and theminers are set assuming that their
future actions and states are the same as the initial ones, i.e.,
V0 ss; a; r; pð Þ ¼
X/
t¼0
gtu pjað Þ ¼ u pjað Þ
1� g
¼ p� �Tb
1� g
X
n2N an;
(59)
V n
0 snjan; a�nð Þ ¼
X
rn2Rn
Pr rn0 ¼ rnjan; a�n
� �
un an; rnjp0ð Þ:
(60)
In the following, we evaluate the performance of the pro-
posed exact and approximate RL, denoted as “Exact RL”
and “Approx RL”, respectively, realized according to a
hierarchical learning procedure presented in Section 5. The
performance of these algorithms is benchmarked with the
performance of models below:
1) Stackelberg game under complete information de-
noted as “Complete Info” where at every stage t, each
minerUn has a full knowledge of the current and past
actions a�n
t ; a�n
t�1; . . . ; a
�n
0 of other miners. Thus, no
miner Un needs to learn the actions of its opponents.
Instead, it takes a best response action
an	t ¼ max
ant 2An
X
rn2Rn
Pr rnt ¼ rnjant ; a�n
t
� �
un ant ; r
njpt
� �
; (61)
that maximizes its expected payoff. The SP sets the current
optimal ECU’s price p	t based on anticipat-ed best responses
ASHERALIEVA AND NIYATO: LEARNING-BASED MOBILE EDGE COMPUTING RESOURCE MANAGEMENT TO SUPPORT PUBLIC BLOCKCHAIN ... 1103
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
of miners, i.e., the NE a	t . In particular, the SP decides on the
price by solving a stochastic optimization problem in (9)
through the Bellman equation. Given known best responses
of miners, the Bellman equation takes the form:
p	t ¼ argmax
p2P
Vtþ1 sst; at; rt; pð Þ; (62)
Vtþ1 ss; a; r; pð Þ ¼
X
�r2R
Pr �rja	t
� �
u p; a	t
� ��
þ gVt ss � paþ r; a	t ;�r; p
� ��
:
(63)
Such a Complete Info model is similar to those in [8], [9],
except that here we take into account the constrainedminers’
budgets, whereas in [8], [9], the miners’ budgets are unlim-
ited. Since the model is rather unrealistic, we adopt it only
for bench-marking the proposed algorithms.
2) Simultaneous play [47] Stackelberg game denoted as
“Simul Play” where at the beginning of stage t, no
miner Un knows the immediate actions a�n
t of other
miners, although it can observe these actions after
they are executed, i.e., at the end of stage t. As such,
the miner’s decision process is modeled as an MDP
defined by the tuple ðSn;An;Pr; un; gÞ, where a
stochastic state space Sn ¼ Rn � P�A�n is fully-
observable. The miner’s best response an	t is found
from the Bellman equation:
an	t ¼ argmax
an2An
V n
t snt�1jan
� �
; (64)
V n
t snjanð Þ ¼
X
�sn2Sn
Pr �snjan; snf g un an; �snð Þð
þ gV n
t�1 �snjanð Þ�: (65)
There are twoways in which the SP can assign the price in
this game: i) directly estimate the miners’ best responses a	t ;
ii) learn the best responses. The first approach is complex, as
it requires solving the Bellman equation in (64) and (65) for
every miner Un. Therefore, we deploy the second approach
in which the SP learns the best responses through the RL
algorithm presented in Section 5. Note that this model is
more realistic than Complete Info, as no miner knows imme-
diate actions of its opponents. However, in the model, the
actions of miners are fully-observable by other miners. This
assumption is infeasible in the blockchain network, as miners
compete against each other to obtain the reward and, hence,
they are not interested in revealing their hash rate decisions
to otherminers.
3) Stackelberg game under private information about
miners’ actions with no RL involved denoted as “No
RL”. Here, the assumptions of the game are the same
as in this paper, but neither the SP nor the miners use
RL. Each miner Un operates under its initial belief Bn
0
about actions of other miners and at any stage t, it
selects the best response
an	t ¼ max
an2An
X
rn2Rn
Pr rnjan;Bn
0
� �
un an; rnjptð Þ; (66)
maximizing its expected payoff. Given B1
0; . . . ; B
N
0 , the SP
can compute miners’ best responses a	t and assign the opti-
mal price p	t by solving the problem in (9) with the Bellman
equation in (62) and (63).
6.2 Simulation Results
Figs. 3 and 4 show the time dynamics of the average payoffs
of a miner and the SP, respectively. Note that the graphs for
the Complete Info model show the average payoff of the
miner at the NE and the SP’s payoff given the optimal price.
Observe that in Simul Play, Exact RL andApprox RL, payoffs
increase consistently with time, converging to the stable
close-to-optimal levels after about 70, 80, and 90 iterations,
respectively. Moreover, Simul Play converges faster than
Exact and Approx RL. The reason is that in Simul Play, the
state sn of miner Un has an observable transition Prf�snjan;
sng. On the contrary, in Exact RL andApprox RL, in addition
to the state sn, there is also an unobservable state a�n which
is estimated based on the observable transitions by updating
miners’ beliefs with the Bayes’ rule. Clearly, RL in POMDPs
takes a longer time (i.e., number of stages) than that in
MDPs, because it requires convergence of the beliefs before
convergence of the best responses. Hence, Simul Play con-
verges slower than Exact or Approx RL. Finally, note that
Exact RL converges faster than Approx RL, since the latter is
based on the approximated solution. Therefore, to reach a
stable state, it requiresmore exploration.
Fig. 5 shows the convergence time of a hierarchical learn-
ing algorithm and its components, i.e., RL for the SP and
RL for the miners, depending on the number of miners N.
Fig. 3. Time dynamics of average miner’s payoff in different models. Fig. 4. Time dynamics of the SP’s payoff in different models.
1104 IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 20, NO. 3, MARCH 2021
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
Observe that the convergence time increases linearly with the
number of miners. Note that the algorithm for the SP con-
verges after the algorithm for the miners. This can be
explained as follows. The payoff of the SP is defined by its
ability to accurately predict miners’ best responses. On the
other hand, the best responses depend on the current miners’
beliefs about unobservable actions. Thus, when the beliefs are
fluctuating, i.e., the RL process for miners has not reached its
stable state, the SP’s prediction accuracy is low and, conse-
quently, its payoff is far from the optimal one. However,
when the beliefs are stable, the SP’s prediction accuracy
increases, and its payoffs approaches the optimal one. There-
fore, the algorithm for the SP converges after the algorithm
for theminers.
Figs. 6 and 7 demonstrate the convergence time in a
dynamic scenario in which the number of minersN can ran-
domly change at anymining stage. In particular, Fig. 6 shows
the results in the case in which the miners’ mean arrival rate
per BS varies from � ¼ 2 to � ¼ 22miners/hour and themin-
ers’ mean departure rate per BS is fixed to m ¼ 2 miners/
hour. Fig. 7 shows the results in the case when the miners’
mean departure rate per BS varies fromm ¼ 2 to m ¼ 22min-
ers/hour and the miners’ mean arrival rate per BS is fixed to
� ¼ 22 miners/hour. Thus, in both cases, we have � � m. As
such, the expected number of miners is growing at each
stage. Note that by increasing � or decreasing m, we increase
not only the expectation of the number of miners but also its
standard deviation. That is, deviations in the number of min-
ers are higher for the larger values of � or smaller values
of m. Indeed, from Figs. 6 and 7, the convergence time
increases with � and decreases with m. However, such a
growth rate is nondeterministic, i.e., has a non-zero standard
deviation that increases with the ratio �=m. Furthermore,
observe that unlike Fig. 5 showing a linear dependency of
the convergence time from the number of miners, in
Figs. 6 and 7, this dependency is concave. Hence, the
added uncertainty has a positive impact on the conver-
gence time of RL algorithms. The reason is that additional
randomness increases the exploration abilities of learners
(i.e., the SP and miners). As such, the learners are forced to
select unexplored options that can result in the better
payoffs compared to those that have been already deter-
mined as the optimal ones, which improves the perfor-
mance of RL.
Figs. 8 and 9 show the impact of the number of BSsM and
the associated number of miners N on the average payoffs of
aminer and the SP, respectively, in differentmodels after con-
vergence. The results in these figures are collected for the
number of BSs varying from M ¼ 3 to M ¼ 7 and the associ-
ated number of miners varying from N ¼ 80 to N ¼ 120.
Figs. 10, 12, 13 demonstrate the average payoffs of a miner
and the SP, respectively, in a dynamic and random scenario.
In particular, Figs. 10 and 11 show the results in the case in
which the miners’ mean arrival rate per BS varies from � ¼ 2
to � ¼ 22 miners/ hour and the miners’ mean departure rate
per BS is fixed tom ¼ 2miners/hour. Figs. 12 and 13 show the
results in the casewhen themean departure rate per BS varies
from m ¼ 2 to m ¼ 22 miners/hour and the mean arrival rate
per BS is fixed to � ¼ 22 miners/hour. From the figures, the
Fig. 5. Convergence time of hierarchical RL and its components as a
function of the number of miners.
Fig. 6. Convergence time of hierarchical RL and its components as a
function of miners’ mean arrival rate per BS.
Fig. 7. Convergence time of hierarchical RL and its components as a
function of miners’ mean departure rate per BS.
Fig. 8. Average miner’s payoff as a function of the number of miners.
ASHERALIEVA AND NIYATO: LEARNING-BASED MOBILE EDGE COMPUTING RESOURCE MANAGEMENT TO SUPPORT PUBLIC BLOCKCHAIN ... 1105
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
average miner’s payoff decreases with N and �=m. On the
contrary, the SP’s payoff increases with N and �=m. The rea-
son is that when N and �=m are increasing, the chances of
winning a mining reward are reducing, which means that
there is a growing number of miners that pay for MEC serv-
ices without receiving a reward. This results in the decreasing
miners’ payoffs and increasing SP’s payoff. Furthermore,
observe that the uncertainty added in a dynamic scenario
evaluated in Figs. 10, 12, 13 has a positive impact on the algo-
rithms’ performance because of the reason mentioned before.
That is, the additional randomness increases the exploration
abilities of learners and, hence, may result in the increased
payoffs of theminers and the SP.
Figs. 14 and 15 show the impact of the mean delay factor,
i.e., network delay, on the averageminer’s payoff and payoff
of the SP, respectively. The figures present the results of
simulations in which the delay factor is modeled according
to the exponential distribution (as in [48]) with the fixed
expected value varying from 10 to 110 ms. Observe the aver-
age miner’s payoff decreases exponentially with the mean
delay factor. The reason is that growing network delays
increase the orphaning probability (see equation (3)). On the
other hand, when the orphaning probability increases, the
probability of winning a mining reward reduces (see equa-
tion (1)). Thus, given the same hash rates, miners’ chances to
win a reward reduce when the network delay increases. As
Fig. 9. Payoff of the SP as a function of the number of miners.
Fig. 10. Average miner’s payoff as a function of miners’ mean arrival rate
per BS.
Fig. 11. Payoff of the SP as a function of miners’ mean arrival rate per BS.
Fig. 12. Average miner’s payoff as a function of miners’ mean departure
rate per BS.
Fig. 13. Payoff of the SP as a function of miners’ mean departure rate
per BS.
Fig. 14. Average miner’s payoff depending on the mean delay factor.
1106 IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 20, NO. 3, MARCH 2021
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
such, there is a growing number of miners that pay for MEC
services without receiving a reward. This leads to decreased
miners’ payoffs. On the other hand, the impact of the net-
work delay on the SP’s payoff is smaller than that for the
miners. In particular, the average SP’s payoff decreases loga-
rithmically with the mean delay factor. The reason is that the
delay do not affect the SP’s payoff directly, since miners pay
for their hash rates, and not the block throughputs. Yet, the
SP’s payoff is affected by the delay factor indirectly. To see
why, recall that theminers have limited budgets. That is, the SP
can only assign such a price which is affordable by the miners.
However, when the miner’s chances of winning a reward
reduce, its expected budget also reduces. As a result, the SP
must assign a lower price to miners to assure that they can
afford to pay such a price. This leads to a decreased SP’s payoff.
The impact of the initial miner’s budget on its payoff in
different models after convergence is shown in Fig. 16.
Observe that the miner’s payoff increases with its initial
budget. This is due to the fact that when the budget is small,
the miner cannot afford to buy a higher hash rate, which
decreases its winning probability and results in a decreased
payoff. Figs. 17 and 18 present the average miner’s payoffs
and payoff of the SP, respectively, depending on the
expected (rather than deterministic) initial miner’s budget
after convergence. Observe the average miner’s payoff
decreases, whereas, the average payoff of the SP increases
with the expected budget. Such results can be explained as
follows. When the expected initial miner’s budget is low, the
SP assigns a lower price per ECU which reduces the SP’s
payoff but increases the payoffs of miners. To summarize, all
presented simulation results show that after convergence,
the proposed algorithms exhibit the performance close to
that achieved in the optimal Complete Infomodel.
7 CONCLUSION
We have proposed a novel hierarchical learning framework
for a stochastic Stackelberg game under private information.
The gamemodels the interactions between the SP andminers
in a public blockchain network implemented in the mobile
edge computing system. Within the framework, we have
developed the exact and approximate reinforcement learn-
ing algorithms based on the fully- and partially-observable
MDPs for the decision making of the SP and miners. We
have shown that the proposed exact and approximate learn-
ing procedures converge to stable states where the miners’
hash rate decisions are the best responses to the optimal price
of edge computing services.
The proposed framework can be easily extended to
include multiple SPs, i.e., when the miners are free to select
their associated SPs. In this case, each SP (i.e., the leader)
assigns the price per unit hash rate by considering not only
budget constraints of its associated miners (i.e., the fol-
lowers) but also the fact that if the assigned price is too high,
its followers can switch to another SP (which will reduce its
payoff). As such, the long-term payoff of the SP will also
depend on the price assignments (i.e., actions) of other SPs.
That is, not only the miners compete against each other, but
also the SPs. Then, the stochastic state in the MDP of the SP
will include the actions of other SPs in addition to actions,
Fig. 15. Payoff of the SP depending on the mean delay factor.
Fig. 16. Miner’s payoff depending on the initial miner’s budget.
Fig. 17. Average miner’s payoff depending on the expected miner’s
budget.
Fig. 18. Payoff of the SP depending on the expected miner’s budget.
ASHERALIEVA AND NIYATO: LEARNING-BASED MOBILE EDGE COMPUTING RESOURCE MANAGEMENT TO SUPPORT PUBLIC BLOCKCHAIN ... 1107
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
rewards, and initial budgets of miners. On the other hand,
the actions in partially-observable MDPs of the miners will
be represented by theminers’ hash rates and SP selections.
ACKNOWLEDGMENTS
This work was supported in part by the National Natural Sci-
ence Foundation of China Project No. 61950410603, Singapore
NRF National Satellite of Excellence, Design Science and
Technology for Secure Critical Infrastructure NSoE DeST-
SCI2019-0007, A	STAR-NTU-SUTD Joint Research Grant Call
on Artificial Intelligence for the Future of Manufacturing
RGANS1906, WASP/NTUM4082187 (4080), Singapore MOE
Tier 1 2017-T1-002-007 RG122/17, MOE Tier 2 MOE2014-T2-
2-015 ARC4/15, Singapore NRF2015-NRF-ISF001-2277, and
Singapore EMAEnergy ResilienceNRF2017EWT-EP003-041.
REFERENCES
[1] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,”
2008, [Online]. Available: http://bitcoin.org/bitcoin.pdf.
[2] K. Christidis andM. Devetsikiotis, “Blockchains and smart contracts
for the internet of things,” IEEEAccess, vol. 4, pp. 2292–2303, 2016.
[3] F. Tschorsch and B. Scheuermann, “Bitcoin and beyond: A techni-
cal survey on decentralized digital currencies,” IEEE Commun.
Surveys Tuts., vol. 18, no. 3, pp. 2084–2123, 3Q 2016.
[4] K. Yeow, A. Gani, R. W. Ahmad, J. J. P. C. Rodrigues, and K. Ko,
“Decentralized consensus for edge-centric internet of things: A
review, taxonomy, and research issues,” IEEE Access, vol. 6,
pp. 1513–1524, 2018.
[5] W. Wang et al., “A survey on consensus mechanisms and mining
management in blockchain networks,” 2018, arXiv:1805.02707.
[6] N. Herbaut and N. Negru, “A model for collaborative blockchain-
based video delivery relying on advanced network services
chains,” IEEE Commun. Mag., vol. 55, no. 9, pp. 70–76, Sep. 2017.
[7] J. Kang et al., “Enabling localized peer-to-peer electricity trading
among plug-in hybrid electric vehicles using consortium block-
chains,” IEEE Trans. Ind. Informat., vol. 13, no. 6, pp. 3154–3164,
Dec. 2017.
[8] Z. Xiong et al., “When mobile blockchain meets edge computing,”,
2017, arXiv:1711.05938.
[9] Z. Xiong et al., “Cloud/fog computing resource management and
pricing for blockchain networks,” IEEE Internet Things J., vol. 6,
no. 3, pp. 4585–4600, Jun. 2019.
[10] A. Asheralieva, “Optimal computational offloading and content
caching in wireless heterogeneous mobile edge computing sys-
tems with hopfield neural networks,” IEEE Trans. Emerging Topics
Comput. Intell., to be published, doi: 10.1109/TETCI.2019.2892733.
[11] N. Abbas, Y. Zhang, A. Taherkordi, and T. Skeie, “Mobile edge com-
puting: A survey,” IEEE Internet Things J., vol. 5, no. 1, pp. 450–465,
Feb. 2018.
[12] R. Yang, F. R. Yu, P. Si, Z. Yang, and Y. Zhang, “Integrated Block-
chain and edge computing systems: A survey, some research
issues and challenges,” IEEE Commun. Surveys. Tuts., vol. 21, no. 2,
pp. 1508–1532, 2Q 2019.
[13] N. Houy, “The Bitcoin mining game,” Ledger, vol. 1, pp. 53–68, Dec.
2016, Accessed: Dec. 24, 2019. [Online]. Available: https://
ledgerjournal.org/ojs/index.php/ledger/article/view/13
[14] N. Fotiou and G. C. Polyzos, “Decentralized name-based security
for content distribution using blockchains,” in Proc. IEEE Conf.
Comput. Commun. Workshops, 2016, pp. 415–420.
[15] T. Jin et al., “Blockndn: A bitcoin blockchain decentralized system
over named data networking,” in Proc. IEEE Int. Conf. Ubiquitous
Future Netw., 2017, pp. 75–80.
[16] N. Herbaut and N. Negru, “A model for collaborative blockchain-
based video delivery relying on advanced network services
chains,” IEEE Commun. Mag., vol. 55, no. 9, pp. 70–76, Sep. 2017.
[17] W. Wang et al., “Decentralized caching for content delivery based
on blockchain: A game theoretic perspective,” in Proc. IEEE Int.
Conf. Commun., 2018, pp. 1–6.
[18] K. Kotobi and S. G. Bil�en, “Blockchain-enabled spectrum access in
cognitive radio networks,” in Proc. IEEE Wireless Telecommun.
Symp., 2017, pp. 1–6.
[19] A. Lei, H. Cruickshank, Y. Cao, P. Asuquo, C. P. A. Ogah, and
Z. Sun, “Blockchain-based dynamic key management for hetero-
geneous intelligent transportation systems,” IEEE Internet Things
J. vol. 4, no. 6, pp. 1832–1843, Dec. 2017.
[20] J. Kang, Z. Xiong, D, Niyato, D. Ye, D. I. Kim, and J. Zhao, “Toward
secure blockchain-enabled internet of vehicles: Optimizing consen-
susmanagement using reputation and contract theory,” IEEE Trans.
Veh. Technol., vol. 68, no. 3, pp. 2906–2920,Mar. 2019.
[21] W. Wang et al., “A survey on consensus mechanisms and mining
strategy management in blockchain networks,” in IEEE Access,
vol. 7, pp. 22328–22370, Jan. 2019.
[22] Y. Jiao et al., “Social welfare maximization auction in edge com-
puting resource allocation for mobile blockchain,” in Proc. IEEE
Int. Conf. Commun., 2018, pp. 1–6.
[23] N. C. Luong et al., “Optimal auction for edge computing resource
management in mobile blockchain networks: A deep learning
approach,” in Proc. IEEE Int. Conf. Commun., 2018, pp. 1–6.
[24] C. Decker and R. Wattenhofer, “Information propagation in the
bitcoin network,” in Proc. IEEE P2P, 2013, pp. 1–10.
[25] S. V. Albrecht, J. W. Crandall, and S. Ramamoorthy, “Belief and
truth in hypothesised behaviours,” Artif. Intell., vol. 235, no. 2016,
pp. 63–94, 2016.
[26] A. Asheralieva, “Bayesian reinforcement learning-based coalition
formation for distributed resource sharing by device-to-device
users in heterogeneous cellular networks,” IEEE Trans. Wireless
Commun., vol. 16, no. 8, pp. 5016–5032, Aug. 2017.
[27] A. Asheralieva and D. Niyato, “Hierarchical game-theoretic and
reinforcement learning framework for computational offloading in
UAV-enabled mobile edge computing networks with multiple ser-
vice providers,” IEEE Internet Things J., vol. 6, no. 5, pp. 8753–8769,
Oct. 2019.
[28] M. Ghavamzadeh et al., “Bayesian reinforcement learning: A
survey,” Foundations Trends Mach. Learn., vol. 8, no. 5-6, 2016,
pp. 359–483.
[29] A. K. Dixit et al., Investment Under Uncertainty, Princeton, NJ, USA:
Princeton university press, 1994.
[30] R. C. Merton, “An intertemporal capital asset pricing model,”
Econometrica: J. Econometric Soc., 1973, pp. 867–887.
[31] M. J. Osborne and A. Rubenstein. A Course in Game Theory.
Cambridge, MA, USA: MIT Press, 1994.
[32] S. Sorin, “Stochastic games with incomplete information,” Stochastic
Games andApplications. Berlin, Germany: Springer, 2003, pp. 375–395.
[33] D. Rosenberg, E. Solan, and N. Vieille, “Stochastic games with a
single controller and incomplete information,” SIAM J. Control
Optim., vol. 43, no. 1, pp. 86-1–10, 2004.
[34] P. Poupart et al., “An analytic solution to discrete Bayesian rein-
forcement learning,” in Proc. ACM Int. Conf. Mach. Learn., 2006,
pp. 697–704.
[35] R. Sutton and A. Barto. Reinforcement Learning: An Introduction.
Cambridge, MA, USA: MIT Press, 1998.
[36] M. Geist and O. Pietquin, “A brief survey of parametric value
function approximation,” Rapport interne, Sup�elec, Sep. 2010.
[37] M. Irodova and R. H. Sloan, “Reinforcement learning and function
approximation,” in Proc. FLAIRS Conference, 2005, pp. 455–460.
[38] F. S. Melo, S. P. Meyn, and M. I. Ribeiro, “An analysis of reinforce-
ment learning with function approximation,” in Proc. ACM Int.
Conf. Mach. learn., 2008, pp. 664–671.
[39] J. Schmidhuber, “Deep learning in neural networks: An over-
view,”Neural Netw., vol. 61, pp. 85–117, 2015.
[40] L. Deng and D. Yu, “Deep learning: Methods and applications,”
Foundactions Trends Signal Process., vol. 7, no. 3–4, pp 197–387,
2014.
[41] S. Gu et al., “Continuous deep q-learning with model-based acceler-
ation,” inProc. ACM Int. Conf. Mach. Learn., Jun. 2011, pp. 2829–2838.
[42] T. Hester et al., “Deep q-learning from demonstrations,” in Proc.
AAAI Conf. Artif. Intell., 2018, pp. 3223–3230.
[43] T. P. Lillicrap et al., “Continuous control with deep reinforcement
learning,” 2015, arXiv:1509.02971.
[44] OPNET simulation and development tool, Accessed: Dec. 24,
2019. [Online]. Available: http://www.opnet.com.
[45] Evolved universal terrestrial radio access (E-UTRA) and evolved
universal terrestrial radio access network (E-UTRAN); overall
description; Stage 2, 3GPP TS 36.300 (Release 13), 2016.
[46] T. Camp, J. Boleng, and V. Davies, “A survey of mobility models
for ad hoc network research,” Wireless Commun. Mobile Comput.,
vol. 2, no. 5, pp. 1–27, Sep. 2002.
1108 IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 20, NO. 3, MARCH 2021
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
http://bitcoin.org/bitcoin.pdf
http://dx.doi.org/10.1109/TETCI.2019.2892733
https://ledgerjournal.org/ojs/index.php/ledger/article/view/13
https://ledgerjournal.org/ojs/index.php/ledger/article/view/13
http://www.opnet.com
[47] T. Imai, “Essays in revealed preference theory and behavioral eco-
nomics,” PhD diss., Division Humanities Social Sci., CA Inst.
Technol., 2016.
[48] G. Fanti et al., “Barracuda: The power of ‘-polling in proof-of-
stake blockchains,” in Proc. ACM Int. Symp. Mobile Ad Hoc Netw.
Comput., 2019, pp. 351–360.
[49] Z. Han et al., Game Theory in Wireless and Communication Networks:
Theory, Models, and Applications. Cambridge, UK: Cambridge
University Press, 2012.
[50] M. Schatzman, Numerical Analysis: A Mathematical Introduction. 1st
ed. Oxford, UK: Oxford University Press, 2002.
Alia Asheralieva received the BS degree from
Kyrgyz Technical University, Bishkek, Kyrgyzstan,
in 2004, the ME degree from the Asian Institute of
Technology, Bangkok, Thailand, in 2007, and the
PhD degree from the University of Newcastle, Call-
aghan NSW, Australia, in 2014. In 2015 and 2016,
she was a research assistant professor in the
Graduate School of Information Science and Tech-
nology at Hokkaido University, Sapporo, Japan.
From 2017, she was a postdoctoral research fellow
in the Wireless Networks and Decision Systems
Group of the Information Systems Technology and Design Pillar, Singa-
pore University of Technology and Design. She is currently an assistant
professor with the Department of Computer Science and Engineering of
the Southern University of Science and Technology in Shenzhen, China.
Her main research interests include many areas of communications and
networking, including cognitive radio networks, heterogeneous networks,
device-to-device and Internet of Things communications, cloud/edge/fog
computing, cross-layer resource allocation and optimization, congestion
control and routing, game theory, computational and artificial intelligence
for wireless networks, queuing theory, simulation and network modelling,
QoS, and performance evaluation.
Dusit Niyato (M’09–SM’15–F’17) received the
BEng degree from the King Mongkut’s Institute of
Technology Ladkrabang, Thailand, in 1999, and
the PhD degree in electrical and computer engi-
neering from the University of Manitoba, Canada,
in 2008. He is currently a professor with the School
of Computer Science and Engineering, Nanyang
Technological University, Singapore. His research
interests include energy harvesting for wireless
communication, Internet of Things, and sensor
networks.
" For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/csdl.
ASHERALIEVA AND NIYATO: LEARNING-BASED MOBILE EDGE COMPUTING RESOURCE MANAGEMENT TO SUPPORT PUBLIC BLOCKCHAIN ... 1109
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:24 UTC from IEEE Xplore.  Restrictions apply. 
<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles true
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.4
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /sRGB
  /DoThumbnails true
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo true
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts true
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
    /Algerian
    /Arial-Black
    /Arial-BlackItalic
    /Arial-BoldItalicMT
    /Arial-BoldMT
    /Arial-ItalicMT
    /ArialMT
    /ArialNarrow
    /ArialNarrow-Bold
    /ArialNarrow-BoldItalic
    /ArialNarrow-Italic
    /ArialUnicodeMS
    /BaskOldFace
    /Batang
    /Bauhaus93
    /BellMT
    /BellMTBold
    /BellMTItalic
    /BerlinSansFB-Bold
    /BerlinSansFBDemi-Bold
    /BerlinSansFB-Reg
    /BernardMT-Condensed
    /BodoniMTPosterCompressed
    /BookAntiqua
    /BookAntiqua-Bold
    /BookAntiqua-BoldItalic
    /BookAntiqua-Italic
    /BookmanOldStyle
    /BookmanOldStyle-Bold
    /BookmanOldStyle-BoldItalic
    /BookmanOldStyle-Italic
    /BookshelfSymbolSeven
    /BritannicBold
    /Broadway
    /BrushScriptMT
    /CalifornianFB-Bold
    /CalifornianFB-Italic
    /CalifornianFB-Reg
    /Centaur
    /Century
    /CenturyGothic
    /CenturyGothic-Bold
    /CenturyGothic-BoldItalic
    /CenturyGothic-Italic
    /CenturySchoolbook
    /CenturySchoolbook-Bold
    /CenturySchoolbook-BoldItalic
    /CenturySchoolbook-Italic
    /Chiller-Regular
    /ColonnaMT
    /ComicSansMS
    /ComicSansMS-Bold
    /CooperBlack
    /CourierNewPS-BoldItalicMT
    /CourierNewPS-BoldMT
    /CourierNewPS-ItalicMT
    /CourierNewPSMT
    /EstrangeloEdessa
    /FootlightMTLight
    /FreestyleScript-Regular
    /Garamond
    /Garamond-Bold
    /Garamond-Italic
    /Georgia
    /Georgia-Bold
    /Georgia-BoldItalic
    /Georgia-Italic
    /Haettenschweiler
    /HarlowSolid
    /Harrington
    /HighTowerText-Italic
    /HighTowerText-Reg
    /Impact
    /InformalRoman-Regular
    /Jokerman-Regular
    /JuiceITC-Regular
    /KristenITC-Regular
    /KuenstlerScript-Black
    /KuenstlerScript-Medium
    /KuenstlerScript-TwoBold
    /KunstlerScript
    /LatinWide
    /LetterGothicMT
    /LetterGothicMT-Bold
    /LetterGothicMT-BoldOblique
    /LetterGothicMT-Oblique
    /LucidaBright
    /LucidaBright-Demi
    /LucidaBright-DemiItalic
    /LucidaBright-Italic
    /LucidaCalligraphy-Italic
    /LucidaConsole
    /LucidaFax
    /LucidaFax-Demi
    /LucidaFax-DemiItalic
    /LucidaFax-Italic
    /LucidaHandwriting-Italic
    /LucidaSansUnicode
    /Magneto-Bold
    /MaturaMTScriptCapitals
    /MediciScriptLTStd
    /MicrosoftSansSerif
    /Mistral
    /Modern-Regular
    /MonotypeCorsiva
    /MS-Mincho
    /MSReferenceSansSerif
    /MSReferenceSpecialty
    /NiagaraEngraved-Reg
    /NiagaraSolid-Reg
    /NuptialScript
    /OldEnglishTextMT
    /Onyx
    /PalatinoLinotype-Bold
    /PalatinoLinotype-BoldItalic
    /PalatinoLinotype-Italic
    /PalatinoLinotype-Roman
    /Parchment-Regular
    /Playbill
    /PMingLiU
    /PoorRichard-Regular
    /Ravie
    /ShowcardGothic-Reg
    /SimSun
    /SnapITC-Regular
    /Stencil
    /SymbolMT
    /Tahoma
    /Tahoma-Bold
    /TempusSansITC
    /TimesNewRomanMT-ExtraBold
    /TimesNewRomanMTStd
    /TimesNewRomanMTStd-Bold
    /TimesNewRomanMTStd-BoldCond
    /TimesNewRomanMTStd-BoldIt
    /TimesNewRomanMTStd-Cond
    /TimesNewRomanMTStd-CondIt
    /TimesNewRomanMTStd-Italic
    /TimesNewRomanPS-BoldItalicMT
    /TimesNewRomanPS-BoldMT
    /TimesNewRomanPS-ItalicMT
    /TimesNewRomanPSMT
    /Times-Roman
    /Trebuchet-BoldItalic
    /TrebuchetMS
    /TrebuchetMS-Bold
    /TrebuchetMS-Italic
    /Verdana
    /Verdana-Bold
    /Verdana-BoldItalic
    /Verdana-Italic
    /VinerHandITC
    /Vivaldii
    /VladimirScript
    /Webdings
    /Wingdings2
    /Wingdings3
    /Wingdings-Regular
    /ZapfChanceryStd-Demi
    /ZWAdobeF
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 150
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages false
  /ColorImageDownsampleType /Bicubic
  /ColorImageResolution 150
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.40
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 150
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages false
  /GrayImageDownsampleType /Bicubic
  /GrayImageResolution 300
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.40
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 1200
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages false
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False
  /CreateJDFFile false
  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Suggested"  settings for PDF Specification 4.0)
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice