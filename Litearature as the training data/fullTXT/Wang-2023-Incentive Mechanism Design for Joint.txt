Incentive Mechanism Design for Joint Resource Allocation in Blockchain-Based Federated Learning
1536 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 34, NO. 5, MAY 2023
Incentive Mechanism Design for Joint Resource
Allocation in Blockchain-Based Federated Learning
Zhilin Wang , Graduate Student Member, IEEE, Qin Hu , Ruinian Li , Minghui Xu ,
and Zehui Xiong , Member, IEEE
Abstract—Blockchain-based federated learning (BCFL) has re-
cently gained tremendous attention because of its advantages, such
as decentralization and privacy protection of raw data. However,
there has been few studies focusing on the allocation of resources
for the participated devices (i.e., clients) in the BCFL system.
Especially, in the BCFL framework where the FL clients are also the
blockchain miners, clients have to train the local models, broadcast
the trained model updates to the blockchain network, and then
perform mining to generate new blocks. Since each client has a
limited amount of computing resources, the problem of allocating
computing resources to training and mining needs to be carefully
addressed. In this paper, we design an incentive mechanism to help
the model owner (MO) (i.e., the BCFL task publisher) assign each
client appropriate rewards for training and mining, and then the
client will determine the amount of computing power to allocate
for each subtask based on these rewards using the two-stage Stack-
elberg game. After analyzing the utilities of the MO and clients, we
transform the game model into two optimization problems, which
are sequentially solved to derive the optimal strategies for both the
MO and clients. Further, considering the fact that local training
related information of each client may not be known by others, we
extend the game model with analytical solutions to the incomplete
information scenario. Extensive experimental results demonstrate
the validity of our proposed schemes.
Index Terms—Blockchain, federated learning, game theory,
incentive mechanism, resource allocation.
I. INTRODUCTION
S INCE its emergence in 2016, federated learning (FL) has
been greatly developed and widely applied in many fields,
Manuscript received 15 February 2022; revised 13 January 2023; accepted
2 March 2023. Date of publication 7 March 2023; date of current version 16
March 2023. This work was supported in part by the US NSF under Grant
CNS-2105004, in part by the National Natural Science Foundation of China
under Grant 62232010, in part by the National Research Foundation (NRF) and
Infocomm Media Development Authority through the Future Communications
Research Development Programme (FCP), in part by the SUTD under Grant
SRG-ISTD-2021-165, in part by the SUTD-ZJU IDEA Grant (SUTD-ZJU (VP)
202102), and in part by the Ministry of Education, Singapore, through its SUTD
Kickstarter Initiative (SKI 20210204). Recommended for acceptance by M.
Zhao. (Corresponding Author: Qin Hu.)
Zhilin Wang and Qin Hu are with the Department of Computer and Informa-
tion Science, Indiana University-Purdue University Indianapolis, Indianapolis,
IN 46202 USA (e-mail: wangzhil@iu.edu; qinhu@iu.edu).
Ruinian Li is with the Department of Computer Science, Bowling Green State
University, Bowling Green, OH 43551 USA (e-mail: Lir@bgsu.edu).
Minghui Xu is with the School of Computer Science and Technology, Shan-
dong University, Jinan 250100, China (e-mail: Mhxu@sdu.edu.cn).
Zehui Xiong is with the Pillar of Information Systems Technology and
Design, Singapore University of Technology Design, Singapore 487372 (e-mail:
Zehui_xiong@sutd.edu.sg).
Digital Object Identifier 10.1109/TPDS.2023.3253604
such as Internet of Things [1], [2], [3], smart transportation [4],
[5] and healthcare [6], [7], [8]. One of the most important advan-
tages of FL is that there is no transmission of raw data from local
devices (i.e., clients) to the centralized server for model training;
instead, by training models on clients and aggregating all local
models, FL significantly reduces the possibility of leaking data
privacy to a large extent [9]. However, some challenges still may
restrain the implementation of FL, e.g., the risk of the single
point of failure, malicious attacks from participated clients, and
the lack of participation incentives [10], [11], [12], [13].
In recent years, researchers resort to blockchain technology
to tackle the challenges of FL, where the blockchain system
usually works as a decentralized system to provide incentives
and data verification [14], [15], [16], [17], [18]. The combination
of blockchain and FL is termed blockchain-based FL (BCFL). In
the BCFL framework, model updates submitted by clients will
be verified by miners before the global aggregation algorithm
is conducted. Once the global model is obtained, it will be
updated into the main chain that all qualified participants can
access. Though BCFL can partially address the aforementioned
challenges of traditional FL, some remaining issues still need to
be addressed.
One of the most critical problems in BCFL is the resource
allocation of clients. First, clients in the BCFL system are
heterogeneous with different computational resources, and they
usually have other tasks to complete while handling the BCFL
task, so a universal resource allocation scheme for all clients
is not practical. In addition, the whole system may not work
effectively and sustainably if no reasonable rewards are allocated
to clients. Furthermore, both training and mining in the frame-
work of BCFL consume a significant amount of resources and
time, and thus it is difficult for clients to appropriately allocate
their limited resources to ensure the performance of the global
model during the required time period. Lastly, since the system
may not know the amount of training data each client owns, it
can be challenging for the model owner (MO), i.e., the BCFL
task publisher, to make proper decisions regarding the reward
distribution.
There exist very few studies that tackle the above chal-
lenges [19], [20]. They are mainly based on two assumptions
that are not practical: 1) all clients have identical computational
power and data volume; and 2) the system knows all the informa-
tion about the computation resources of clients. Besides, there
are several studies focusing on resource allocation [21], [22]
and incentive mechanism design [23], [24] in BCFL. But these
1045-9219 © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
https://orcid.org/0000-0002-0188-0332
https://orcid.org/0000-0002-8847-8345
https://orcid.org/0000-0003-4452-0502
https://orcid.org/0000-0003-3675-3461
https://orcid.org/0000-0002-4440-941X
mailto:wangzhil@iu.edu
mailto:qinhu@iu.edu
mailto:Lir@bgsu.edu
mailto:Mhxu@sdu.edu.cn
mailto:Zehui_xiong@sutd.edu.sg
WANG et al.: INCENTIVE MECHANISM DESIGN FOR JOINT RESOURCE ALLOCATION IN BLOCKCHAIN-BASED FEDERATED LEARNING 1537
schemes cannot address above mentioned challenges. To fill the
gap, we propose an incentive mechanism for joint resource allo-
cation on clients in BCFL that can be applied to the incomplete
information scenario without two impractical assumptions.
For the first challenge regarding the unbalanced distribution
of resources on clients, we let clients decide how much computa-
tional power they are willing to devote to the training and mining
tasks. By this means, clients can flexibly allocate computation
resources for their own tasks. In addition, training and mining
are performed sequentially in our model, and the amount of
computational power devoted to these two tasks can be different.
To overcome the second challenge of motivating clients to
join BCFL, we design an incentive mechanism to reward clients.
Training and mining are two different tasks that require a dif-
ferent amount of computational power; thus, the rewards should
also be different. To ensure a fair distribution of rewards to all
clients, we employ the approach of Shapley Value (SV) [25] to
determine clients’ contributions in the training process, which
will affect the constraints in their respective optimization prob-
lems.
To address the last two challenges, we build the Stackelberg
game model under the complete and incomplete information sit-
uations, which are solved separately but with different insights.
Our system can make optimal decisions based on the derived
optimal solutions in different information conditions.
In summary, our contributions can be summarized as below:
� In the BCFL system with heterogenous clients, we model
the resource allocation problem as a two-stage Stackelberg
game to help the MO make decisions on assigning how
many rewards to each client for training and mining and
to assist clients in determining the corresponding amount
of computational power to be devoted in each subtask, via
maximizing their respective utilities.
� In order to maintain the stability and sustainability of the
whole BCFL system, we design a fair reward allocation
scheme inspired by SV to calculate the rewards for clients
based on their contributions to the training process.
� Considering that the training related information of devices
may not be known to others in the practical application sce-
nario, we further study the resource allocation mechanism
under the incomplete information situation and derive the
optimal solutions accordingly.
� We test our proposed resource allocation mechanisms
through extensive experiments. The experimental results
show that these mechanisms are effective.
The rest of this paper is organized as follows. We introduce the
system model and problem formulation based on the two-stage
Stackelberg game in Section II. The detailed models and solu-
tions under complete and incomplete information scenarios are
reported in Sections III and IV, respectively. Experimental eval-
uations are presented in Section V. We present the related work
in Section VI. Finally, we conclude this paper in Section VII.
II. SYSTEM MODEL AND PROBLEM FORMULATION
In this section, we will illustrate the system model of our
considered blockchain-based federated learning (BCFL) and
TABLE I
KEY NOTATIONS
then formulate the problem from the perspective of resource allo-
cation and incentive mechanism design based on the Stackelberg
game. For convenience, we list the key notations in Table I.
A. System Overview
Inspired by [26], we consider one of the most widely used
BCFL frameworks, i.e., the fully coupled BCFL system, where
each client of FL also works as the blockchain node and thus
has to handle both FL-related and blockchain-related computing
activities. Without loss of generality, we term the local devices as
clients and call the FL-related computing activities training and
blockchain-related computing activities mining. The topology
of the BCFL system is shown in Fig. 1, which is consisted of
multiple clients and one blockchain system. The set of clients can
be denoted as N = {1, . . . , i, . . . , N} with N representing the
total number of clients in the BCFL system. The task requester,
i.e., the model owner (MO), can access the BCFL system and
publish tasks, aiming to receive a well-trained final global model
from the BCFL system. After the FL task is published on the
blockchain, clients train their local models and broadcast the
obtained model updates to the blockchain network once the local
training process is finished.
It is worth noting that the BCFL system considered in this
paper can adopt any blockchain consensus depending on the
application scenario. Specifically, Proof of Work (PoW) [27] can
be employed as the consensus protocol to provide the security
guarantee, while other lightweight consensus, such as Practical
Byzantine Fault Tolerance (PBFT) [28], Delegated Proof of
Stake (DPoS) [29], and Raft [30], can also be implemented
to reduce the resource consumption. There will be a generator
of new block after running blockchain consensus in the BCFL
system, which will also be responsible for aggregating the local
models to derive the global model. In addition, the considered
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
1538 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 34, NO. 5, MAY 2023
Fig. 1. An illustration of the BCFL system, which is the fully coupled BCFL
with the FL clients also being the blockchain nodes. First, clients train local
models and submit their model updates to the blockchain network; then, clients
start mining to determine a block generator, which will also be responsible for
global model aggregation.
BCFL system is compatible with any type of blockchain; in other
words, the BCFL system can also apply public blockchain and
private blockchain in addition to the consortium blockchain.
We can describe the workflow of our BCFL system after
the MO publishes the task as below: 1) once receiving the FL
task, each client trains the local model and then broadcasts the
model updates to the blockchain network; 2) clients run the
consensus protocol and determine a new block generator who
will aggregate the received model updates to derive a new global
model; 3) then the block generator will create a new block,
containing all the submitted updates and the new global model.
In our considered BCFL system, each client would be respon-
sible for training and mining, which is defined as the BCFL task.
These two procedures are not parallel; in other words, mining
can only starts after the training is completed. In practice, clients
may need to handle other tasks besides the BCFL task. Given
that their computational resources are limited, they have to al-
locate available resources to both training and mining carefully.
Besides, to motivate clients to complete the BCFL task, the MO
usually provides a certain amount of rewards; however, since
the MO’s budget is limited, the distribution of rewards to clients
becomes challenging to obtain a high-performance global model
in an efficient manner.
B. Utility Models
Based on the above analysis, we design an incentive mech-
anism to assist the resource allocation for our proposed BCFL
system, so that the MO can distribute rewards to clients properly
and get a well-trained global model, and clients can allocate the
computing resources (i.e., CPU cycles per second) to training
and mining reasonably and gain satisfying rewards. In this part,
we build the utility models of both the MO and clients from
resource allocation and incentive mechanism perspectives.
1) Client’s Utility: We assume that the maximum number
of client i’s CPU cycles per second is qi, and the number of
CPU cycles per second used to train and mine are qti and qmi,
respectively. Then we have qti, qmi ≤ qi. Let π be the number
of training iterations for clients during one round of BCFL to
submit a model update, which is usually fixed for all clients.
Let Di be the number of the data size of client i, and di be
the number of CPU cycles used for training each data sample.
Therefore, we can define the total CPU cycles required to finish
the local training to generate model updates as
μi = πdiDi.
Since any client i can decide its CPU cycles used to train the
local model, the time used to finish the local training varies for
each client. We can calculate the time spent on training for client
i via Tti =
μi
qti
. Besides, we denote the total CPU cycles used to
mine for each client as ψ, which is the same for all clients since
mining a new block in the blockchain system usually consumes
fixed computational resources. Thus, the time spent on mining
can be calculated as
Tmi =
ψ
qmi
.
So we can have the total time cost of client i to finish a round
of BCFL task as Ti = Tti + Tmi. Since it is impossible to let
Tti and Tmi be limitless according to the convergence time
requirement, we denote the upper bound of time consumption
in one round of BCFL by T . Thus, we have Ti ≤ T .
In order to encourage clients to join BCFL, the MO provides
some rewards to clients, where the prices per second for training
and mining are denoted as pti and pmi, respectively. Clients can
allocate unit CPU cycles for training and mining based on the
unit prices given by the MO. Then the rewards of client i for
training and mining to generate one round of local model updates
are calculated by
Rti = Ttipti,
Rmi = Tmipmi.
Thus, the total rewards for client i in one round of BCFL is
Ri = Rti +Rmi.
Based on a widely used model in [31] for calculating the com-
putational energy consumption, we can respectively calculate
the energy costs for training and mining as
Cti = ρiμiq
2
ti,
Cmi = ρiψq
2
mi,
where ρi is the parameter correlated to the chip architecture. In
this way, the total cost1 can be calculated as Ci = Cti + Cmi.
Finally, we can obtain the utility of client i in one round of
BCFL as
Ui = Ri − Ci
1As for the communication cost, since the sizes of the clients’ submissions
are the same, we can consider it as a constant value, which cannot be optimized
anymore and thus is omitted here.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
WANG et al.: INCENTIVE MECHANISM DESIGN FOR JOINT RESOURCE ALLOCATION IN BLOCKCHAIN-BASED FEDERATED LEARNING 1539
=
μi
qti
pti +
ψ
qmi
pmi − ρiμiq2ti − ρiψq2mi. (1)
2) MO’s Utility: The main concerns related to the utility
of the MO are the performance of the global model, the time
consumption, and the rewards paid to all participants in each
round of BCFL, where the first one is a sort of revenue and the
last two are related to the cost for the MO.
Generally, the performance of an ML model will be affected
by the number of CPU cycles spent on training. Thus, we
define the performance of the global model after one round
of local training and mining as G, which can be calculated
by G = f(
∑N
i=1 μi). Here f(·) is a monotonically increasing
function, indicating that the more CPU cycles used for the local
training by all clients, the better performance of the global model
after aggregation will be achieved. As for the MO, its utility
depends on the performance of the BCFL system (G), total
time cost (
∑N
i=1(Ti)), and total rewards distributed to clients
(
∑N
i=1(Ri)). Thus, the utility of the MO in one round of BCFL
can be expressed as
Umo = f
(
N∑
i=1
μi
)
− ξ
N∑
i=1
(Ti +Ri)
=f
(
N∑
i=1
μi
)
− ξ
N∑
i=1
(
μi
qti
+
ψ
qmi
+
μi
qti
pti +
ψ
qmi
pmi
)
,
(2)
where ξ > 0 is a scalar parameter to balance the revenue and
cost.
C. Problem Formulation Using Two-Stage Stackelberg Game
According to the above analysis of our system model, client
i provides its computational power to finish BCFL tasks based
on the rewards given by the MO. In other words, the unit prices
pti and pmi determine the unit computational power qti and
qmi. We can formulate the interactions between clients and the
MO as a two-stage Stackelberg game, which is widely used for
the complete information dynamic game [32]. In this game, the
MO determines the unit prices of the CPU frequency used for
training and mining, and then client i decides its CPU cycles
per second based on the received prices, which means that the
decision of client i is impacted by the decision of the MO. In
this case, we can define the process of the two-stage Stackelberg
game as below:
� Stage I: The MO sets the unit prices per second for training
and learning for each client, i.e., pti and pmi, via maximiz-
ing its own utility, which is specifically based on its budget
and the total number of CPU cycles consumed for training
submitted by each client. Considering the distribution’s
fairness in setting prices, we need to design a fair reward
allocation scheme here.
� Stage II: After receiving the unit prices from the
MO, clients determine their corresponding computational
power, i.e., qti and qmi, through optimizing their respective
utilities.
In practical situations, qti and qmi are not independent of each
other because of time and reward budget constraints; similarly,
pti and pmi influence each other as well. Therefore, we should
consider these constraints when modeling to make the decisions
reasonable.
Recall μi in Section II-B1, and we know that μi is a variable
correlated to the data size of client i and the performance of
the corresponding device, which may not always be known
to the MO or the system. As for ψ, it can be predefined by
the system since generating a new block usually consumes a
constant amount of resources. Therefore, we can classify the
two-stage Stackelberg game into information complete and in-
complete scenarios based on whetherμi is known to the MO. The
models derived for these two scenarios are different, and hence
the strategies of the MO and clients are different accordingly,
which will be explored in Sections III and IV.
III. RESOURCE ALLOCATION WITH COMPLETE INFORMATION
In this section, we will elaborate on the expressions of the pro-
posed Stackelberg game model and the corresponding solutions
for clients and the MO in the scenario of complete information,
which means that the MO makes its decisions when μi of
each client is known as a prior. First, we propose a fair reward
allocation scheme for clients, and then we transfer the two-stage
Stackelberg game into two separate optimization problems that
are resolved sequentially. The methodology we adopt to solve the
two problems is backward induction, which requires analyzing
the optimal strategies of Stage II first and then the strategies of
Stage I.
A. Fair Reward Allocation
Before we formulate the game model, we should clarify the
fair reward allocation scheme first. In our system, we consider
that each client has an equal chance to participate in both the
training and mining processes with fair rewards. And since the
allocation of rewards to each client in training and mining sig-
nificantly impacts the system fairness and further participation
willingness, we need to design a fair reward allocation scheme.
Although we have already defined the payoff of each client
during the training and mining processes in the above section,
it is necessary to investigate their upper bounds based on the
MO’s rewards budget. And the rewards distribution should not
only be associated with the device’s computing power but also
take into account the performance of its work. On the one hand,
the reward budget of the MO and the rewards that each client can
get is limited; on the other hand, if the resources are allocated
only based on the computing power devoted, it could lead to a
situation where devices with sufficient computing power take
most of the rewards, while devices with less power cannot get
enough rewards, making the system unstable and unsustainable.
1) Upper Bound of Rewards for Mining: For simplicity, we
set a fixed total reward budget η in each round of BCFL. Since
the computational power consumed by generating a new block
is constant, with ηm denoting the upper bound of the reward for
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
1540 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 34, NO. 5, MAY 2023
mining that all clients can receive, we have
ψ
qmi
pmi ≤ Rmi = ηm
N
,
whereRmi is the upper bound of the reward for mining that each
client can get.
2) Upper Bound of Rewards for Training: Since the devices
in our BCFL system are assumed to be heterogeneous and
may have different computing capabilities, we cannot simply
distribute the rewards evenly to each client. To guarantee the
fairness of reward distribution, we allocate rewards based on the
contribution of each client in the training process. Considering
that Shapley Value (SV) [25] is a methodology that can dis-
tribute the rewards to participants according to their respective
contributions, here we apply it to facilitate reward distribution.
The SV of client i is defined as
SVi(N , v) =
∑
i/∈S,S⊆N
s!(N − s− 1)!
N !
(v(S ∪ i)− v(S)),
(3)
where S ⊆ N is a subset of clients and s = |S| is the number
of devices in the set S; v(S) is a function describing the perfor-
mance of the training result with the client set S. Then, we give
the expression of function v(S). Recall G in Section II-B2, we
can assume that v(S) is a function correlated to G and it can be
defined as
v(S) =W −
∥∥∥∥
∑s
i=1G
s
− g
∥∥∥∥
2
, (4)
whereW = maxS⊆N‖
∑s
i=1G
s − g‖2 and ‖ · ‖2 is the euclidean
norm; g is the targeted performance value. Then, we can calcu-
late the upper bound of the reward distributed to each device for
training as
Rti =
SVi(N , v)
v(N )
(η − ηm).
For each client, its rewards should not exceed the upper bound
so that we can have the following constraint
μi
qti
pti ≤ Rti.
It is worth pointing out that with a fixed reward budget of
the MO, the maximum rewards per client will decrease as the
number of clients increases.
B. Stage II: Clients Set CPU Cycles Per Second Based on Unit
Rewards
Since each client i has a limited amount of computational
resources and should follow the working rules of BCFL, the
goal of client i is to maximize its utility as follows:
Problem 1 : max : Ui,
s.t.
μi
qti
pti − ρiμiq2ti ≥ 0, (5)
ψ
qmi
pmi − ρiψq2mi ≥ 0, (6)
μi
qti
+
ψ
qmi
≤ T, ∀i ∈ N , (7)
where the first two constraints (5) and (6) mean that client i
wishes to gain non-negative payoffs in both training and mining;
and the last constraint (7) indicates that client i should finish the
working process, including training and mining, within the time
period T .
It is clear that Problem 1 is a nonlinear optimization problem
with inequality constraints, so we adopt the method of Karush-
Kuhn-Tucker (KKT) conditions to solve it. By solving Problem
1, we get the following theorem:
Theorem III.1. The optimal strategies of client i in the sce-
nario of complete information are given by
q∗ti =
(
pti
ρi
) 1
3
, (8)
q∗mi =
ψ
T − μi
(
ρi
pti
) 1
3
. (9)
Proof. The Lagrangian correlated to Problem 1 is expressed
as:
L1 =
μi
qti
pti +
ψ
qmi
pmi − ρiμiq2ti − ρiψq2mi
− λ1
(
ρiμiq
2
ti −
μi
qti
pti
)
− λ2
(
ρiψq
2
mi −
ψ
qmi
pmi
)
− λ3
(
μi
qti
+
ψ
qmi
− T
)
, ∀i,
where λ1, λ2, and λ3 are non-negative parameters correlated to
the constraints of Problem 1.
The KKT conditions are as below
∂L1
∂qti
=
∂L1
∂qmi
= 0, ∀i, (10)
λ1 ≥ 0, λ2 ≥ 0, λ3 ≥ 0, ∀i, (11)
λ1
(
ρiμiq
2
ti −
μi
qti
pti
)
= 0, ∀i,
λ2
(
ρiψq
2
mi −
ψ
qmi
pmi
)
= 0, ∀i,
λ3
(
μi
qti
+
ψ
qmi
− T
)
= 0, ∀i,
μi
qti
t
pti − ρiμiq2ti ≥ 0, ∀i,
ψ
qmi
pmi − ρiψq2mi ≥ 0, ∀i,
μi
qti
+
ψ
qmi
≤ T, ∀i. (12)
According to (10), we can have
∂L1
∂qti
=
uiλ3
q2ti
− 2(1 + λ1)ρiuiqti, ∀i. (13)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
WANG et al.: INCENTIVE MECHANISM DESIGN FOR JOINT RESOURCE ALLOCATION IN BLOCKCHAIN-BASED FEDERATED LEARNING 1541
Let the above equation equal to 0 and we have
λ3
q2ti
= 2(1 + λ1)ρiqti, ∀i. (14)
Similarly, we have
λ3
q2mi
= 2(1 + λ2)ρiqmi, ∀i. (15)
Then, let’s consider (12). Assume that μi
qti
+ ψ
qmi
− T 
= 0, ∀i,
according to (12), we have λ3 = 0. From (14), we can see that
if λ3 = 0, this equation will be 2(1 + λ1)ρiqti = 0, ∀i, then we
have λ1 = −1 < 0. Since λ1 is constrained by (11), it should
always be non-negative. Therefore, this assumption is invalid.
We can obtain the same conclusion from (15) as well. So we can
conclude that for any i, equation μi
qti
+ ψ
qmi
− T = 0 is always
satisfied. In this way, λ3 > 0 can be deduced.
Based on the KKT conditions and μi
qti
+ ψ
qmi
− T 
= 0, ∀i, we
can analyze the optimal solutions of Problem 1 as follows:
Case 1: λ1 = λ2 = 0, μi
qti
+ ψ
qmi
− T = 0, ∀i.
In this case, since λ1 = λ2 = 0, we can derive qti = qmi =
( λ3
2ρi
)
1
3 ≥ 0 using (14) and (15), respectively. But λ3 is a non-
negative parameter, and it is not a constant value, so we still can
not get the optimal solutions of Problem 1. Thus, this case is
not suitable.
Case 2: ρiμiq2ti − μi
qti
pti = ρiψq
2
mi − ψ
qmi
pmi = 0, ∀i, μi
qti
+
ψ
qmi
− T = 0, ∀i.
By solving ρiμiq2ti − μi
qti
pti = 0, ∀i and ρiψq2mi − ψ
qmi
pmi =
0, ∀i, we have qti = (ptiρi )
1
3 , ∀i and qmi = (pmi
ρi
)
1
3 , ∀i.
Since μi
qti
+ ψ
qmi
− T = 0, ∀i, even though the above two
functions can give the expression of the solution of Problem
1, it is still constrained by this function. In other words, one of
the KKT conditions, i.e., (12), is not satisfied. Thus, this case is
not suitable.
Case 3: ρiμiq
2
ti − μi
qti
pti = 0, λ2 = 0, μi
qti
+ ψ
qmi
− T =
0, ∀i.
From (12), we can get the relationship between qti and
qmi is qmi =
ψ
T− μi
qti
. Solving ρiμiq2ti − μi
qti
pti = 0 yields qti =
(ptiρi )
1
3 , ∀i. Based on qmi =
ψ
T− μi
qti
, we let qti = (ptiρi )
1
3 , ∀i, then
we can derive that qm(t) = ψ
T− μi
(
pti
ρi
)
1
3
, ∀i. From (14) and (15),
we have λ1 = λ3
2ρiq3ti
− 1 and λ2 = λ3
2ρiq3mi
− 1. Since λ3, qti,
qmi and ρi are positive, so λ1 = λ3
2ρiq3ti
> 0, and λ3 can be
large enough to make sure λ1 = λ3
2ρiq3ti
≥ 1, thus λ1 ≥ 0 can be
guaranteed. Similarly, λ2 ≥ 0 can be derived. From the above
analysis, Case 3 satisfies all the KKT conditions; therefore, the
optimal solutions are obtained.
Case 4:ρiψq2mi − ψ
qmi
pmi = 0, ∀i, λ1 = 0, μi
qti
+ ψ
qmi
− T =
0, ∀i. This case is similar to Case 3.
Based on the above analysis, the optimal solutions of Problem
1 are q∗ti = (ptiρi )
1
3 , ∀i, and q∗mi =
ψ
T−μi(
ρi
pti
)
1
3
.
Thus Theorem III.1 is proved.
From the above theorem, we can see that the number of
optimal CPU cycles per second client i putting into training
grows as the unit price for training given by the MO increases.
The optimal CPU cycles per second devoted to mining are
constrained by ψ, indicating that if the mining work requires
more CPU cycles, client i should mine with a larger q∗mi.
C. Stage I: MO Sets Unit Prices for Clients
The MO expects to get a global model with good performance
consuming time and cost for rewards as less as possible, so its
goal is to maximize the utility function Umo, and the optimiza-
tion problem can be formulated as follows:
Problem 2 : max : Umo,
s.t.
μi
qti
pti ≤ Rti, (16)
ψ
qmi
pmi ≤ Rmi, ∀i ∈ N , (17)
where (16) and (17) are the constraints of individual rewards
from training and mining to meet the MO’s budget.
It is clear that Problem 2 is also a nonlinear optimization
problem, and Umo is also concave, so we can list all the KKT
conditions to find its maximum value. By solving Problem 2,
we can have:
Theorem III.2. The optimal strategies of the MO in the sce-
nario of complete information are
p∗ti =
(
1
ρi
) 1
2
(
Rti
μi
) 3
2
, (18)
p∗mi =
Rmi
T − (ρiμi)
3
2
(
1
Rti
) 1
2
. (19)
Proof. The Lagrangian correlated to Problem 2 is
L2 = f
(
N∑
i=1
μi
)
− ξ
N∑
i=1
(
μi
qti
+
ψ
qmi
+
μi
qti
pti +
ψ
qmi
pmi
)
− θ1
(
μi
qti
pti−Rti
)
− θ2
(
ψ
qmi
pmi−Rmi
)
,
where θ1 and θ2 are the Lagrange multipliers correlated to the
constraints of Problem 2. The following constraints should be
met
∂L2
∂pti
=
∂L2
∂pmi
= 0, ∀i,
θ1 ≥ 0, θ2 ≥ 0, ∀i,
θ1
(
μi
qti
pti −Rti
)
= 0, ∀i,
θ
(
μi
qti
pti +
ψ
qmi
pmi − ω
)
= 0, ∀i,
μi
qti
pti +
ψ
qmi
pmi ≤ ω,∀i.
μi
qti
pti ≤ Rti, ∀i,
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
1542 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 34, NO. 5, MAY 2023
ψ
qmi
pmi ≤ Rmi, ∀i.
First, let qti = q∗ti and qmi = qi(n)
∗.
Case 1: θ1 = 0, ψ
qmi
pmi −Rmi = 0, ∀i.
In this case, we can have
∂L2
∂pti
=
−(μi(ξpmi + 2ξpti + θ2pmi))
3pti(
pti
ρi
)
1
3
, ∀i.
Setting this equation equal to 0 yields pti =
−pmi(ξ+θ2)
2ξ . Ob-
viously, we cannot find a positive θ2 to satisfy this equation,
making this solution invalid.
Case 2: θ2 = 0, ψqti pti −Rti = 0, ∀i.
This case is similar to Case 1.
Case 3: ψ
qti
pti −Rti = 0, ψ
qmi
pmi −Rmi = 0, ∀i.
By solving ψ
qti
pti −Rti = 0, ψ
qmi
pmi −Rmi = 0, ∀i, we can
get (18) and (19). We can also prove that this case satisfies the
rest of the KKT conditions.
Thus, Theorem III.2 is proved. �
In the optimal solutions above, p∗mi and p∗ti are highly corre-
lated. This is because there are time and budget constraints so
that p∗ti and p∗mi are not independent variables from each other.
In other words, the MO needs to balance p∗ti and p∗mi to satisfy
the constraints when making decisions. Furthermore, we can
find that μi and ψ influence the optimal decisions as well.
We summarize the resource allocation mechanism with com-
plete information in Algorithm 1. The MO calculates the unit
prices given to the client for training and mining first and then
calculates its utility based on the previous unit prices (Lines
1-2). If Umo is the optimal utility for the MO, then the optimal
decisions of MO can be obtained (Lines 3-5). Next, the MO sends
the unit prices to clients, and each client calculates the number of
CPU cycles per second used for training and mining; if the utility
for client i is optimal, client i can make its optimal decisions and
start to train and mine (Lines 6-12). By observing Algorithm 1,
we can see that its time complexity is mainly influenced by the
number of clients, which can be expressed as O(N).
In general, the case of complete information is an ideal situa-
tion, and we find that it mainly influences the optimal decisions
of the MO. Therefore, we can study the optimal decisions in
the case of incomplete information by adjusting the decision
mechanism of the MO.
IV. RESOURCE ALLOCATION WITH INCOMPLETE INFORMATION
In this section, we will discuss the game model in the case
of incomplete information where the MO has no knowledge
of the true value of μi for each client. Thus, the MO needs to
set the unit price so that each client has a non-negative payoff
while ensuring that clients honestly report the value ofμi. Before
designing the resource allocation mechanism, we first give two
definitions below.
Definition IV.1. (Individual Rationality). The incentive mech-
anism for resource allocation is individually rational if the utility
of client i given the rewards provided by the MO is non-negative,
i.e.,
Ui(qti, qmi, pti, pmi, μi) ≥ 0, ∀i. (20)
Algorithm 1: Resource Allocation Mechanism With Com-
plete Information.
Require: T , μi, ψ, ρi, η, Rmi
Ensure: q∗ti, q
∗
mi, p
∗
ti, p
∗
mi
1: The MO calculates p̂ti and p̂mi via (18) and (19)
2: The MO calculates Umo based on p̂ti and p̂mi via (2)
3: if Umo(p̂ti, p̂mi) ≥ Umo(pti, pmi) then
4: p∗ti ← p̂ti, p∗mi ← p̂mi
5: end if
6: The MO sends p∗ti and p∗mi to the client i
7: for i ∈ N do
8: Calculate q̂ti and q̂mi via (8) and (9)
9: if Ui(q̂ti, q̂mi) ≥ Ui(qti, qmi) then
10: q∗ti ← q̂ti, q∗mi ← q̂mi
11: Client i uses q∗ti to train and q∗mi to mine
12: end if
13: end for
14: return q∗ti, q
∗
mi, p
∗
ti, p
∗
mi
Definition IV.2. (Incentive Compatibility). The incentive
mechanism for resource allocation is incentive compatible if
each client can get the optimal utility by reporting its μi truth-
fully, i.e.,
Ui(qti, qmi, pti, pmi, μi) ≥ Ui(qti, qmi, pti, pmi, μ̂i), ∀i, (21)
where μ̂i represents any value of μi.
Based on the previous analysis, clients make decisions based
on their non-negative utility. Since clients should ensure that
the rewards they receive are not less than the total costs they
spend, they can participate in the BCFL task in such a situation.
So in incomplete information, the MO needs to guarantee that
its decisions should satisfy (20) to encourage clients to join the
work. Besides, μi of client i is not known by the MO, and the
decisions of the MO are required to be based on the correct value
of μi reported by clients, so the MO needs to satisfy (21) when
making the decisions.
Since the client sets the CPU cycles per second after the unit
prices are given by the MO, the decisions of the client in the case
of incomplete information are the same as those made under
the complete information case as discussed in Section III-B.
Therefore, we will only focus on the derivation of the optimal
strategies of the MO in this section.
With incomplete information, the MO has to ensure that the
allocation of rewards to all clients is fair, the clients’ utilities are
non-negative, and clients reportμi truthfully. Thus, the decision-
making problem of the MO with incomplete information can be
transformed into the following optimization problem:
Problem 3 : max : Umo
s.t. (16), (17), (20), (21),
∀i ∈ N ,
where (20) and (21) are the constraints of individual rationality
and incentive compatibility for the mechanism; (16) and (17)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
WANG et al.: INCENTIVE MECHANISM DESIGN FOR JOINT RESOURCE ALLOCATION IN BLOCKCHAIN-BASED FEDERATED LEARNING 1543
are the constraints of individual rewards for meeting the MO’s
budget.
To solve Problem 3, we can write it in Lagrangian form
according to its optimization objective and constraints and then
analyze its KKT conditions. The optimal solutions can be solved
as below:
Theorem IV.1. The optimal strategies of the MO in the sce-
nario of incomplete information are
p∗ti =
(
1
ρi
) 1
2
(
Rti
μi
) 3
2
, (22)
p∗mi =
ρi ψ
3(
T − μi
(
ρi
p∗ti
) 1
3
)3 . (23)
Proof. Then, we will provide the solution to Problem 3. The
Lagrangian of Problem 3 can be written as
L3 = f
(
N∑
i=1
μi
)
− ξ
N∑
i=1
(
μi
qti
+
ψ
qmi
+
μi
qti
pti +
ψ
qmi
pmi
)
− α1
(
μi
qti
pti−Rti
)
− α2
(
ψ
qmi
pmi−Rmi
)
− α3
(
μi
qti
pti +
ψ
qmi
pmi − ρiμiq2ti − ρiψq2mi
)
.
where α1, α2 and α3 are the Lagrange multipliers. The KKT
conditions are similar to Problem 2 except for the following
three conditions:
α3 ≥ 0, ∀i,
α3
(
μi
qti
pti +
ψ
qmi
pmi − ρiμiq2ti − ρiψq2mi
)
= 0, ∀i,
μi
qti
pti +
ψ
qmi
pmi − ρiμiq2ti − ρiψq2mi ≥ 0, ∀i.
We then analyze the solutions under different cases. Actually,
there should be nine cases in this problem, but we only consider
two of them to analyze since the other situation can be interpreted
similarly.
Case 1: α1 = α2 = α3 = 0, ∀i.
In this case, we can have
∂L3
∂pti
=
−(μiξ(pmi + 2pti))
3ρi(
pti
ρi
)
4
3
,
and let it equal to 0 we can get pti =
−pmi
2 . Obviously, since pti
and pmi are non-negative values, we cannot find a pmi to satisfy
the above equation. So this case is invalid.
Case 2: μi
qti
pti +
ψ
qmi
pmi − ρiμiq2ti − ρiψq2mi = 0, ψ
qti
pti −
Rti = 0,α2 = 0, ∀i.By solving the above equations, we get (22)
and (23). We can verify that the solutions above are incentive
compatible and satisfy all the KKT conditions. Thus Theorem
IV.1 is proved. �
The optimal solution for p∗ti in the incomplete information
case is the same as the optimal solution in the complete infor-
mation case, while p∗mi is different. Since the decision of the
Algorithm 2: Resource Allocation Mechanism With Incom-
plete Information.
Require: T , μi, ψ, ρi, η, Rmi
Ensure: q∗ti, q
∗
mi, p
∗
ti, p
∗
mi
1: The MO calculates p̂ti and p̂mi via (22) and (23)
2: if Umo(p̂ti, p̂mi) ≥ Umo(pti, pmi) then
3: The MO calculates the expected utility Ûi of client i
4: if Ûi ≥ 0 then
5: p∗ti ← p̂ti, p∗mi ← p̂mi
6: end if
7: end if
8: The MO sends p∗ti and p∗mi to client i
9: for i ∈ N do
10: Calculate q̂ti and q̂mi
11: if Ui(q̂ti, q̂mi) ≥ Ui(qti, qmi) then
12: q∗ti ← q̂ti, q∗mi ← q̂mi
13: Client i uses q∗ti to train and q∗mi to mine
14: end if
15: end for
16: return q∗ti, q
∗
mi, p
∗
ti, p
∗
mi
MO in the case of incomplete information is not only influenced
by the budget of the reward but also required to satisfy the two
conditions (20) and (21) in the above definitions. In other words,
the decisions in this case are more conservative so the MO would
prefer to minimize its cost by reducing the payments to training
and mining. We will illustrate the specific differences in the
decisions in the two scenarios through experiments in Section V.
The resource allocation mechanism in the incomplete-
information case is presented in Algorithm 2, which is similar to
Algorithm 1, except for the decision process of the MO. In the
scenario, the MO should ensure that its utility is optimal and that
the utility for each client is non-negative (Lines 2-7). We can see
from the pseudocode that the computational cost will increase
with N , so the time complexity of Algorithm 2 is O(N).
We can see that the time complexity of both Algorithm 1 and
Algorithm 2 is O(N), which means that the time consumption
of solving these two optimization problems increases with the
number of clients linearly. Therefore, our proposed algorithms
can work efficiently in practice.
V. EXPERIMENTAL EVALUATION
In this section, we will conduct numerical experiments to
verify and support our designed mechanism. We first clarify
the experimental settings and then illustrate the results. We
implement the simulations using Matlab 2019b in macOS 11.0.1
running on an Intel i7 processor with 32 GB RAM and 1 TB SSD.
A. Experimental Setting
In our experiments, we mainly focus on the impacts of four
variables (i.e., μi, ψ, pti and pmi) on our designed models
under complete and incomplete situations. The basic settings for
these simulations are slightly different, and we will clarify the
different parts of the settings in each experiment. For simplicity
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
1544 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 34, NO. 5, MAY 2023
Fig. 2. Utilities changing with strategy pairs.
Fig. 3. Utilities of the client and the MO changing with the total CPU cycles
for training and mining.
of calculation and presentation, we use GHz as the unit of CPU
cycles per second and minute as the unit of time. We first set
η = 1500 andRmi = 5. Since we adopt SV to calculate the total
rewards distributed to the individual client and SV is correlated
to the value of μi (see (3) and (4)), we let G =
∑N
i=1 μi
N . By
running the algorithm of SV we can get the value of Rti for
each client. The settings for other parameters are ρi = 0.01,
ξ = 0.1, g = 10 and T = 15. Note that we conducted extensive
experiments with other experimental settings, while we found
that different values of the parameters would not influence the
trends of the results. So we only present the results of the
experiments based on the above settings.
B. Experimental Results
First, we prove the correctness of the optimal strategies de-
rived from our models. We assume there are 50 clients in total
and each client has the same data size, so we set μi = 10. In
our experiments, for clients and the MO, there are four strategy
combinations, i.e., both sides choose the optimal strategies, one
chooses the random strategies while the other chooses optimal
strategies, and both choose the random strategies. For example,
we define the strategy combination Random versus Optimal as
clients choose the random strategies and the MO chooses the
optimal strategy. We compare the utilities of clients and the MO
with random strategies and optimal strategies, respectively. The
results in Fig. 2 illustrate that clients and the MO can obtain
higher utilities than all other strategies when they both choose the
optimal strategies, proving the validity of our proposed optimal
strategies.
Then, the experiments will be designed to study the impacts of
μi andψ on the utility of clients and the MO under the situations
of complete and incomplete information. We set μi ∈ [0, 5] and
ψ ∈ [0, 5]. The simulation results are shown in Fig. 3. We can
see that both μi and ψ have a significant impact on the utility of
Fig. 4. Utilities of the client and the MO changing with the unit prices of
training and mining.
Fig. 5. Impacts of µi on CPU cycles per second and unit prices for training
and mining.
the MO. That is because the higher CPU power will shorten the
time in each round and improve the performance of the global
model. However, for clients, devoting more CPU cycles does
not result in more utility due to higher energy consumption.
We then study the effect of pti and pmi on the utility of the MO
and clients. We setpti ∈ [0, 10] andpmi ∈ [0, 10]. The results are
shown in Fig. 4. If the unit price of training increases, clients can
be stimulated to provide more computing power, which reduces
the time cost and improves the model performance so that the
MO utility will be improved. However, the revenue of clients
does not grow significantly with the increase of the unit price of
training because the cost of energy consumption also rises. pmi
has the same effect on utility for both complete and incomplete
information cases, and the results are shown on the right side
of Fig. 4. When the unit price of mining increases, the utility of
both clients and the MO can be improved. This is because with
the increase of pmi, clients can receive more mining revenue by
devoting more qmi. At the same time, the MO can reduce the
time cost and improve its utility by encouraging clients to devote
more CPU cycles per second to mining.
Next, we conduct experiments to analyze the relationship
between μi and the unit price for training and mining. We set
μi ∈ [0, 5], and the results are illustrated in Fig. 5. We can
see that both the unit price and the number of CPU cycles
for training increase with μi. If μi increases, more rewards are
needed to motivate clients to put more computational resources
into training. In general, μi does not affect pmi and qmi a lot,
as the benefits of mining are relatively constant and are more
influenced by the resource allocation scheme.
In the end, we explore the influence of pti on both qti and qmi
to figure out how the decisions of MO influence the decisions of
client i. We set μi = 10 and pti ∈ [0, 10]. In this setting, the
simulation results are shown in Fig. 6. We can see that the
unit CPU cycles used in local model training have a positive
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
WANG et al.: INCENTIVE MECHANISM DESIGN FOR JOINT RESOURCE ALLOCATION IN BLOCKCHAIN-BASED FEDERATED LEARNING 1545
Fig. 6. CPU cycles per second for training and mining changing with pti.
relationship with the unit price of training offered by the MO
because more unit rewards for training will incentivize clients to
put more computational power into model training. As for CPU
cycles per second used in mining, it decreases with the increase
of pti. This makes sense because if clients are motivated to put
more computing power into training, the training time will be
reduced and the mining time will be correspondingly increased.
In this way, clients do not need to set a high qmi for mining.
VI. RELATED WORK
Most of the existing studies related to BCFL focus on pro-
tecting privacy, achieving decentralization, and improving the
performance of model training [33], [34], [35], [36], [37]. Our
paper mainly focuses on resource allocation and incentive mech-
anism design in BCFL. Thus, we provide the literature review
of these two areas in this section.
A. Resource Allocation in BCFL
As for resource allocation, researchers mainly consider the
homogeneous computational power of all clients and make
decisions through the reinforcement learning approach.
In [19], the resource allocation problem is resolved for the
local devices with the same computational power in BCFL. An
upper bound of the global loss function was proposed to evaluate
the performance of training; in the meantime, the relationship
among update rounds, block generation rate, and learning rate
was explored. Although the proposed method can easily control
the training and mining time by adjusting the number of updates
to allocate resources, it is based on the assumption that all clients
have the same amount of computing resources and local data,
which is not practical. Hieu et al. [20] design a deep reinforce-
ment learning approach to help mobile devices determine the
data volume and energy used for training and to assist the system
in deciding the block generation rate. Yang et al. [21] propose a
trustworthy BCFL framework to address the privacy and security
issues of FL, where a joint optimization mechanism is designed
to allocate communication and computing resources. In [22],
a dynamic resource allocation scheme is designed to optimize
the process of client selection and model training jointly for the
BCFL system, which can efficiently improve the performance
of the global model.
According to the above discussion, it can be seen that the
studies related to resource allocation in BCFL are insufficient.
One of the reasons is that the research regarding BCFL is still in
the early stage. Another reason is that there are many types of
BCFL structures depending on the role the blockchain plays
in FL, making it difficult to have a common framework for
resource allocation. In order to assist the MO and the clients
of the BCFL system in making the proper decisions, we design
the mechanisms based on the two-stage Stackelberg game in
this paper. Besides, we consider allocating resources in the fully
coupled BCFL with FL clients working as blockchain nodes.
B. Incentive Mechanism in BCFL
Some studies about BCFL focus on regulating the behaviors of
clients through incentive mechanism design, thus encouraging
them to work honestly and efficiently according to the predefined
rules.
Toyoda et al. [38] propose an economic approach based on the
assumption that clients would act rationally, where the repeated
competition method was utilized to ensure that clients would
follow the protocol. In [24], the authors propose a blockchain-
based incentive mechanism to assist the hierarchical federated
learning to work in a secure and privacy-preserving way. Kang
et al. [23] design a data freshness based incentive mechanism to
motivate the devices to work efficiently in a BCFL system. Bao
et al. [39] design an incentive mechanism to attract more data and
computational power contributing to the framework of BCFL. In
their proposed system, honest clients can gain fairly partitioned
rewards while malicious clients will be punished via a timely
behavior detection scheme. In [40], an incentive mechanism
that integrated reputation and contract theory is proposed to
encourage clients to provide high-quality data to train the local
models. As for the fairness of reward allocation, Liu et al. [41]
use Shapley Value (SV) to calculate the contributions of clients
of the FL system and then allocate the rewards accordingly.
However, this approach is not able to make incentive decisions
for training and mining, respectively.
The existing studies about incentive mechanism design in
BCFL focus on how to provide incentives for FL through
blockchain without considering the incentives for blockchain
and FL in a systematical manner. In other words, blockchain
and FL are in different phases for BCFL, so they should both
have reasonable incentives. In our paper, we design a pricing
mechanism for the MO based on the computing power provided
by clients, thus providing incentives to the whole BCFL system.
In general, the existing studies have paid little attention to
resource allocation for clients in BCFL and assume that clients
join the task voluntarily. To address this challenge, we design
a resource allocation mechanism for clients, which also offers
reward suggestions to the MO so as to motivate clients to
participate in BCFL.
VII. CONCLUSION
This paper studies the resource allocation of clients in BCFL
by designing an incentive mechanism. We describe the interac-
tions between clients and the MO as a two-stage Stackelberg
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
1546 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 34, NO. 5, MAY 2023
game. Within our model, clients with varying computing power
can determine the resources to invest in training and mining
based on the rewards provided by the MO through maximizing
their utilities, while the MO can also obtain the optimal utility.
Since the local training related information of clients may not
be known to the MO, we further study the game model and
optimal solutions in the incomplete information case. Numerous
experimental results show that our proposed mechanisms are
effective.
REFERENCES
[1] Y. Zhao et al., “Privacy-preserving blockchain-based federated learning
for IoT devices,” IEEE Internet Things J., vol. 8, no. 3, pp. 1817–1829,
Feb. 2020.
[2] W. Zhang et al., “Blockchain-based federated learning for device failure
detection in industrial IoT,” IEEE Internet Things J., vol. 8, no. 7, pp. 5926–
5937, Apr. 2020.
[3] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain and
federated learning for privacy-preserved data sharing in industrial IoT,”
IEEE Trans. Ind. Informat., vol. 16, no. 6, pp. 4177–4186, Jun. 2020.
[4] Y. Qi, M. S. Hossain, J. Nie, and X. Li, “Privacy-preserving blockchain-
based federated learning for traffic flow prediction,” Future Gener.
Comput. Syst., vol. 117, pp. 328–337, 2021.
[5] G. Hua, L. Zhu, J. Wu, C. Shen, L. Zhou, and Q. Lin, “Blockchain-based
federated learning for intelligent control in heavy haul railway,” IEEE
Access, vol. 8, pp. 176830–176839, 2020.
[6] J. Passerat-Palmbach, T. Farnan, R. Miller, M. S. Gross, H. L. Flannery,
and B. Gleim, “A blockchain-orchestrated federated learning architecture
for healthcare consortia,” 2019, arXiv:1910.12603.
[7] S. Aich et al., “Protecting personal healthcare record using blockchain
& federated learning technologies,” in Proc. IEEE 23rd Int. Conf. Adv.
Commun. Technol., 2021, pp. 109–112.
[8] R. Kumar et al., “Blockchain-federated-learning and deep learning models
for COVID-19 detection using CT imaging,” IEEE Sensors J., vol. 21,
no. 14, pp. 16301–16314, Jul. 2021.
[9] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Ar-
cas, “Communication-efficient learning of deep networks from decentral-
ized data,” in Proc. 20th Int. Conf. Artif. Intell. Statist., PMLR, 2017,
pp. 1273–1282.
[10] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How
to backdoor federated learning,” in Proc. Int. Conf. Artif. Intell. Statist.,
PMLR, 2020, pp. 2938–2948.
[11] F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek, “Robust and
communication-efficient federated learning from non-iid data,” IEEE
Trans. Neural Netw. Learn. Syst., vol. 31, no. 9, pp. 3400–3413, Sep. 2020.
[12] L. Lyu, H. Yu, and Q. Yang, “Threats to federated learning: A survey,”
2020, arXiv:2003.02133.
[13] V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and
G. Srivastava, “A survey on security and privacy of federated learning,”
Future Gener. Comput. Syst., vol. 115, pp. 619–640, 2021.
[14] P. Ramanan and K. Nakayama, “Baffle: Blockchain based aggregator free
federated learning,” in Proc. IEEE Int. Conf. Blockchain, 2020, pp. 72–81.
[15] Y. J. Kim and C. S. Hong, “Blockchain-based node-aware dynamic weight-
ing methods for improving federated learning performance,” in Proc. IEEE
20th Asia-Pacific Netw. Operations Manage. Symp., 2019, pp. 1–4.
[16] Y. Liu, Z. Ai, S. Sun, S. Zhang, Z. Liu, and H. Yu, “Fedcoin: A peer-to-peer
payment system for federated learning,” in Federated Learning. Berlin,
Germany: Springer, 2020, pp. 125–138.
[17] W. Issa, N. Moustafa, B. Turnbull, N. Sohrabi, and Z. Tari, “Blockchain-
based federated learning for securing Internet of Things: A comprehensive
survey,” ACM Comput. Surv., vol. 55, no. 9, pp. 1–43, 2022.
[18] J. Zhu, J. Cao, D. Saxena, S. Jiang, and H. Ferradi, “Blockchain-
empowered federated learning: Challenges, solutions, and future direc-
tions,” ACM Comput. Surv., vol. 55, no. 11, pp. 1–31, 2022.
[19] J. Li et al., “Blockchain assisted decentralized federated learning (BLADE-
FL): Performance analysis and resource allocation,” IEEE Trans. Parallel
Distrib. Syst., vol. 33, no. 10, pp. 2401–2415, Oct. 2021.
[20] N. Q. Hieu, T. T. Anh, N. C. Luong, D. Niyato, D. I. Kim, and E. Elmroth,
“Resource management for blockchain-enabled federated learning: A deep
reinforcement learning approach,” 2020, arxiv:2004.04104.
[21] Z. Yang, Y. Shi, Y. Zhou, Z. Wang, and K. Yang, “Trustworthy feder-
ated learning via blockchain,” IEEE Internet Things J., vol. 10, no. 1,
pp. 92–109, Jan. 2023.
[22] X. Deng et al., “Blockchain assisted federated learning over wireless
channels: Dynamic resource allocation and client scheduling,” IEEE Trans.
Wireless Commun., to be published, doi: 10.1109/TWC.2022.3219501.
[23] J. Kang et al., “Blockchain-based federated learning for industrial meta-
verses: Incentive scheme with optimal AoI,” in Proc. IEEE Int. Conf.
Blockchain, 2022, pp. 71–78.
[24] X. Wang, Y. Zhao, C. Qiu, Z. Liu, J. Nie, and V. C. Leung, “InFEDge:
A blockchain-based incentive mechanism in hierarchical federated learn-
ing for end-edge-cloud communications,” IEEE J. Sel. Areas Commun.,
vol. 40, no. 12, pp. 3325–3342, Dec. 2022.
[25] X. Qu, Q. Hu, and S. Wang, “Privacy-preserving model training archi-
tecture for intelligent edge computing,” Comput. Commun., vol. 162,
pp. 94–101, 2020.
[26] Z. Wang and Q. Hu, “Blockchain-based federated learning: A comprehen-
sive survey,” 2021, arXiv:2110.02182.
[27] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,” Decen-
tralized Bus. Rev., vol. 9, 2008, Art. no. 21260.
[28] M. Castro et al., “Practical byzantine fault tolerance,” in Proc. Conf.
Operating Syst. Des. Implementation, 1999, pp. 173–186.
[29] D. Mingxiao, M. Xiaofeng, Z. Zhe, W. Xiangwei, and C. Qijun, “A review
on consensus algorithm of blockchain,” in Proc. IEEE Int. Conf. Syst.,
Man, Cybern., 2017, pp. 2567–2572.
[30] D. Ongaro and J. Ousterhout, “In search of an understandable consensus
algorithm,” in Proc. USENIX Annu. Tech. Conf., 2014, pp. 305–319.
[31] T. D. Burd and R. W. Brodersen, “Processor design for portable systems,”
J. VLSI Signal Process. Syst. Signal, Image Video Technol., vol. 13, no. 2,
pp. 203–221, 1996.
[32] J. Zhang and Q. Zhang, “Stackelberg game for utility-based cooperative
cognitiveradio networks,” in Proc. 10th ACM Int. Symp. Mobile Ad Hoc
Netw. Comput., 2009, pp. 23–32.
[33] Z. Peng et al., “VFChain: Enabling verifiable and auditable federated
learning via blockchain systems,” IEEE Trans. Netw. Sci. Eng., vol. 9,
no. 1, pp. 173–186, Jan./Feb. 2021.
[34] H. B. Desai, M. S. Ozdayi, and M. Kantarcioglu, “BlockFLA: Accountable
federated learning via hybrid blockchain architecture,” in Proc. 11th ACM
Conf. Data Appl. Secur. Privacy, 2021, pp. 101–112.
[35] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, “Blockchain
empowered asynchronous federated learning for secure data sharing in
internet of vehicles,” IEEE Trans. Veh. Technol., vol. 69, no. 4, pp. 4298–
4311, Apr. 2020.
[36] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “On-device federated learning
via blockchain and its latency analysis,” 2018, arXiv:1808.03949.
[37] Q. Hu, Z. Wang, M. Xu, and X. Cheng, “Blockchain and federated
edge learning for privacy-preserving mobile crowdsensing,” IEEE Internet
Things J., to be published, doi: 10.1109/JIOT.2021.3128155.
[38] K. Toyoda and A. N. Zhang, “Mechanism design for an incentive-aware
blockchain-enabled federated learning platform,” in Proc. IEEE Int. Conf.
Big Data, 2019, pp. 395–403.
[39] X. Bao, C. Su, Y. Xiong, W. Huang, and Y. Hu, “FLChain: A blockchain
for auditable federated learning with trust and incentive,” in Proc. IEEE
5th Int. Conf. Big Data Comput. Commun., 2019, pp. 151–159.
[40] J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, “Incentive mechanism
for reliable federated learning: A joint optimization approach to combining
reputation and contract theory,” IEEE Internet Things J., vol. 6, no. 6,
pp. 10700–10714, Dec. 2019.
[41] Y. Liu, S. Sun, Z. Ai, S. Zhang, Z. Liu, and H. Yu, “FedCoin: A peer-to-peer
payment system for federated learning,” 2020, arxiv:2002.11711.
Zhilin Wang (Graduate Student Member, IEEE) re-
ceived the BS degree from Nanchang University, in
2020. He is currently working toward the PhD degree
in computer and information science with Indiana
University-Purdue University Indianapolis (IUPUI).
He is a research assistant with IUPUI, and he is also
the reviewer of the 2022 IEEE ICC, IEEE Access,
IEEE Transactions on Parallel and Distributed Sys-
tems, IEEE Internet of Things Journal, and Jour-
nal of Network and Computer Applications, and he
also serves as a TPC member for IEEE ICC 2022
Workshop. His research interests include blockchain, federated learning, edge
computing, and optimization theory.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
https://dx.doi.org/10.1109/TWC.2022.3219501
https://dx.doi.org/10.1109/JIOT.2021.3128155
WANG et al.: INCENTIVE MECHANISM DESIGN FOR JOINT RESOURCE ALLOCATION IN BLOCKCHAIN-BASED FEDERATED LEARNING 1547
Qin Hu received the PhD degree in computer sci-
ence from the George Washington University, in
2019. She is currently an assistant professor with the
Department of Computer and Information Science,
Indiana University-Purdue University Indianapolis
(IUPUI). She has served on the Editorial Board of
two journals, the guest editor for multiple journals,
the TPC/Publicity co-chair for several workshops,
and the TPC member for several international con-
ferences. Her research interests include wireless and
mobile security, edge computing, blockchain, and
federated learning.
Ruinian Li received the PhD degree in computer
science from the George Washington University, in
2018. He is currently an assistant professor with the
Department of Computer Science, Bowling Green
State University (BGSU), USA. His research inter-
ests include security and privacy-preserving compu-
tations, applied cryptography, and blockchain tech-
nology. He has been working in a wide area of
social networks, auction systems, and IoT, and his
work has been published in top-tier journals, such as
IEEE Transactions on Services Computing, and IEEE
Transactions on Network Science and Engineering.
Minghui Xu received the BS degree in physics
from the Beijing Normal University, Beijing, China,
in 2018, and the PhD degree in computer science
from George Washington University, Washington
DC, USA, in 2021. He is currently an assistant pro-
fessor with the School of Computer Science and
Technology, Shandong University, China. His current
research focuses on blockchain, distributed comput-
ing, and quantum computing.
Zehui Xiong (Member, IEEE) received the PhD de-
gree from Nanyang Technological University, Sin-
gapore. He is currently an assistant professor with
the Pillar of Information Systems Technology and
Design, Singapore University of Technology and De-
sign. Prior to that, he was a researcher with Alibaba-
NTU Joint Research Institute, Singapore. He was
the visiting scholar with Princeton University and
University of Waterloo. His research interests include
wireless communications, network games and eco-
nomics, blockchain, and edge intelligence. He has
published more than 140 research papers in leading journals and flagship
conferences and many of them are ESI Highly Cited Papers. He has won
more than 10 Best Paper Awards in international conferences and is listed in
the World’s Top 2% scientists identified by Stanford University. He is now
serving as the editor or guest editor for many leading journals including IEEE
Journal on Selected Areas in Communications, IEEE Transactions on Vehicular
Technology, IEEE Internet of Things Journal, IEEE Transactions on Cognitive
Communications and Networking, IEEE Transactions on Network Science and
Engineering, IEEE Systems Journal, Journal of the Atmospheric Sciences. He
is the recipient of IEEE TCSC Early Career researcher Award for Excellence
in Scalable Computing, IEEE CSIM Technical Committee Best Journal Paper
Award, IEEE SPCC Technical Committee Best Paper Award, IEEE VTS Singa-
pore Best Paper Award, Chinese Government Award for Outstanding Students
Abroad, and NTU SCSE Best PhD Thesis Runner-Up Award. He is the founding
vice chair of Special Interest Group on Wireless Blockchain Networks in IEEE
Cognitive Networks Technical Committee.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 09:10:54 UTC from IEEE Xplore.  Restrictions apply. 
<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles true
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.4
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /sRGB
  /DoThumbnails true
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo true
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts true
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
    /Algerian
    /Arial-Black
    /Arial-BlackItalic
    /Arial-BoldItalicMT
    /Arial-BoldMT
    /Arial-ItalicMT
    /ArialMT
    /ArialNarrow
    /ArialNarrow-Bold
    /ArialNarrow-BoldItalic
    /ArialNarrow-Italic
    /ArialUnicodeMS
    /BaskOldFace
    /Batang
    /Bauhaus93
    /BellMT
    /BellMTBold
    /BellMTItalic
    /BerlinSansFB-Bold
    /BerlinSansFBDemi-Bold
    /BerlinSansFB-Reg
    /BernardMT-Condensed
    /BodoniMTPosterCompressed
    /BookAntiqua
    /BookAntiqua-Bold
    /BookAntiqua-BoldItalic
    /BookAntiqua-Italic
    /BookmanOldStyle
    /BookmanOldStyle-Bold
    /BookmanOldStyle-BoldItalic
    /BookmanOldStyle-Italic
    /BookshelfSymbolSeven
    /BritannicBold
    /Broadway
    /BrushScriptMT
    /CalifornianFB-Bold
    /CalifornianFB-Italic
    /CalifornianFB-Reg
    /Centaur
    /Century
    /CenturyGothic
    /CenturyGothic-Bold
    /CenturyGothic-BoldItalic
    /CenturyGothic-Italic
    /CenturySchoolbook
    /CenturySchoolbook-Bold
    /CenturySchoolbook-BoldItalic
    /CenturySchoolbook-Italic
    /Chiller-Regular
    /ColonnaMT
    /ComicSansMS
    /ComicSansMS-Bold
    /CooperBlack
    /CourierNewPS-BoldItalicMT
    /CourierNewPS-BoldMT
    /CourierNewPS-ItalicMT
    /CourierNewPSMT
    /EstrangeloEdessa
    /FootlightMTLight
    /FreestyleScript-Regular
    /Garamond
    /Garamond-Bold
    /Garamond-Italic
    /Georgia
    /Georgia-Bold
    /Georgia-BoldItalic
    /Georgia-Italic
    /Haettenschweiler
    /HarlowSolid
    /Harrington
    /HighTowerText-Italic
    /HighTowerText-Reg
    /Impact
    /InformalRoman-Regular
    /Jokerman-Regular
    /JuiceITC-Regular
    /KristenITC-Regular
    /KuenstlerScript-Black
    /KuenstlerScript-Medium
    /KuenstlerScript-TwoBold
    /KunstlerScript
    /LatinWide
    /LetterGothicMT
    /LetterGothicMT-Bold
    /LetterGothicMT-BoldOblique
    /LetterGothicMT-Oblique
    /LucidaBright
    /LucidaBright-Demi
    /LucidaBright-DemiItalic
    /LucidaBright-Italic
    /LucidaCalligraphy-Italic
    /LucidaConsole
    /LucidaFax
    /LucidaFax-Demi
    /LucidaFax-DemiItalic
    /LucidaFax-Italic
    /LucidaHandwriting-Italic
    /LucidaSansUnicode
    /Magneto-Bold
    /MaturaMTScriptCapitals
    /MediciScriptLTStd
    /MicrosoftSansSerif
    /Mistral
    /Modern-Regular
    /MonotypeCorsiva
    /MS-Mincho
    /MSReferenceSansSerif
    /MSReferenceSpecialty
    /NiagaraEngraved-Reg
    /NiagaraSolid-Reg
    /NuptialScript
    /OldEnglishTextMT
    /Onyx
    /PalatinoLinotype-Bold
    /PalatinoLinotype-BoldItalic
    /PalatinoLinotype-Italic
    /PalatinoLinotype-Roman
    /Parchment-Regular
    /Playbill
    /PMingLiU
    /PoorRichard-Regular
    /Ravie
    /ShowcardGothic-Reg
    /SimSun
    /SnapITC-Regular
    /Stencil
    /SymbolMT
    /Tahoma
    /Tahoma-Bold
    /TempusSansITC
    /TimesNewRomanMT-ExtraBold
    /TimesNewRomanMTStd
    /TimesNewRomanMTStd-Bold
    /TimesNewRomanMTStd-BoldCond
    /TimesNewRomanMTStd-BoldIt
    /TimesNewRomanMTStd-Cond
    /TimesNewRomanMTStd-CondIt
    /TimesNewRomanMTStd-Italic
    /TimesNewRomanPS-BoldItalicMT
    /TimesNewRomanPS-BoldMT
    /TimesNewRomanPS-ItalicMT
    /TimesNewRomanPSMT
    /Times-Roman
    /Trebuchet-BoldItalic
    /TrebuchetMS
    /TrebuchetMS-Bold
    /TrebuchetMS-Italic
    /Verdana
    /Verdana-Bold
    /Verdana-BoldItalic
    /Verdana-Italic
    /VinerHandITC
    /Vivaldii
    /VladimirScript
    /Webdings
    /Wingdings2
    /Wingdings3
    /Wingdings-Regular
    /ZapfChanceryStd-Demi
    /ZWAdobeF
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 150
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages false
  /ColorImageDownsampleType /Bicubic
  /ColorImageResolution 900
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.00111
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages true
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.40
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 150
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages false
  /GrayImageDownsampleType /Bicubic
  /GrayImageResolution 1200
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.00083
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages true
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.40
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 1200
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages false
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 1600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.00063
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False
  /CreateJDFFile false
  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Suggested"  settings for PDF Specification 4.0)
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice