Bayesian Reinforcement Learning and Bayesian Deep Learning for Blockchains With Mobile Edge Computing
IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, VOL. 7, NO. 1, MARCH 2021 319
Bayesian Reinforcement Learning and Bayesian
Deep Learning for Blockchains With
Mobile Edge Computing
Alia Asheralieva and Dusit Niyato , Fellow, IEEE
Abstract—We present a novel game-theoretic, Bayesian
reinforce-ment learning (RL) and deep learning (DL) framework
to represent interactions of miners in public and consortium
blockchains with mobile edge computing (MEC). Within the
framework, we formulate a stochastic game played by min-
ers under incomplete information. Each miner can offload its
block operations to one of the base stations (BSs) equipped
with the MEC server. The miners select their offloading BSs
and block processing rates simultaneously and independently,
without informing other miners about their actions. As such,
no miner knows the past and current actions of others and,
hence, constructs its belief about these actions. Accordingly, we
devise a Bayesian RL algorithm based on the partially-observable
Markov decision process for miner’s decision making that allows
each miner to dynamically adjust its strategy and update its
beliefs through repeated interactions with each other and with
the mobile environment. We also propose a novel unsuper-
vised Bayesian deep learning algorithm where the uncertainties
about unobservable states are approximated with Bayesian neural
networks. We show that the proposed Bayesian RL and DL algo-
rithms converge to the stable states where the miners’ actions
and beliefs form the perfect Bayesian equilibrium (PBE) and
myopic PBE, respectively.
Index Terms—Bayesian methods, blockchains, deep learning,
game theory, incomplete information, machine learning, mobile
edge computing, partially-observable Markov decision process,
reinforcement learning, resource management.
Manuscript received July 21, 2019; revised January 18, 2020; accepted
May 4, 2020. Date of publication May 14, 2020; date of current ver-
sion March 8, 2021. This work was supported in part by the National
Natural Science Foundation of China (NSFC) Project No. 61950410603,
National Research Foundation (NRF), Singapore, under Singapore Energy
Market Authority (EMA), Energy Resilience, NRF2017EWT-EP003-041,
Singapore NRF2015-NRF-ISF001-2277, Singapore NRF National Satellite of
Excellence, Design Science and Technology for Secure Critical Infrastructure
NSoE DeST-SCI2019-0007, A*STAR-NTU-SUTD Joint Research Grant
on Artificial Intelligence for the Future of Manufacturing RGANS1906,
WASP/NTU M4082187 (4080), Singapore MOE Tier 2 MOE2014-T2-2-
015 ARC4/15, and MOE Tier 1 2017-T1-002-007 RG122/17. The associate
editor coordinating the review of This article and approving it for publication
was K. Zeng. (Corresponding author: Alia Asheralieva.)
Alia Asheralieva is with the Department of Computer Science and
Engineering, Southern University of Science and Technology, Shenzhen
518055, China (e-mail: aasheralieva@gmail.com).
Dusit Niyato is with the School of Computer Science and
Engineering, Nanyang Technological University, Singapore 639798 (e-mail:
dniyato@ntu.edu.sg).
This paper has supplementary downloadable material available at
https://ieeexplore.ieee.org, provided by the authors.
Digital Object Identifier 10.1109/TCCN.2020.2994366
I. INTRODUCTION
BLOCKCHAIN is a distributed ledger technology in
which data is processed and stored with in-build robust-
ness, as it cannot be controlled by a single entity and is
able to avoid a single failure point. Public blockchains are
also characterized by their open access, transparency and
disintermediation [1]–[3]. In the blockchain, data is organized
in the form of blocks, e.g., transaction records, which repre-
sent a linked list structure and preserve logical relations in
the stored data. The copies of blocks are distributed across
an entire blockchain network that comprises all blockchain
users. This guarantees improved data integrity and security
comparing to centralized ledgers. The process by which the
blocks are processed, verified and added to a blockchain is
referred to as a mining [1]. During mining, a blockchain user
or miner executes a compute-intensive task, e.g., block pro-
cessing or validation. The block transaction is confirmed and
stored in a blockchain only if its output reaches a consensus
that can be based on such protocols as proof-of-work (PoW),
proof-of-stake (PoS), or practical Byzantine fault tolerance
(pBFT) [4], [5].
Although blockchains have already been adopted in cer-
tain distributed system scenarios, e.g., content delivery
networks [6] and smart-grids [7], they are still not widely
utilized in the Internet of Things (IoT) and other mobile
systems. The main reason is that the users’ mobile termi-
nals and IoT devices have scarce computing resources. Thus,
they cannot perform many compute-intensive block tasks. To
deal with this issue, MEC concept has been proposed to
facilitate future IoT and mobile applications [3], [8], [9]. In
the MEC, virtual cloud computing capabilities are extended
at the network “edges” through MEC servers installed
at BSs, so that the end-users can offload their tasks
via the BSs in proximity [10]–[12]. Compared to cloud
computing where all data are transmitted to, and pro-
cessed at a centralized cloud (far from end-users) which
leads to heavy backhaul load, high propagation delays, and
increased chances of being manipulated on a way to the
cloud, MEC can provide reduced backhaul traffic, conges-
tion, energy consumption and latency, and improved secu-
rity, as data are managed locally [3], [12]. Nevertheless, to
allow a successful realization of the blockchain with MEC,
it is important to study the profitability and practicality
of the system from the perspective of its users, i.e.,
miners.
2332-7731 c© 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
https://orcid.org/0000-0002-4430-5928
https://orcid.org/0000-0002-7442-7416
320 IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, VOL. 7, NO. 1, MARCH 2021
In particular, due to its nature, blockchain system operates
in a distributed manner – every miner makes an independent
decision on how to handle its block task aiming to maximize
its individual payoff. The payoff is the difference between the
reward that the miner receives for its task and the cost of
computing power spent on running the task. The probability
that the miner wins a reward depends on: i) computing powers
of other miners; ii) orphaning probability for the miner’s task,
i.e., probability that the task is discarded due to long latency.
Accordingly, to make a decision that maximizes it payoff, the
miner must know exact computing powers of other miners and
the orphaning probability for its task, which is unrealistic in
blockchains with MEC due to the following critical issues:
- In the MEC, information exchange among miners leads
to additional signaling that increases delay/energy over-
heads, especially in public and consortium blockchains
where the number of miners and, hence, amount of
communications is very large. As such, miners must
decide on their computing power simultaneously and
independently, without revealing their decisions to other
miners [1], [3], [13]. Therefore, the computing power
selected by a miner represents its private information
unobservable by other miners.
- In the MEC, the orphaning probability for the miner’s
task is inversely proportional to the rate of a chan-
nel between the miner and its associated BS [3], [14],
which depends on stochastic parameters of wireless
environment (e.g., miners’ locations, channel quality
and co-channel interference). As a result, to estimate
the orphaning probability, the miner must be able to
predict the environmental parameters and adjust its
task-handling decision in response to their changes.
Consequently, in this paper, we formulate the novel game-
theoretic, Bayesian RL and DL framework which enable us
to address the above critical issues in order to allow all min-
ers to decide how to handle their tasks more efficiently, i.e.,
with maximal payoffs, and, thus, ensure the system practi-
cality and profitability of the blockchain with MEC from the
perspective of miners. We use a non-cooperative game theory,
because it is the most suitable tool to model the interac-
tions of miners – it reflects a distributed blockchain nature
by assuming that all miners act as independent players aim-
ing to maximize their individual (rather than socially-optimal)
payoffs. On the other hand, by adopting learning, miners
can predict changes in the random and unobservable parame-
ters via repeated interactions with the other miners and with
the environment. The main contributions of the paper are as
follows:
1) We present a new system model of public and con-
sortium blockchain applications in the MEC network
formed by the set of BSs, each of which is equipped
with the MEC server. Unlike prior blockchain systems
(e.g., [8], [9]), the model takes into account the dynamic
MEC features. In the model, each miner can select any
BS in its transmission range to offload a block task and
decide on its block processing rate (BPR), i.e., comput-
ing power, at a selected BS. Since the miner is rational, it
aims at maximizing its expected payoff, i.e., difference
between its expected reward and cost of BPR at the
offloading BS.
2) We show that the interactions of miners can be described
by the stochastic game with incomplete information.
The game is stochastic, because the payoffs of its
players/miners depend on the random environmental
state with unknown state transitions. Moreover, since
no miner can observe the task-handling decisions or
actions of other miners, the game is played under
incomplete information. To model the miner’s decision
process, we adopt a partially-observable Markov deci-
sion process (POMDP), the solution of which represents
a perfect Bayesian equilibrium (PBE) where the strate-
gies of the miners maximize their expected long-term
payoffs. Based on the POMDP, we develop the “on-line”
Bayesian RL (BRL) algorithm that enables all the min-
ers to update their beliefs about unobservable actions
and reach a PBE.
3) To reduce exponential complexity of a BRL algorithm,
we formulate a novel Bayesian DL (BDL) algorithm
of the polynomial complexity, in which the uncertain-
ties about unobservable states of POMDP are mod-
eled by a Bayesian neural network (BNN) – neural
network (NN) with random weights distributed accord-
ing to a predefined prior. The BDL algorithm is unsuper-
vised – it enables a self-organized on-line learning that
does not require any prior “off-line” training and/or pre-
existing training datasets. We prove that the BDL algo-
rithm converges to the state where the miners’ strategies
form a myopic PBE (MPBE) – a PBE with short-sighted
players aiming to maximize expected stage payoffs.
The rest of the paper is organized as follows. In Section II,
we review related research on mobile blockchains. In
Section III, we present the system model of a blockchain with
MEC. In Section IV, we define a stochastic game and design
a BRL algorithm for the miners. In Section V, we formulate
a BDL algorithm. In Section VI, we evaluate the performance
of the proposed framework. The list of acronyms used in the
paper in the alphabetical order is provided in Appendix A in
the supplementary material.
II. RELATED WORK
The existing works on mobile blockchains can be
arbitrarily classified into three groups: i) blockchain-
based protocols for mobile and IoT applications (e.g.,
[14]–[27]); ii) consensus algorithms for mobile blockchains
(e.g., [7], [28]–[35]); iii) pricing and resource manage-
ment in mobile blockchains (e.g., [8], [9], [36]–[39]).
The first largest group adopts the concept of blockchains
in developing data communication and control protocols
to support various mobile/IoT applications, e.g., spectrum
sharing [15]–[19], network virtualization [20]–[22], or data
and resource management [14], [23]–[27], by taking advan-
tage of such useful blockchain features as traceability, decen-
tralization, immutability and privacy. E.g., in cognitive radio
networks, blockchains can address spectrum scarcity and
increase utilization in a distributed, secure and transparent
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: BAYESIAN REINFORCEMENT LEARNING AND BAYESIAN DEEP LEARNING 321
way [15]–[19]. In this case, blockchains serve as a middle
layer between primary and secondary spectrum users to pro-
vide system verification and access, and perform spectrum
sharing and trading. In network virtualization, blockchains
can be used to slice the virtual networks, so that the
resource owners can sublease resources to virtual network
operators [20]–[22]. The participants of each slice are man-
aged by the respective block slice, which provides auditability,
monitoring of operations and data access. Blockchains can
also be applied for data/ resource management in mobile/IoT
networks, in which case they act as distributed platforms for
authentication and design of smart contracts [14], [25]–[27].
The second group (e.g., [7], [28]–[35]) explores consensus
algorithms and protocols applicable to mobile/IoT blockchain
systems. The challenge here is the scalability measured w.r.t.
three metrics: i) throughput – number of transactions verified
per time unit; ii) security – number of malicious peers the
system can tolerate; iii) storage – size of blockchain that can
be handled by the miners with limited storage capabilities.
Scalability means that all metrics can improve or, at least,
not deteriorate as the number of miners increases [4], [5].
Poor scalability is the biggest issue of current blockchain
systems, e.g., Ethereum and Bitcoin, that are based on clas-
sical PoW and PoS protocols. Although these systems can
tolerate up to 49% of malicious miners, they have low
throughput and storage efficiency [28]–[30]. Thus, several
alternatives, e.g., delegated PoS (DPoS), pBFT, sharding, and
their variations [7], [28]–[35], have been proposed to increase
throughput and storage, without compromising security.
The third group (e.g., [8], [9], [36]–[39]) study pricing
and resource management in blockchains with MEC, where
miners offload their tasks to the BSs equipped with MEC
servers. The focus here is on the mining under PoW con-
sensus which leads to a competition among miners to win
a reward. Authors in [8], [9], [36] and [37] analyze the inter-
play between miners and MEC service providers (SPs), and
propose the methods to maximize the miners’ payoffs and the
revenues of the SPs by adopting Stackelberg game [8], [9] and
auction theory [36], [36]. Authors in [38] and [39] propose
the optimization-based approaches for resource allocation,
offloading and block size adaptation with the objective to
maximize the average miner’s reward. As such, the studies
in [8], [9], [36]–[39] explore the profitability of blockchains
with MEC for the miners and offer various mechanisms
to maximize miners’ payoffs. However, they have several
limitations which are listed below:
- The works in [8], [9], [36]–[39] ignore dynamics of
the mobile environment, e.g., time-varying locations of
miners, co-channel interference and channel quality. In
particular, games/actions in [8], [9], [36] and [37] are
played in a static deterministic environment result-
ing in short-term solutions; optimization problems
in [38] and [39] assume that all the wireless parameters
remain fixed. Hence, the applicability of these works to
practical blockchains operating in highly-dynamic MEC
networks is unclear.
- The main assumption of works in [8], [9], [36]–[39] is
the complete information about the miners’ actions and
other system parameters. This is rather unrealistic, since
in PoW blockchains, miners take their actions simultane-
ously and independently, and compete against each other
to win a reward [1], [3], [13]. Thus, the action of each
miner is the private information of this miner unknown
to other miners.
In summary, the framework proposed in this paper belongs
to the third group of studies in [8], [9], [36]–[39]. Similar to
these studies, we analyze the practicality of blockchains with
MEC for the miners, but adopt more relevant to the mobile
environment considerations:
1) The presented system model addresses the random-
ness of a wireless environment which is not considered
in existing research. The formulated game model is
stochastic, i.e., it is played in a dynamic environment
and returns both the long-term and short-term solutions
represented by the PBE and MPBE, respectively.
2) In our framework, information about the miners’ actions
is incomplete. To deal with incomplete information, we
adopt the BRL and BDL algorithms which enable each
miner to form and update its belief about unobservable
actions of other miners. As such, unlike many exist-
ing learning-based methods which show the algorithm
convergence to some fixed (but not always optimal)
value [40], [41], we prove that our BRL and BDL
algorithms converge to the PBE and MPBE, i.e., the
long-term and short-term solutions of the miners’ game,
respectively.
3) Although our framework is originally developed for
PoW protocols, it can be applied (with slight modifi-
cations) with other protocols, such as PoS or pBFT.
III. ANALYTICAL MODEL OF A BLOCKCHAIN WITH MEC
A. System Model
Consider a blockchain application implemented in the MEC
network, as shown in Figure 1. The network includes a set
M = {1, . . . ,M } of macro- and small-cell BSs labeled as
BS1, . . . , BSM , each of which is equipped with the MEC
server, and a set N = {M +1, . . . ,M +N } of blockchain min-
ers labeled as UM+1, . . . , UM+N , for notation consistency.
In the system, miners perform the range blockchain opera-
tions: i) process data in the form of blocks, e.g., transaction
records; ii) verify generated blocks; iii) store verified blocks.
The BSs provide offloading and communication services to
the miners. That is, any miner Un ,n ∈ N, can offload its
compute-intensive block task, e.g., block processing or ver-
ification, from its mobile device to one of the BSs in its
transmission range. We denote by Mn ⊆ M the subset of
BSs in the range of miner Un . Every BSm , m ∈M, operates
on an orthogonal spectrum of the bandwidth Bm that may
overlap with the spectra of other BSs. Without loss of gener-
ality, BSs can be privately-owned, or controlled by different
SPs. Each BS sells its computing resources measured in terms
of edge computing units (ECUs) to the miners. The comput-
ing resources of the BS are limited by the maximal number of
ECUs supported by its MEC server. We denote by Xm and
cm the maximal number of supported ECUs and the cost per
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
322 IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, VOL. 7, NO. 1, MARCH 2021
Fig. 1. Blockchain application in the MEC network with 11 BSs, macro-cell
BS1 and BS6, micro-cell BS2, BS5, BS7, BS9 and BS11, and femto-cell
BS3, BS4, BS8 and BS10. Each miner can offload its compute-intensive task
(e.g., block processing or verification) to any BS in its transmission range.
ECU at BSm , respectively. The list of main notations used
in the paper is provided in Appendix B in the supplementary
material.
The process of blockchain mining consists of the number
of stages, denoted as t = 0, . . . ,T , during which every sub-
sequent block in the chain is processed. Within each stage,
all network parameters (e.g., the numbers of miners and BSs,
locations of miners, channel and computing resource alloca-
tions) remain constant. As a result, the connectivity of the
MEC network is preserved during one mining stage, i.e., there
are no handovers or access blockages for miners. However, the
parameters can change as we head to the next stage. The BSs
and miners can join the blockchain system at any mining stage
and remain in the system for an indefinitely-long time (even
if the duration of their stay in the system may be finite, it is
unknown). That is, the total number of stages is T →∝. In
order to be admitted into a blockchain system, a BS should
register its operating bandwidth, the maximal number of sup-
ported ECUs, and the cost per its ECU. The miner should
submit a digital signature for its identification in the system.
If admitted during stage t, the BS and the miner are assigned
with unique labels m /∈M and n /∈ N, and the sets of BSs and
miners are updated as M ← M ∪ {m} and N ← N ∪ {n} at
the beginning of the next mining stage t + 1. Afterwards, each
miner Un can determine the subset Mn of BSs in its range by
adopting conventional pilot signals [44], [45] and, then, starts
participating in block mining. On the other hand, any BSm
and each miner Un that are going to leave at the next stage
t + 1 must notify the system about their leave before the start
of stage t + 1 (note that neither the BSs and nor the miners
can leave the system until the current stage t concludes). In
this case, at the beginning of stage t + 1, the sets of BSs and
miners are updated as M ← M\{m} and N ← N\{n}. As
such, the sets M and N are fixed during one stage, but can
change as we move to the next stage. The information about
the sets M and N is available publicly, i.e., all BSs and all
miners are aware about the updates in the sets M and N.
B. Mining Process
Recall that during mining, each miner Un must perform
a compute-intensive block task which can be offloaded to one
of the BSs in the transmission range of the miner. As such, at
any stage t, the miner must select one BS to offload its task and
the exact BPR with which the task is offloaded. Let bn
t ∈Mn ,
such that bn
t ∈ R, where R is the one-dimensional real vector
space, be a one-dimensional BS decision of miner Un at stage
t. Let xn
t ∈ Xn , such that xn
t ∈ R, be a one-dimensional BPR
decision of miner Un at stage t, where Xn = {1, . . . ,X n} is
a finite discrete set containing all possible BPR decisions of
miner Un , and X n is the maximal BPR that can be selected by
miner Un . The maximal BPR is constrained by the maximal
computing power of the miner’s BS. As such, at any stage t,
we must have
∑
n∈N
1bn
t =mX n ≤ Xm , ∀m ∈M, (1a)
where 1y = 1, if y is true and 0, otherwise. For example, BSm
can restrict the maximal BPR of any miner Un associated with
this BS to stay within
X n ≤
⎢⎢⎢⎣
∑
m∈Mn
1bn
t =mXm/
⎛
⎝
∑
i∈N
1bi
t=m
⎞
⎠
⎥⎥⎥⎦, ∀n ∈ N,
(1b)
where 
 · � is the floor function. Upon deciding on its BS/BPR
at stage t, miner Un sends a connection request to the selected
BS, which allocates a wireless channel of a certain bandwidth
βn to the miner. Then, miners start simultaneously mining the
next block in the chain. During mining, the miner executes
the following operations: 1) processes the block by offloading
all related computations to the selected BS; 2) broadcasts the
produced output to other miners via a selected BS; 3) verifies
accuracy of outputs generated by other miners; 4) broadcasts
verification results to other miners. Any miner which is the first
to generate the output that reaches a consensus obtains a min-
ing reward. Assuming that the PoW protocol is adopted, the
consensus on the generated output is reached only if all miners
agree that the output is correct [4], [5]. However, other options
are also possible. For example, the consensus can be reached
based on weighted voting in which the weight of the miner’s
vote is proportional to its stake submitted by the miner upon
its registration in the system, as in the PoS protocol [4], [5].
Alternatively, only some selected group of miners can be
allowed to validate the output. The group can be selected from
the miners with the highest stakes, as in the DPoS protocol, or
from the miners with the highest reputations, as in the pBFT
protocol [5].
As such, the reward rn
t of miner Un resulting from its BS/
BPR decision (bn
t , xn
t ) at stage t is defined by
rn
t =
{
R + rτn > 0, output is the first to reach consensus,
0, otherwise.
(2)
That is, a positive reward rn
t = R + rτn > 0 is obtained only
if output generated by the miner is the first to reach consensus.
Otherwise, rn
t = 0. As such, we have rn
t ∈ Rn , where Rn =
{0, R+rτn} is a finite discrete set of possible rewards of the
miner. A positive reward R + rτn includes a fixed part R > 0
and a variable part rτn . A variable part is the product of some
given reward factor r > 0 and a block size τn , i.e., the number
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: BAYESIAN REINFORCEMENT LEARNING AND BAYESIAN DEEP LEARNING 323
Fig. 2. Flowchart of the block mining performed by each miner Un .
of transactions per block selected by miner Un . The block
size does not affect the complexity or the cost of its mining.
Nonetheless, the chances of winning a reward decrease when
the block propagates more slowly. That is, if the size τn is too
large, the block is likely to be discarded due to long latency,
which is called orphaning [3], [13]. After the winner of stage t
is determined, a winning block is appended to the blockchain.
At this moment, stage t of the mining process concludes and
the next stage t + 1 begins. A graphical illustration of block
mining performed by miner Un and the respective reward rn
t
is presented in Figure 2, where block mining process is shown
in the form of a flowchart.
C. Winning Probabilities and Delay Factors for Miners
Let b−n
t = {bi
t }i∈N\{n} ∈ M−n and x−n
t =
{x i
t }i∈N\{n} ∈ X−n denote the BSs and BPRs, respec-
tively, selected by all miners except Un . The sets M−n =
×i∈N\{n}Mi and X−n= ×i∈N\{n}Xi contain all possible
BS and BPR decisions, respectively, of all miners except Un .
Then, the probability of a reward rn ∈ Rn for miner Un
given its own BS/BPR decision (bn
t , xn
t ) ∈ Mn × Xn and
a decision profile (b−n
t , x−n
t ) ∈M−n ×X−n of other miners
is defined by
Pr
{
rn
∣∣bn
t , xn
t ,b−n
t , x
−n
t
}
=
{
Pn
s
(
xn
t , x−n
t
)(
1−Pn
o
(
bn
t ,b−n
t
))
, rn = R+rτn ,
1− Pn
s
(
xn
t , x−n
t
)(
1−Pn
o
(
bn
t ,b−n
t
))
, rn = 0,
(3a)
where Pn
s is the probability that miner Un is successful in
producing the output first, which is proportional to the miner’s
relative BPR, i.e.,
Pn
s
(
xn
t , x−n
t
)
= xn
t /
⎛
⎝
∑
i∈N
x i
t
⎞
⎠; (3b)
Pn
o is the orphaning probability which depends on the latency
in the wireless channel between miner Un and its selected
BS. Similar to [3], [9], [13], we can assume that the occur-
rence of executing any block task is a random variable
following a Poisson distribution with the mean of 1/Tb , where
Tb is the expected block interval time (in the PoW proto-
col, Tb = 600 seconds). Then, the probability Pn
o can be
approximated as [3], [9], [13]
Pn
o
(
bn
t ,b−n
t
)
= 1− exp
(−τnzn(bn
t ,b−n
t
)
/Tb
)
, (3c)
where zn > 0 is the delay factor, i.e., transmission time per
transaction of miner Un which reflects time/space-dependent
features of a wireless channel between the miner and its BS.
As such, the delay factor is inversely proportional to the
transmission rate of a wireless channel allocated to miner Un
by its selected BS. That is,
zn(bn
t ,b−n
t
)
= zn
t =
1/βn
(
bn
t ,b−n
t
)
log2
(
1 + SINRn
(
bn
t
)) , (4a)
where SINRn is signal-to-interference-plus-noise ratio (SINR)
over the channel. Assuming that spectrum resources allocated
by the BS are distributed equally among miners offloading
their block tasks, the bandwidth βn of a channel allocated to
miner Un is given by
βn(bn
t ,b−n
t
)
=
∑
m∈Mn
1bn
t =m
⎛
⎝Bm/
∑
i∈N
1bi
t=m
⎞
⎠. (4b)
Note that since no miner Un knows the BS decisions of other
miners, it cannot estimate the values of βn directly according
to (4b). On the other hand, SINRn is given by
SINRn(bn
t ) =
∑
m∈Mn
1bn
t =mpnGn,m
∑
i∈M\{m} oi ,m p̃i G̃ i ,m+σ2
, (4c)
where oi ,m ∈ {0, 1} is the binary bandwidth overlap indica-
tor, such that oi ,m = 1, if the spectrum band of BSm overlaps
with the spectrum band of BSi ; pn and Gn,m are the trans-
mit power of miner Un and link gain of wireless channel
between miner Un and BSm , respectively; p̃i and G̃ i ,m are
the average transmit power of miners associated with BSi and
average link gain of wireless channels between the miners
associated with BSi and BSm , respectively; σ2 is the vari-
ance of a zero-mean additive white Gaussian noise (AWGN)
power. Note that equation in (3a) presumes the interference
among miners’ transmissions, although the interference-free
scenario is also possible. Any miner Un knows the values of
pn and Gn,m and can estimate the values of p̃i , G̃ i ,m , oi ,m
and σ2, e.g., based on the channel statistics. Hence, the miner
can always compute the value of SINRn according to (4c)
when selecting its offloading BS.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
324 IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, VOL. 7, NO. 1, MARCH 2021
Accordingly, at any stage t, every miner Un selects one
BS, bn
t ∈Mn , to offload its task with the BPR xn
t ∈ Xn over
a wireless channel with the bandwidth βn . As such, since
all system parameters (including the numbers of miners and
BSs, locations of miners, and channel and computing resource
allocations) are constant during each stage, the connectivity of
the blockchain network is always preserved within one mining
stage, i.e., there are no handovers or access blockages for the
miners.
D. Expected Stage and Long-Term Payoffs of Miners
The instantaneous stage payoff that miner Un receives at the
end of stage t from its BS/BPR decision (bn
t , xn
t ) ∈Mn×Xn
resulting in the reward rn
t ∈ Rn is expressed by
un(bn
t , xn
t
∣∣rn
t
)
= rn
t −
∑
m∈Mn
1bn
t =mcmxn
t . (5)
The payoff in (5) is a function of the miner’s BS/BPR decision
(bn
t , xn
t ) and a random reward rn
t . From (3a), the probability
of winning a reward (in (3a)) depends not only on the miner’s
own BS/BPR decision (bn
t , xn
t ) but also on the decision profile
of other miners (b−n
t , x−n
t ), which is unobservable by miner
Un . Since miner Un is uncertain about the accurate values of
(b−n
t , x−n
t ), it cannot compute its payoff directly based on (5)
when making its BS/BPR decision. Nonetheless, the miner can
construct its own estimate of (b−n
t , x−n
t ). This estimate, also
called a “belief”, represents the joint probability distribution
Bn
t = {Bn
t (b−n
t , x−n
t )}(b−n
t ,x−n
t )∈M−n×X−n , where
Bn
t
(
b−n
t , x−n
t
)
=
∏
i∈N\{n}
Bn
t
(
bi , x i
)
= Pr
{(
b−n
t , x−n
t
)
=
(
b−n , x
−n
)}
=
∏
i∈N\{n}
Pr
{(
bi
t , x
i
t
)
=
(
bi , x i
)}
is the probability that miner Un assigns to other miners hav-
ing a decision profile (b−n , x
−n) at stage t; Bn
t (x i ) is the
marginal probability.
Given its current belief Bn
t , any miner Un can estimate the
expected (w.r.t. belief) stage payoff from its BS/BPR decision
(bn
t , xn
t ), as in
U n(Bn
t , bn
t , xn
t ) =
∑
(b−n
t ,x−n
t )∈M−n×X−n
Bn
t
(
b−n
t , x−n
t
)
× U n(bn
t , xn
t
∣∣ b−n
t , x−n
t
)
, (6a)
where U n(bn
t , xn
t
∣∣ b−n
t , x−n
t ) is the expected stage payoff of
miner Un from its BS/BPR decision (bn
t , xn
t ) given a decision
profile (b−n
t , x−n
t ) of other miners, defined as
U n
(
bn
t , xn
t
∣∣ b−n
t , x−n
t
)
=
∑
rn∈Rn
Pr
{
rn
∣∣ bn
t , xn
t , b−n
t , x−n
t
}
un (bn
t , xn
t | rn )
= xn
t
∑
m∈Mn
1bn
t =m
(
R + rτn
∑
i∈N x i
t
exp
(−τnzn
(
bn
t , b−n
t
)
/Tb
) − cm
)
.
(6b)
Next, note that at any stage t, miner Un aims to maximize the
payoff from its BS/BPR decision (bn
t , xn
t ). Thus, if the miner
is myopic or short-sighted, it will select a decision (bn
t , xn
t ) =
argmax(bn ,xn ) U n(Bn
t , bn , xn) maximizing its expected
stage payoff U n in (6a). If the miner is long-visional, it will
select a decision (bn
t , xn
t ) = argmax(bn ,xn ) V n(Bn
t , bn , xn)
maximizing its expected long-term payoff V n that is defined
by an infinite discounted sum of its stage payoffs (as the miner
stays in the system for an indefinitely-long time) [46], i.e.,
V n(Bn
t , bn , xn)
= E
{ ∝∑
τ=t
γτ−tun(bn
τ , xn
τ | rn
τ )
∣∣∣∣B
n
t , bn
t = bn , xn
t = xn
}
= E
{ ∝∑
τ=t
γτ−tU n(Bn
τ , bn
τ , xn
τ )
∣∣∣∣B
n
t , bn
t = bn , xn
t = xn
}
,
(7)
where γ ∈ (0, 1] is a discounting factor.
As such, the interactions of miners can be modeled as
a stochastic game played repeatedly at every stage t by the
set N of miners. At the beginning of stage t, each miner Un
takes an action, i.e., the BS/BPR decision (bn
t , xn
t ) that max-
imizes its expected stage payoff U n or long-term payoff V n .
The miners select their actions simultaneously and indepen-
dently, and do not exchange information about their actions.
As a result, no miner Un can observe the actions (b−n
t , x−n
t )
taken by other miners, i.e., information about miners’ actions
is incomplete. To deal with this issue, the miner forms its
belief Bn
t about actions (b−n
t , x−n
t ). The game is stochastic,
since the miner’s payoff is defined by the random reward rn
t
which depends on the unobservable actions (b−n
t , x−n
t ). In
the following, we formulate a game for the miners, develop
a POMDP model of miners’ decisions to find the game solu-
tion, and propose the novel BRL and BDL framework that
enables all miners to update their beliefs about actions of other
miners and find their optimal strategies.
IV. INTERACTIONS OF MINERS IN THE BLOCKCHAIN
A. Stochastic Game for Miners With Incomplete Information
From the description of the mining process in Section III,
the interactions of the miners can be modeled as a stochastic
non-cooperative game with unobservable actions. The game is
played repeatedly by the set N of players/miners at every stage
t. Recall that within any stage t, the set N is fixed, but can
change at the beginning of the next stage t + 1 due to arrivals
and departures of miners during stage t. Information about the
set N is available publicly, i.e., all players know the current
set N. At the beginning of stage t, each player/miner Un real-
izes its action, i.e., BS/BPR decision an
t = (bn
t , xn
t ) ∈ An =
Mn × Xn , where An = Mn × Xn is the action space of the
player that combines the set Mn of its possible BSs’ decisions
(i.e., set of BSs in the range of miner Un ) and set Xn of its
possible BPR decisions. All players select their actions simul-
taneously and independently, without exchanging information
about their action. As a result, no player Un can observe the
actions a−n
t = {ai
t }i∈N\{n} ∈ A−n = ×i∈N\{n}Ai taken by
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: BAYESIAN REINFORCEMENT LEARNING AND BAYESIAN DEEP LEARNING 325
Fig. 3. Observable part of the Markov chain of player Un , where the tran-
sition of state sn = rnt depends on the player’s action an
t = (bnt , xn
t ) and
the unobservable actions a−n
t = (b−n
t , x−n
t ) of other players.
other players, i.e., the information about a−n
t is incomplete.
Hence, the player forms its belief Bn
t about unobservable
actions. After the actions at = {an
t }n∈N ∈ A = ×n∈NAn
are taken by all players, the game moves to some new random
state s = {sn}n∈N ∈ S = ×n∈NSn , where sn = rn
t ∈ Sn is
the state of player Un represented by the reward rn
t received
by the player at the end of stage t. Accordingly, the player’s
state space Sn= Rn = {0, R + rτn} comprises two possible
states, sn = rn
t = 0 and sn = rn
t = R +rτn > 0 with tran-
sitions defined according to (3a), as shown in Figure 3. The
state sn = rn
t is observable by player Un at the end of stage
t, i.e., after all players execute their actions at = (an
t , a−n
t ),
but unknown at the beginning of stage t, i.e, before the actions
at are taken. As such, the state space Sn is fully-observable
but stochastic.
Based on the above, the proposed game is defined by
the tuple Γ = (N,An ,Ωn ,Sn ,Pr,un) with the following
elements:
- set of N players/miners N and, for each player/miner Un ,
- set of actions An = Mn×Xn which is equivalent to the
set of possible BS/BPR decisions of the player;
- partially-observable state space Ωn = Sn × A−n com-
bining the observable and unobservable state spaces,
Sn= Rn and A−n , where the observable state space is
represented by the set Rn of possible player’s rewards
and unobservable state space is represented by the set
of possible action profiles of other players;
- state transition dynamics Pr{śn , á−n
∣∣ sn , an , a−n},
i.e., the probability of transiting to state (śn , á−n) =
(ŕn , b́−n , x́−n ) ∈ Ωn given the action an =
(bn , xn ) ∈ An executed by the player in state
(sn , a−n ) = (rn ,b−n , x−n ) ∈ Ωn that can be further
factored into two probabilities, Pr{śn
∣∣ sn , an , a−n}
and Pr{á−n
∣∣ a−n};
- instantaneous stage payoff un(sn , an , śn) of the player
from its action an = (bn , xn ) ∈ An taken in state
sn = rn ∈ Sn when transiting to state śn = ŕn ∈ Sn ,
that is equivalent to the player’s payoff un(bn , xn | ŕn)
in (5) from its BS/BPR decision (bn , xn) ∈ Mn × Xn
resulting in reward ŕn ∈ Rn .
Note that in conventional stochastic games with observable
actions, the player’s pure strategy αn : Ωn → An is a mapping
from the state space Ωn to action space An [46]. In our game,
however, the player’s state space Ωn comprises two subsets,
Sn and A−n , where the latter subset is unobservable by player
Un . Therefore, the player’s pure strategy αn(Bn , sn ) = an is
a mapping from the belief Bn and observable state sn ∈ Sn
to the action an ∈ An . The game proceeds as follows: 1) at
the beginning of stage t, the game is in state sn ∈ Sn ; 2) after
observing the state sn , the player selects its pure strategy
αn(Bn
t , sn) = an
t which maps from its current belief Bn
t and
state sn ∈ Sn to action an
t ∈ An ; 3) after the action an
t ∈ An
is taken, the game moves to a new random state śn ∈ Sn and
the player receives the payoff un(sn , an
t , śn). This procedure
is repeated in the new state and the game continues for an
infinite number of stages T →∝.
B. Solution Concepts for the Game
One possible solution concept applicable to game Γ is
the Bayesian Nash equilibrium (BNE) [44]. In words, in the
BNE, no player believes that it can increase its expected
stage payoff by deviating from its pure strategy. Formally,
BNE is the tuple (α,B) that comprises a pure strategy pro-
file α = {αn}n∈N ∈ A = ×n∈NAn and the players’ beliefs
B = {Bn}n∈N, such that
U n(Bn
, αn
∣∣ sn) ≥ U n
(
Bn
, an
∣∣∣ sn
)
,
∀an ∈ An ,∀sn ∈ Sn , (8)
for each n ∈ N, where U n(Bn , an |sn) is the expected (w.r.t.
belief Bn ) stage payoff of player Un from the action an =
(bn , xn ) executed in state sn = rn , which is equivalent to the
expected payoff U n(Bn , bn , xn ) defined in (6a) and given by
In (9a), the probability Pr{śn |sn , an , a−n} is equivalent
to the probability Pr{ŕn |bn , xn ,b−n , x−n} defined in (3a),
i.e., (9b), as shown at the bottom of the page, U n(an |sn , a−n )
is the expected stage payoff of player Un from the action
U n(Bn , an
∣∣sn) =
∑
śn∈Sn
Pr
{
śn
∣∣sn , an ,Bn}un(sn , an , śn)
=
∑
a−n∈A−n
Bn(a−n) ∑
śn∈Sn
Pr
{
śn
∣∣sn , an , a−n}un(sn , an , śn)
=
∑
a−n∈A−n
Bn(a−n)U n(an
∣∣sn , a−n) (9a)
Pr
{
śn
∣∣sn , an , a−n} =
{
Pn
s
(
an , a−n
)(
1− Pn
o
(
an , a−n
))
, śn = ŕn = R + rτn
1− Pn
s
(
an , a−n
)(
1− Pn
o
(
an , a−n
))
, śn = ŕn = 0 (9b)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
326 IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, VOL. 7, NO. 1, MARCH 2021
an = (bn , xn ) taken in state sn = rn given actions
a−n = (b−n , x−n) of other players, which is equivalent to the
expected payoff U n(bn , xn |b−n , x−n) in (6b) and given by
U n(an
∣∣sn , a−n) =
∑
śn∈Sn
Pr
{
śn
∣∣sn , an , a−n}
× un(sn , an , śn). (9c)
Note that since the number of players N and their state/action
space S × A are finite, at any mining stage, game Γ admits
a BNE [44].
Unfortunately, the concept of BNE is rather incomplete for
analysis of stochastic games played repeatedly over multiple
stages, as it puts no restrictions on the dynamics (i.e., updating
rule) of players’ beliefs [14], [44], [45]. In order to refine the
solution generated by BNE, the concept of a myopic perfect
Bayesian equilibrium (MPBE) [14], [45] can be utilized. In
words, MPBE is the BNE where, at every stage, the players’
beliefs are consistent, i.e., updated with Bayesian inference,
and their pure strategies are sequentially rational, i.e., for
any information set or history (sn
0 , an
0 , . . . , sn
t−1, a
n
t−1, s
n
t ),
the pure strategies maximize expected stage payoffs of the
players [14], [45]. Formally, MPBE is the tuple (α,B) that
satisfies (8), for all n ∈ N, where the belief Bn is updated
based on the last-observed transition (śn , an , sn) according
to the Bayes’ rule:
Bn(
a−n)=
Pr
{
śn
∣∣ sn , an , a−n
}
Bn
(
a−n
)
∑
á−n∈A−n Pr
{
śn
∣∣ sn , an , á−n
}
Bn(á−n)
,
(10)
for all a−n ∈ A−n . Similar to BNE, game Γ admits a MPBE,
as the number of players and their state/action space are
finite [44].
Note that in the MPBE, the players are myopic, i.e., short-
sighted, because they only select the strategies that maximize
their one-stage (rather than the long-term) expected payoffs. In
this regard, the notion of a perfect Bayesian equilibrium (PBE)
is more suitable for game Γ [14], [44]. In words, PBE is the
MPBE where the player selects a pure strategy that maximizes
its value, i.e., expected long-term payoff [14], [44]. Formally,
PBE is the tuple (α,B), such that
V n(Bn
, αn
∣∣ sn) ≥ V n(Bn
, an
∣∣ sn),
∀an ∈ An ,∀sn ∈ Sn , (11)
for all n ∈ N. In (11), V n(Bn , an |sn) is the value or
expected (w.r.t. belief Bn ) long-term payoff of player Un from
its action an = (bn , xn ) taken in state sn = rn , which can
be defined, according to (7), as
V n(Bn , an | sn)
= E
{ ∞∑
t=0
γtun(sn , an , śn )
∣∣∣∣∣ B
n , an , sn
}
=
∑
śn∈Sn
Pr{śn | sn , an ,Bn}
×
(
un(sn , an , śn)+γV n
(
Bsn ,śn
an , an
∣∣∣ śn
))
= E
{ ∞∑
t=0
γtU n(sn , an , śn)
∣∣∣∣∣ B
n , an , sn
}
= U n(Bn , an | sn)
+ γ
∑
śn∈Sn
Pr{śn | sn , an ,Bn}V n
(
Bsn ,śn
an , an
∣∣∣ śn
)
=
∑
a−n∈A−n
Bn(a−n)V n(an
∣∣ sn , a−n), (12a)
where for every transition (sn , an , śn) and all a−n ∈ A−n ,
the belief Bsn ,śn
an is updated according to the Bayes’ rule
in (10); V n(an |sn , a−n ) is the expected long-term payoff of
player Un from the action an = (bn , xn) executed in state
sn = rn given actions a−n = (b−n , x−n) of other players,
defined by
given actions a−n = (b−n , x−n) of other players
V n(an
∣∣ sn , a−n)
= E
{ ∞∑
t=0
γtun(sn , an , śn)
∣∣∣∣∣ a
n , sn , a−n
}
=
∑
śn∈Sn
Pr
{
śn
∣∣ sn , an , a−n}
× (un(sn , an , śn)+γV n(an
∣∣ śn , a−n))
= E
{ ∞∑
t=0
γtU n(sn , an , śn)
∣∣∣∣∣ a
n , sn , a−n
}
= U n(an
∣∣ sn , a−n)
+ γ
∑
śn∈Sn
Pr
{
śn
∣∣ sn , an , a−n}V n(an
∣∣ śn , a−n).
(12b)
Similar to the BNE and MPBE, game Γ admits a PBE,
as the number of players and their state/action space are
finite [44]. However, the computation of the PBE or MBPE
is non-deterministic polynomial time (NP) hard, because it is
at least as complex as the computation of the BNE, which
is an NP-hard problem [44]. Thus, instead of directly com-
puting the PBE, in the next section, we formulate a POMDP
model of the miner’s decision, the solution of which represents
a PBE. Based on the POMDP, we develop the BRL algorithm
which allows each miner to reach this solution independently,
i.e., without communicating with other miners.
C. Bayesian Reinforcement Learning Algorithm for Miners
In repeated game settings, any player/miner can estimate
the unknown state transition dynamics and update its belief
about unobservable actions of other players by adopting
stochastic dynamic programming and RL. Accordingly, in the
following, we develop the BRL framework that allows all
players/miners to select their optimal strategies and infer the
distributions of unobservable actions and observable states
during repeated interactions with each other and a stochas-
tic environment. We start by defining a POMDP that will be
used to further develop a RL model. Formally, a POMDP is
the tuple (Ω,S,A,Pr, u, γ) with the following elements [46]:
i) partially-observable state space Ω = S × Θ combining the
observable and unobservable state spaces, S and Θ; ii) action
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: BAYESIAN REINFORCEMENT LEARNING AND BAYESIAN DEEP LEARNING 327
space A; iii) state transition dynamics Pr{ś, θ́|s, a, θ} =
Pr{ś|s, a, θ}Pr{θ́
∣∣∣ θ} defining the probability of transiting
to state (ś, θ́) ∈ Ω given the action a ∈ A taken in state
(s, θ) ∈ Ω; iv) payoff u(s, a, ś) from the action a ∈ A
taken in state s ∈ S when transiting to state ś ∈ S; v) dis-
counting factor γ ∈ (0, 1] . As the transition Pr{θ́|θ} is not
directly observable, a learner relies only on the observable
transition (s, a, ś) to infer an underlying distribution or belief
B = {B(θ)}θ∈Θ, for B(θ) = Pr{θ}. The RL problem it to
find the optimal pure strategy α(B) ∈ A that maximizes the
value V α(B) =
∑∝
t=0 γtu(α(Bt ), st ) in all beliefs states,
i.e., V α(B) ≥ V α(B), for all α and B , and satisfies the
Bellman optimality equation [46]:
V α(B) = max
a∈A
∑
ś∈S
Pr{ś | s, a,B}
×
(
u(s, a, ś) + γV α
(
Bs,ś
a
))
, (13)
where the belief Bs,ś
a is updated according to Bayes’ rule.
Apparently, the problem of finding optimal strategies of the
players in a stochastic game with incomplete information can
be cast as a POMDP [46], [47]. In particular, when applied
to game Γ, the POMDP for each player Un is defined by the
tuple (Ωn ,Sn ,An ,Pr, un , γ), with the elements described
in Section IV. The RL problem is to find the optimal strat-
egy αn(Bn , sn) ∈ An which maximizes the player’s value,
i.e., expected long-term payoff V n(Bn , an |sn) defined in
(12a). This problem can be solved with stochastic dynamic
programming [45], [46]. In this case, we obtain the BRL algo-
rithm where at every stage t, we store the set of |Sn × A|
updated values V n
t+1(a
n |sn , a−n ) indexed by the state sn ∈
Sn and actions (an , a−n ) ∈ A. The proposed algorithm is
implemented “on-line” – it is repeated at each stage t of the
mining/learning process and does not require a prior “off-line”
training and/or pre-existing training datasets. At the beginning
of stage t, for each player/miner Un , we perform the following
steps.
Step 1: Select the optimal pure strategy αn(Bn
t , sn ) for
the current belief Bn
t and state sn by solving the Bellman
equation
αn(Bn
t , sn ) = argmax
an∈An
V n
t+1(B
n
t , an | sn), (14a)
where V n
t+1(B
n
t , an |sn) is given by (14b), as shown at the
bottom of the page. V n
t+1(a
n |sn , a−n) is estimated based on
the last-updated value V n
t (an |śn , a−n) according to (14c), as
shown at the bottom of the page, and Pr{śn |sn , an , a−n} is
defined in (9b);
Step 2: Take action an
t = αn(Bn
t , sn), observe a result-
ing state śn . For the observed transition (sn , an
t , śn), update
the belief Bn
t+1(a
−n) = Bsn ,śn
an
t
(a−n ) with the Bayes’ rule
in (10), for all a−n ∈ A−n .
Note that the above algorithm does not need any addi-
tional exploration, because it is implicit in the computation
of value V n
t+1(B
n
t , an |sn
t ) in (14b). Nevertheless, when the
exploration ability is limited, e.g., due to a finite recursion
depth, it may be useful to utilize some explicit exploration
technique [46]. For example, we can adopt a simple ε-greedy
exploration where, for a certain arbitrary small ε ∈ [0, 1), the
optimal strategy an
t = αn(Bn
t , sn ) is selected with proba-
bility 1 − ε, and a non-optimal strategy án
t ∈ An\{an
t } is
selected with probability ε. Proposition 1 below shows that
the proposed BRL algorithm that represents the sequence of
iterations {V n
t }t∈N
converges to the optimal value V n
, i.e.,
value of an optimal pure strategy α.
Proposition 1: The BRL algorithm defined by the sequence
of iterations {V n
t }t∈N
, for all n ∈ N, converges to the optimal
value V n
with probability one as time tends to infinity.
The proof of Proposition 1 is given in Appendix C in
the supplementary material. From Proposition 1, we obtain
Corollary 1 that establishes that our BRL algorithm converges
to a stable state where the players’ pure strategies and beliefs
are in the PBE of game Γ.
Corollary 1: The BRL algorithm defined by the sequence
of iterations {V n
t }t∈N
, for all n ∈ N, converges to a stable
state in which the players’ pure strategies and beliefs (α,B)
are in the perfect Bayesian equilibrium of game Γ.
The proof of Corollary 1 is given in Appendix D in the
supplementary material. Finally, in Proposition 2 below, we
analyze the worst-case computational complexity and the rate
of convergence of the BRL algorithm.
Proposition 2: The BRL algorithm defined by the sequence
of iterations {V n
t }t∈N
, for all n ∈ N, yields the worst-case
time complexity of O(cN ), for c = |maxn∈N An | and an
asymptotic convergence rate equal to O(
√
log(log t)/t), for
γ ≤ 0.5, and O(1/t1−γ), for γ > 0.5.
The proof of Proposition 2 is provided in Appendix E in
the supplementary material.
V. BAYESIAN DEEP LEARNING BY THE MINERS
From Proposition 2, the convergence rate of the proposed
BRL algorithm is sublinear with order 1−γ ∈ (0, 0.5) and its
complexity is exponential in the number of miners N. Hence,
although the BRL algorithm converges to an exact solution of
the POMDP, it is intractable for the large number of miners N
and, to be practically-realizable, it requires an approximation.
Accordingly, in this section, we devise a novel BDL algorithm
where uncertainties about unobservable states of the POMDP
are approximated by the BNN. This enables us to reduce the
V n
t+1
(
Bn
t , an
∣∣sn) =
∑
śn∈Sn
Pr
{
śn
∣∣sn , an ,Bn
t
}(
un(sn , an , śn )+γV n
t
(
Bsn
t ,śn
an , an
∣∣śn
))
=
∑
a−n∈A−n
Bn
t
(
a−n)V n
t+1
(
an
∣∣sn , a−n) (14b)
V n
t+1
(
an
∣∣sn , a−n) =
∑
śn∈Sn
Pr
{
śn
∣∣sn , an , a−n}(un(sn , an , śn)+γV n
t
(
an
∣∣śn , a−n)) (14c)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
328 IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, VOL. 7, NO. 1, MARCH 2021
size of the unobservable state space and, thus, the complexity
of a learning process. In addition, the proposed BDL algorithm
is unsupervised, i.e., it allows the type of self-organized “on-
line” learning that does not need any prior “off-line” training
and/or pre-existing training datasets. The rest of this section is
organized as follows. In Section V-A, we describe the existing
BDL methods for Bayesian inference modeling with BNNs.
In Section VII-B, we construct the BNN model of the miners’
decisions. In Section VII-C, we develop the BDL algorithm.
A. Inference Approximation With Bayesian Neural Networks
DL algorithms have a proven record in produc-
ing rather accurate solutions for fully-observable
MDPs [3], [11], [14], [42], [48]. However, they are not
effective when applied to POMDPs [14], [49], [50]. The
reason is that the DL methods are based on conventional
NNs that cannot capture epistemic uncertainties about model
parameters which are inherent in POMDPs where such
uncertainties are defined as probability distributions over
unobservable states (i.e., the beliefs). As a result, although
the DL algorithms converge to some fixed values, in gen-
eral, these values are rather far from the optimal POMDP
solutions (a detailed explanation of reasons behind the poor
performance of DL algorithms in POMDPs is given in [49]).
Fortunately, epistemic uncertainties can be efficiently tracked
with BDL methods where they are modeled with BNNs
– NNs with random weights following some predefined
prior distribution. Then, instead of optimizing the network
weights directly, as in DL algorithms, we average over all
possible weights, which is known as a marginalization [51].
In the following, we describe the existing BDL methods for
Bayesian inference approximation with BNNs.
Consider a dataset (X, Y) = {(xt , yt )}t=1,...,F that con-
sists of F data points (xt , yt ), where xt ∈ R
1×DI is the row
vector of inputs with DI elements and yt ∈ R
1×DO is the
row vector of outputs, i.e., targets, with DO elements. Let
Pr{yt
∣∣ f W(xt )} be the likelihood of a data point (xt , yt ),
where f W(xt ) = ŷt ∈ R
1×DO is a random row output vector
of the BNN model with some prior distribution over its weights
W, e.g., a standard normal distribution Pr{W} = N (0, I ).
In regression tasks, the likelihood is usually represented by
the Gaussian distribution Pr{yt
∣∣ f W(xt )} = N (f W(xt ), σ2),
where the mean f W(xt ) is defined by the model and a given
observation noise variance σ2. In classification tasks, the
observed target output yt ∈ Y = {1, . . . ,DO} represents the
label in the set Y = {1, . . . ,DO}. Thus, the random out-
put f W(xt ) = ŷt = (ŷ1
t , . . . , ŷDO
t ) ∈ R
1×DO of the BNN
model is predicted being classified with a label in the set
Y = {1, . . . ,DO}. To utilize the model for classification, we
pass the output through an element-wise softmax function to
obtain the likelihood of the output label yt = i ∈ Y, given by
Pr
{
yt = i
∣∣∣∣f
W(xt )
}
= Softmax
(
i , f W(xt )
)
=
e ŷ i
t
∑DO
j=1 e ŷ j
t
, (15)
for each i ∈ Y [49], [51].
Given the likelihood Pr{yt |f W(xt )}, we are interested in
estimating the posterior over model weights W for a data set
(X, Y), i.e., the probability Pr {W|X, Y}. Note that typically,
a posterior is updated with the Bayesian inference Pr {W|X,
Y} = Pr {W}Pr {Y|X,W}/Pr{Y|X} . Unfortunately, in the
BNNs, the computation of inference is intractable, because
the marginal probability Pr {Y|X} involved in the estima-
tion of Pr {W|X, Y} cannot be evaluated analytically [49].
Hence, instead of direct inference, in Bayesian modeling,
a variational inference [49], [51] is utilized where the posterior
Pr {W|X, Y} is fitted with some simple distribution qθ(W)
parameterized by θ, such that Pr{W|X, Y}∼ qθ(W), where
qθ is the optimal approximating distribution. This replaces
an intractable problem of averaging over model weights W
with a simpler problem of optimizing the parameters θ. More
specifically, the optimal parameters θ should minimize the
Kullback-Leibler (KL) divergence to a true posterior Pr {W|X,
Y}, i.e.,
qθ(W) = min
θ
DKL
(
qθ(W)
∣∣∣ Pr{W | X, Y})
)
, (16a)
where
DKL
(
qθ(W)
∣∣∣ Pr{W | X, Y})
)
=
∫
qθ(W)log
qθ(W)
Pr{W | X, Y}dW (16b)
is the KL divergence, i.e., the difference between qθ(W)
and Pr {W|X,Y}. Note that the minimization of a KL diver-
gence is equivalent to the maximization of the evidence lower
bound (ELBO):
ELBO(θ) =
∫
qθ(W)log Pr{Y | X, W}dW
− D
(
qθ(W)
∣∣∣ Pr{W})
)
=
F∑
t=1
∫
qθ(W)log Pr
{
yt
∣∣∣ f W(xt )
}
dW
− DKL
(
qθ(W)
∣∣∣ Pr{W})
)
≤ log Pr{Y | X}, (17)
where log Pr{Y|X} is the log-evidence. Maximizing the first
term in (17) is equivalent to maximizing the expected log-
likelihood. Maximizing the second term in (17) is equivalent
to minimizing a KL divergence between a distribution qθ(W)
and a prior Pr{W}.
A practical approach to approximate inference in large and
complex BNNs is a dropout variational inference [49], where
dropout is interpreted as a variational Bayesian inference and
a distribution qθ(W) is modeled as a mixture of two Gaussians
with small variances and the mean of one of Gaussians fixed
at zero. The objective is to minimize the loss, given by [49]
L(θ) = −ELBO(θ)
= −
F∑
t=1
∫
qθ(W)log Pr
{
yt
∣∣∣ f W(xt )
}
dW
+ DKL
(
qθ(W)
∣∣∣ Pr{W})
)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: BAYESIAN REINFORCEMENT LEARNING AND BAYESIAN DEEP LEARNING 329
= −
F∑
t=1
log Pr
{
yt
∣∣∣ f Ŵt (xt )
}
+
∫
qθ(W)log
qθ(W)
Pr{W}dW, (18a)
where the weights Ŵt are distributed according to qθ(W),
i.e., Pr{Ŵt} = qθ(W). Note that in the classification tasks,
a log-likelihood of an observed output label yt = i ∈ Y is
given by
log Pr
{
yt = i
∣∣∣∣f
Ŵt (xt )
}
= log
(
Softmax
(
i , f Ŵt (xt )
))
,
(18b)
where f Ŵt (xt ) = ŷt = (ŷ1
t , . . . , ŷDO
t ). Thus, to find the
optimal parameters θ of qθ(W), we should estimate the deriva-
tives of loss L(θ) w.r.t. θ, which can be done with Monte
Carlo (MC) estimation [49], [50].
A rather efficient MC estimators is the path-wise derivative
estimator (PDE) [49], also called the infinitesimal perturbation
analysis, stochastic back-propagation, or a re-parameterization
trick. In PDE, qθ(W) is re-parameterized as a parameter-
free distribution q(ε) subject to Ŵt = g(θ, ε), where ε > 0
is an infinitesimal number; g(·, ·) is a deterministic differen-
tiable bivariate transformation, e.g., for Gaussian distribution
qθ(W) = N (μ, σ2), with θ = (μ, σ), we can set Ŵt =
g(θ, ε) = μ + σε and q(ε) = N (0, I ). Then, it can be shown
that for a normal prior Pr{W} = N (0, I ), the loss takes
a simpler form [49]:
L(θ, λ) =
1
2
(1− λ)‖θ‖2 −
F∑
t=1
log Pr
{
yt
∣∣∣ f g(θ,ε)(xt )
}
,
(19a)
where λ ∈ [0, 1] is a dropout probability which also must be
optimized. In the classification tasks, a log-likelihood of any
observed output label yt = i ∈ Y takes the form:
log Pr
{
yt = i
∣∣∣ f g(θ,ε)(xt )
}
= log
(
Softmax
(
i , f g(θ,ε)(xt )
))
. (19b)
where f g(θ,ε) = ŷt = (ŷ1
t , . . . , ŷDO
t ).
Based on the theoretical findings in [49], if the number of
data points F in the data set (X, Y) is sufficiently large, then
i) the loss L(θ, λ) in (19a) approaches the loss L(θ) in (18a),
i.e,
L(θ, λ) −→
F→∞
L(θ) = −ELBO(θ)
= DKL
(
qθ(W)
∣∣∣ Pr{W | X, Y})
)
,
(20a)
ii) weights Ŵt generated according to optimal approxi-
mating distribution qθ(W) = qθ(W) approach weights Wt
updated with Bayesian inference. Thus, if F is sufficiently
large, we have Ŵt −→Wt and⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Pr{W | X, Y} −→
F→∞
qθ(W)= Pr
{
Ŵ
}
,
Pr{y | x, X, Y} −→
F→∞
∫
Pr
{
y
∣∣∣ f Ŵ(x)
}
qθ(W)dW = qθ(y | x),
Pr
{
Y
∣∣∣ X, Ŵ
}
−→
F→∞
Pr{Y | X}.
(20b)
B. Bayesian Neural Network Model of the Miners’ Decisions
To summarize the presentation in Section V-A, the existing
BDL framework provides the means to model and accurately
approximate Bayesian inference with the BNNs. In particular,
given the observable dataset (X,Y) = {(xt , yt )}t=1,...,F and
the likelihood Pr{yt‖f W(xt )} of every data point (xt , yt ), in
the BDL framework, a posterior Pr{W|X, Y} over random
BNN weights W is approximated by some simple distribution
qθ parameterized by θ. This replaces the intractable computa-
tion of Bayesian inference with a simpler problem of optimiz-
ing parameters θ in a way which minimizes the approximation
loss L, so that Pr{W|X, Y} −→
F→∞
qθ(W) [49]. However,
none of the existing BDL methods shows how to adopt
the BNN-based inference approximation in practical learn-
ing. That is, they do not address the application of this BNN
approximation in the settings when the learner must not only
accurately model the Bayesian inference, but also determine an
optimal strategy that maximizes the learner’s value or expected
payoff. As such, in the practical BDL algorithm, a BNN model
should be utilized not only to represent Bayesian inference, but
also to estimate the expected payoffs of a learner from each
of its actions. In the following, we formulate such a practi-
cal BDL algorithm and apply it for decision making of each
player/miner Un in game Γ. More specifically, we: i) devise
a BNN model f W(x) which approximates unobservable state
transitions of the POMDP of the player; ii) show how to
approximate the player’s belief Bn and its value or expected
stage payoff U n with the BNN model f W(x); iii) develop
a BDL algorithm where the player selects a pure strategy αn
that maximizes its approximated value U n and, at the same
time, the random weights W of the BNN are updated to min-
imize the approximation loss L of the model; iv) prove that
our BDL algorithm converges to the stable state in which the
value U n of player Un is optimal and the pure strategies of
all players are in the myopic PBE of game Γ.
The proposed BDL algorithm is unsupervised, i.e., does
not require a prior “off-line” training and/or pre-existing
datasets. The algorithm exploits a feed-forward BNN model
f W(x) to approximate unobservable state transitions of the
POMDP of every player/miner Un . At any stage t, the
dataset (Xt , Yt )= {(xi , yi )}i=t−T ,...,t−1 represents the his-
tory about T = min(t ,F ) past consecutive data points
(xi , yi ), where F is the size of the observation window. A row
input vector xi = (x0
i , . . . , x3
i ) = (an
i , sn
i ) ∈ R
1×DI consists
of DI = 3 elements that define the player’s action an
i =
(bn
i , xn
i ) ∈ R
1×2 and state sn
i = rn
i−1 ∈ R observable at the
beginning of stage i. The observed output yi ∈ Y = {0, 1}
represents the reward rn
i ∈ Rn = {0,R + rτn} received by
the player at the end of stage i, as in
yi=
{
1, rn
τ = R + rτn ,
0, rn
τ = 0.
(21)
Thus, we have a classification task where, given the dataset
(Xt ,Yt ), we must predict the random output f W(xt ) = ŷt =
(ŷ0
t , ŷ1
t ) ∈ R
1×DO of the BNN model with DO = 2 elements
being classified as 0 or 1. For a feed-forward BNN with K ≥ 1
neurons and input vector xt = (an
t , sn
t ) ∈ R
1×DI , the random
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
330 IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, VOL. 7, NO. 1, MARCH 2021
row output vector is given by [49]
f W(xt ) = ϕ
(
xtW(h)+b
)
WT
(v) = ŷt
=
(
ŷ0
t , ŷ1
t
)
∈ R
1×DO . (22)
The weights in (22) represent the tuple W =
(W(h),W(v),b) ∈ R
(DI +DO+1)×K , where
W(h) = (w i ,j
(h)
) ∈ R
DI×K and W(v) = (w i ,j
(v)
) ∈ R
DO×K
are, respectively, the weights in the hidden and visible layers;
b = (bj ) ∈ R
1×K are the biases; ϕ(·) is the logistic sigmoid
function. Similar to the prior research (e.g., [49]–[51]), we
consider a standard normal prior distribution over the weights
W, i.e., Pr{W} = N (0, I )).
As such, the prior weight distribution Pr{W} approximates
a prior belief Bn
t of player Un about unobservable actions
a−n of other players, i.e., Bn
t ∼ Pr{W}. Accordingly, the
likelihood of each data point (xt , yt ), given by
Pr
{
yt
∣∣∣ f W(xt )
}
=
⎧
⎪⎨
⎪⎩
Softmax
(
0, f W(xt )
)
= e ŷ0
t
e ŷ0
t +e ŷ1
t
, yt = 0,
Softmax
(
1, f W(xt )
)
= e ŷ1
t
e ŷ0
t +e ŷ1
t
, yt = 1
(23)
is equivalent to the probability Pr{rn
t |sn
t , an
t ,Bn
t }
of reward rn
t given the action an
t executed by
player Un in state sn
t = rn
t−1 with belief Bn
t , i.e.,
Pr{rn
t |sn
t , an
t ,Bn
t } ∼ Pr{yt |f W(xt )}. Then, the expected
stage payoff U n(Bn
t , an |sn
t ) of the player can be approx-
imated by the function U n(xt |W), as in (24), shown
at the bottom of the page, where the player’s payoff
un(sn
t , an
t , sn
t+1) = un(sn
t , an
t , rn
t ) is presented explicitly as
a function of (xt , yt ) = (sn
t , an
t , rn
t ).
Furthermore, note that in the BRL algorithm, the beliefs
are updated with Bayesian inference. That is, at any stage
t, given the prior belief Bn
t ∼ Pr{W}, we estimate a pos-
terior B
sn
t ,sn
t+1
an
t
= Bsn
t ,rn
t
an
t
∼ Pr{W|xt , yt} for an observed
transition (sn
t , an
t , sn
t+1) = (sn
t , an
t , rn
t ) = (xt , yt ) accord-
ing to Bayes’ rule in (10). As such, this Bayesian inference
is equivalent to the estimation of a posterior Pr{W|xt , yt}.
That is,
B
sn
t ,sn
t+1
an
t
=
Pr
{
sn
t
∣∣∣ sn
t−1, a
n
t−1
, a−n
}
Bn
(
a−n
)
∑
á−n∈A−n Pr
{
sn
t
∣∣∣ sn
t−1, a
n
t−1
, á−n
}
Bn(á−n)
=
Pr
{
rn
t
∣∣ sn
t−1, a
n
t−1, a
−n
}
Bn
(
a−n
)
∑
á−n∈A−n Pr
{
rn
t
∣∣ sn
t−1, a
n
t−1, á
−n
}
Bn(á−n)
∼ Pr{W | xt , yt}
=
Pr
{
yt
∣∣ f W(xt )
}
Pr{W}
Pr{yt | xt}
=
Pr
{
yt
∣∣ f W(xt )
}
Pr{W}
∫
Pr
{
yt
∣∣ f W(xt )
}
Pr{W}dW
, (25)
which is intractable, since the marginal probability distribu-
tion Pr{yt |xt} =
∫
Pr{yt |f W(xt )}Pr{W}dW cannot be
evaluated analytically. Therefore, instead of direct inference,
we utilize variational inference and, instead of integrating
over weights W, we find the parameters θ of the approxi-
mating distribution qθ which minimize a KL divergence in
(16b) or, equivalently, maximize the ELBO in (17), so that
Pr{W|xt , yt} −→
t→∞qθ(W).
To estimate variational inference, we can employ PDE,
in which case we re-parameterize an approximating distri-
bution qθ by the parameter-free distribution q(ε) subject
to Ŵt = g(θ, ε). Then, assuming a Gaussian distribution
qθ(W) = N (μ, σ2), for θ = (μ, σ), Ŵt = g(θ, ε) = μ + σε
and q(ε) =N (0, I ), the BDL problem is to find the parameters
(θ, λ) that minimize the classification loss L(θ, λ), given by
L(θ, λ) = −
t−1∑
i=t−T
(
1yi=0 log
(
Softmax
(
0, f g(θ,ε)(xi )
))
+ 1yi=1log
(
Softmax
(
1, f g(θ,ε)(xi )
)))
+
1
2
(1− λ)‖θ‖2. (26)
This problem can be solved with a stochastic gradient
descent [49], [50]. That is, at any stage t, given the dataset
(Xt , Yt ), we minimize the current loss Lt = L(θt , λt ) by
updating the parameters (θt , λt ) in the direction of a negative
gradient, as
θt+1 = θt − ηt∇θt
Lt and λt+1 = λt − ηt∇ λt
Lt , (27)
until (θt , λt ) converges, where ηt is a learning rate of gradient
descent, such that
∑∞
t=1 ηt =∞ and
∑∞
t=1 η2
t <∞, typically
set as ηt = 1/(t + η1)η2 ,∀η1 ∈ (0, 1), η2 ∈ (0, 1).
C. Practical Bayesian Deep Learning Algorithm for Miners
Based on the above, we design a practical BDL algo-
rithm for each player/miner Un in game Γ where, at any
stage t, we deploy the parameter θt = (θt(h), θt(v), θt(b)) ∈
R
(DI +DO+1)×K representing the tuple formed by elements
θt(h) = (θi ,j
t(h)
) ∈ R
DI×K , θt(v) = (θi ,j
t(v)
) ∈ R
DO×K
and θt(b) = (θj
t(b)
) ∈ R
1×K . The elements θt(h) and
U n(Bn
t , an | sn
t ) =
∑
sn
t+1∈Sn
Pr
{
sn
t+1
∣∣ sn
t , an
t ,Bn
t
}
un(sn
t , an
t , sn
t+1
)
=
∑
rn
t ∈Rn
Pr{rn
t | sn
t , an
t ,Bn
t }un(sn
t , an
t , rn
t ) ∼ U n(xt |W)
=
∑
yt∈Y
Pr
{
yt
∣∣∣ f W(xt )
}
un(xt , yt ) (24)
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: BAYESIAN REINFORCEMENT LEARNING AND BAYESIAN DEEP LEARNING 331
θt(v) define the approximated weights Ŵ(h) ∈ R
DI×K and
Ŵ(v) ∈ R
DO×K ; the element θt(b) defines the approx-
imated biases b̂ ∈ R
1×K of the BNN. Accordingly,
we re-parameterize the Gaussian approximating distribution
qθ(Wt ) =
∏
i ,j qθi,j
(h)(w i ,j
t(h)
)
∏
i ,j qθi,j
(v)(w i ,j
t(v)
)
∏
j qθi
(b)(bj
t )
by the standard normal parameter-free distribution
q(εt ) =
∏
i ,j
q
(
ε
i ,j
t(h)
)∏
i ,j
q
(
ε
i ,j
t(v)
)∏
j
q
(
ε
j
t
)
(28a)
subject to:
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
ŵ i ,j
(h)
= g
(
θ
i ,j
t(h)
, ε
i ,j
t(h)
)
,∀i ∈ {0, . . . , DI − 1},
ŵ i ,j
(v)
= g
(
θi ,j
t(v)
, εi ,j
t(v)
)
,∀i ∈ {0,DO − 1},
b̂j = g
(
θ
j
t(b)
, εit(b)
)
,
(28b)
for εt = (εt(h), εt(v), εt(b)) ∈ R
(DI +DO+1)×K , εt(h) =
(εi ,j
t(h)
) ∈ R
DI×K , εt(v) = (εi ,j
t(v)
) ∈ R
DO×K , εt(b) =
(εj
t(b)
) ∈ R
1×K , and j ∈ {0, . . . ,K − 1}.
As a result, we obtain the algorithm where at every stage
t, we store the last-updated parameters (θt+1, λt+1) and
a dataset (Xt+1, Yt+1). At stage t = 0, we initialize the
parameter θ0 according to Pr{θ0} = Pr{Ŵ} = N (0, I ). That
is, we draw the random samples θ
i ,j
0(h)
, θ
0,j
0(v)
, θ
1,j
0(v)
, θ
j
0(b)
,
i ∈ {0, . . . , DI − 1}, j ∈ {0, . . . , K − 1}, from the stan-
dard normal distribution. At stage t = 0, . . . ,T , we repeat the
following steps.
Step 1: Generate parameter εt according to (ε) = N (0, I ),
i.e., draw random samples εi ,j
0(h)
, ε0,j
0(v)
, ε1,j
0(v)
, εj
0(b)
, for i ∈
{0, . . . , DI−1}, j ∈ {0, . . . ,K−1}, from the standard normal
distribution. Given parameters (θt , εt ), estimate the weights
Ŵ = g(θt , εt ) based on (28b).
Step 2: Observe the current state sn
t = rn
t−1
and select a pure strategy αn(θt , sn
t ) =
argmaxan∈An U n(an |sn
t , g(θt , εt )) that maximizes the
approximated expected stage payoff:
U n(an | sn
t , g(θt , εt)) = U n
(
an
∣∣∣ sn
t , Ŵ
)
=
∑
yt∈Y
Pr
{
yt
∣∣∣ f Ŵ(sn
t , an)
}
un(sn
t , an , yt)
(29)
with f Ŵ(sn
t , an) = f g(θt ,εt ) and Pr{yt |f Ŵ(sn
t , an)} defined
in (22) and (23), respectively.
Step 3: Execute action an
t = αn(θt , sn
t ), observe the reward
rn
t , and update a dataset as (Xt+1, Yt+1) = (Xt , Yt ) ∪
(xt , yt )\ (xt−T , yt−T ), where (xt , yt ) = (sn
t , an
t , rn
t ) and
the parameters (θt , λt ) in the negative gradient direction as
in (27a).
Note that the above algorithm is unsupervised, i.e., it allows
“on-line” learning that does not need a prior “off-line” training
and/or pre-existing training datasets. This algorithm does not
require any additional exploration, because it is implicit in the
computation of the random BNN output f Ŵ(sn
t , an) in (22).
Proposition 3 below shows that the proposed BDL algorithm
that represents a sequence of iterations {U n
t }t∈N
, with U n
t =
U n(an |sn
t , g(θt , εt )), converges to an optimal value U n
, i.e.,
the expected stage payoff of the player from its optimal pure
strategy αn .
Proposition 3: The BDL algorithm defined by sequence of
iterations {U n
t }t∈N
, for all n ∈ N, converges to the optimal
value U n
with probability one as time tends to infinity.
The proof of Proposition 3 is given in Appendix F in
the supplementary material. From Proposition 3, we obtain
Corollary 2 that establishes that our BDL algorithm converges
to a stable state where the players’ pure strategies are in the
MPBE of game Γ.
Corollary 3: The BDL algorithm defined by the sequence
of iterations {U n
t }t∈N
, for all n ∈ N, converges to a stable
state in which the players’ pure strategies and beliefs (α,B)
are in the myopic perfect Bayesian equilibrium of game Γ.
The proof of Corollary 2 is given in Appendix G in the
supplementary material. Finally, in Proposition 4 below, we
analyze the worst-case computational complexity and the rate
of convergence of the BDL algorithm.
Proposition 4: The BDL algorithm defined by the sequence
of iterations {U n
t }t∈N
, for all n ∈ N, yields the worst-case
time complexity of O(K (c + T )), where c = |maxn∈N An |,
K is the number of neurons and T is the number of data points
in the BNN model, and the convergence rate of O(1/t1−
1
2−λ ),
where λ ∈ [0, 1) is the average dropout rate.
The proof of Proposition 4 is given in Appendix H in the
supplementary material. From Proposition 4, the convergence
rate of the BDL algorithm is sublinear with order 1− 1
2−λ ∈
(0, 0.5] and its complexity is polynomial. In particular, the
algorithm complexity depends on the size of the BNN model
defined by the product KT of the number of neurons K and
number of data point T.
VI. PERFORMANCE EVALUATION
We simulate the blockchain application realized in
the long-term evolution advanced (LTE-A) time division
duplex (TDD) [52] based MEC network. A simulation model
of the network is implemented with the OPNET development
package [53]. The model comprises M = 3 BSs represented
by the macro-cell LTE-A evolved NodeB (eNB) labeled as
BS1, a micro-cell eNB labeled as BS2, and a femto-cell
eNB labeled as BS3, that are placed as depicted in Figure 4.
The default number of miners is N = 10. Other parameters
of the simulation model are specified in Appendix I in the
supplementary material.
Note that the most efficient learning algorithm returns the
highest miners’ payoffs in a shortest time with the smallest
number of computations. Hence, in the following, we evaluate
the efficiency of the proposed BRL and BDL algorithms w.r.t.
payoffs of the miners, convergence time and complexity under
different simulation settings, and compare their performance
with that of two state-of-the-art on-line DL models:
1) UDL - unsupervised DL algorithm (described, e.g.,
in [41]) where at any stage t, miner Un selects a strategy
αn(ωt , sn
t ) = argmaxan∈An U n(an |sn
t , ωt ) maximizing its
expected stage payoff U n(an |sn
t , ωt ) which is approximated
by the NN with deterministic weights ωt . The goal is to
minimize the loss, i.e., distance between U n(an |sn
t , ωt ) and
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
332 IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, VOL. 7, NO. 1, MARCH 2021
Fig. 4. Placement of BSs in the model: service area of the MEC network coin-
cides with the macro-cell area, i.e., BS1 is placed in the center; the diameters
of BS1, BS2, and BS3 are 4000, 1000, and 200 m, respectively.
approximation target represented by the optimal expected stage
payoffs U n
.
2) DQL - deep Q-learning algorithm (described, e.g.,
in [14], [41], [48]), where miner Un selects a strategy
αn(ωt , sn
t ) = argmaxan∈An V n(an |sn
t , ωt ) that maximizes
its approximated expected long-term payoff V n(an |sn
t , ωt ).
Thus, unlike UDL where miners are myopic, here the miners
are long-visional.
We also benchmark the performance of the algorithms with
the performance of following game models:
1) CI - complete information game, where miner Un knows
current actions a−n
t of other miners. Thus, no miner Un needs
to learn the actions of its opponents. Instead, at any stage
t, it selects an action an
t that maximizes its expected one-
stage payoff U n(an |sn
t , a−n
t ). As such, the miners’ actions
at form a Nash equilibrium (NE). Note that the CI scenario
is infeasible in mobile blockchains. Thus, we use it only for
benchmarking purposes.
2) NL - game with no learning under incomplete
information, where assumptions are the same as in BRL and
BDL, i.e., no miner Un knows the actions a−n
t , . . . , a−n
0 of
other miners, but no learning is adopted. As such, each miner
Un operates under some fixed initial beliefs Bn
0 about unob-
servable actions. At any stage t, the miner selects an action an
t
that maximizes its expected one-stage payoff U n(Bn
0 , an |sn
t ),
i.e., miners’ actions at form a BNE.
Figure 5 shows dynamics of the average miner’s payoff in
all simulated learning/game models under default parameters
during the period t ∈ {30, . . . , 70} stages. Note that the aver-
age payoff is stable in CI and NL where the miners’ actions
form a NE and a BNE, respectively. Results also show that
payoffs achieved in UDL, DQL, BRL and BDL grow consis-
tently with time converging to some stable levels after about
70, 65, 55 and 60 stages, respectively. Yet, the stable-state val-
ues of the payoffs in BRL and BDL are higher than those in
UDL and DQL. In particular, in BRL and BDL, these values
approach the optimum (i.e., a NE payoff achieved in CI where
Fig. 5. Average miner’s payoff vs. mining stage, t.
Fig. 6. Convergence time vs. number of miners, N.
Fig. 7. Convergence time vs. discounting factor, γ.
miners are fully-informed about actions of other miners). In
UDL and DQL, these values are closer to that in NL (i.e.,
a BNE payoff received under fixed miners’ beliefs with no
learning adopted) and comprise less than 83% of the optimal
value. The reason for such poor performance of UDL and
DQL is that they are based on the conventional NN models.
Hence, although they can produce the accurate approximations
of fully-observable MDPs, they are not efficient for POMDPs,
as they do not have any means to track epistemic uncertainties
about unobservable states of POMDPs where these uncer-
tainties are represented by the beliefs [49]–[51]. Accordingly,
although both UDL and DQL converge to the stable states, the
values obtained in these states are far from the optimal one
and closer to that received under fixed miner’s beliefs.
Figure 6 shows the convergence times of different learn-
ing algorithms plotted as functions of the number of miners
N. From Figure 6, BRL converges faster than BDL, UDL and
DQL. The reason is that in BDL, UDL and DQL, the loss L is
non-convex w.r.t. weights of the BNN/NN models. Hence, in
general, a stochastic gradient descent applied to non-convex
function goes through several local minima until it reaches
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
ASHERALIEVA AND NIYATO: BAYESIAN REINFORCEMENT LEARNING AND BAYESIAN DEEP LEARNING 333
Fig. 8. Average complexity in log-scale vs. number of miners, N.
Fig. 9. Average complexity vs. size of NN/BNN model, KT.
a global optimum, which increases the total convergence time
of BDL, UDL and UDL. Moreover, from [14], the conver-
gence rate of UDL and DQL depends on the weights of the
NN model which are optimized during training and, hence,
cannot be predicted before we train the model. Similarly, from
Proposition 4, the convergence rate of BDL is O(1/t1−
1
2−λ ),
i.e., it depends on the average dropout rate λ that is optimized
together with the weights of the BNN. That is, the value of
λ cannot be specified in advance. As such, exact convergence
times of BDL, UDL and DQL cannot be determined prior to
training the model. They can only be estimated (e.g., based
on the training records) after the algorithms converge. On the
contrary, from Proposition 2, the convergence rate of BRL is
O(
√
log(log t)/t), for γ ≤ 0.5 and O(1/t1−γ), for γ > 0.5,
i.e., it depends only on the predefined value of a discounting
factor γ. This dependency is demonstrated in Figure 7 that
shows convergence times of different algorithms as functions
of γ. From Figure 7, for γ > 0.85, the convergence time of
BRL is larger than those of BDL, UDL and DQL for γ > 0.85;
for γ ∈ [0.1, 0.65], the convergence time of BRL is less than
that of BDL, UDL and DQL; the minimal convergence time
is achieved with γ ≈ 0.4. As such, in BRL, it is possible to
preselect the value of γ that yields the minimal convergence
time, which is not possible to do in BDL, UDL and DQL.
Figures 8 and 9 show computational complexity of different
learning algorithms measured in terms of average number of
computations as functions of the number of miners N and the
size KT of NN/BNN models, respectively. From Figure 8, the
number of computations in BRL is exponential in the number
of miners N. This conforms to the finding of Proposition 2 that
the complexity of BRL is O(cN ). As such, although the BRL
algorithm shows a superior performance, it is intractable for
a large number of miners. On the other hand, from Figure 9,
Fig. 10. Average miner’s payoff vs. number of miners, N.
Fig. 11. Average miner’s payoff vs. average computing power (BPR), xn .
Fig. 12. Average miner’s reward vs. average computing power (BPR) xn .
the numbers of computations in UDL, DQL and BDL are lin-
ear in the product of the number of neurons K and number
of data points T in the NN/BNN model. This conforms to
the findings in [14] and Proposition 4 that the complexities
of UDL and DQL are linear in KT, and complexity of BDL
is O(K (c + T )), respectively. Thus, due to its performance
and scalability (w.r.t. number of miners), a BDL algorithm is
preferable when the number of miners is large.
Figure 10 shows the impact of the number of miners N on
their average payoff in the stable states in different learning/
game models. From Figure 10, the average payoff decreases
with the number of miners N. Indeed, when N is increasing,
the ratio of miners that do not receive a reward is growing
and, hence, the average miner’s payoff is reducing. The impact
of the computing power or BPR (in ECU) xn selected by
each miner Un on its average stable-state payoff is shown in
Figure 11. From Figure 11, the miner’s payoff increases with
xn . The reason is that when xn increases, the probability Pn
s
that miner Un is the first to generate the output (which is
proportional to its relative BPR in (3b)) grows. This increases
the chances that miner Un wins a mining reward, as it follows
from Figure 12 which shows the average stable-state reward
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
334 IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, VOL. 7, NO. 1, MARCH 2021
Fig. 13. Average miner’s payoff vs. average reward, rn .
of miner Un as a function of its computing power or BPR xn .
Finally, Figure 13 shows the relationship between the reward
rn of miner Un and its average payoff achieved in the stable
states in different learning/game models. From Figure 13, the
average miner’s payoff increases with its reward rn , which
follows directly from the payoff expression in (5), i.e., the
miner’s payoff is the difference between its reward and the
cost of its selected computing power.
VII. CONCLUSION
We have presented a novel Bayesian RL and DL framework
for a stochastic game with incomplete information to model
interactions among miners in a blockchain with MEC. Within
the framework, we have formulated a BRL algorithm based
on the POMDP model of the miners’ decisions and developed
a novel unsupervised BDL algorithm where uncertainties
about unobservable states of POMDP are modeled with the
BNN. We have proven that the proposed BRL and BDL algo-
rithms converge to the stable states where the miners’ actions
form the PBE and MPBE, respectively.
REFERENCES
[1] S. Nakamoto. (2008). Bitcoin: A Peer-to-Peer Electronic Cash System.
[Online]. Available: http://bitcoin.org/bitcoin.pdf
[2] D. C. Nguyen, P. N. Pathirana, M. Ding, A. Seneviratne, “Blockchain
for 5G and beyond networks: A state of the art survey,” Dec. 2019.
[Online]. Available: arXiv:1912.05062.
[3] A. Asheralieva and D. Niyato, “Learning-based mobile edge com-
puting resource management to support public blockchain networks,”
IEEE Trans. Mobile Comput., early access, Dec. 16, 2019,
doi: 10.1109/TMC.2019.2959772.
[4] K. Yeow, A. Gani, R. W. Ahmad, J. J. P. C. Rodrigues, and K. Ko,
“Decentralized consensus for edge-centric Internet of Things: A review,
taxonomy, and research issues,” IEEE Access, vol. 6, pp. 1513–1524,
2017.
[5] W. Wang et al., “A survey on consensus mechanisms and mining
strategy management in blockchain networks,” IEEE Access, vol. 7,
pp. 22328–22370, 2019.
[6] N. Herbaut and N. Negru, “A model for collaborative blockchain-based
video delivery relying on advanced network services chains,” IEEE
Commun. Mag., vol. 55, no. 9, pp. 70–76, Sep. 2017.
[7] J. Kang, Z. Xiong, D. Niyato, D. Ye, D. Kim, and J. Zhao, “Toward
secure blockchain-enabled Internet of vehicles: Optimizing consensus
management using reputation and contract theory,” IEEE Trans. Veh.
Technol., vol. 68, no. 3, pp. 2906–2920, Mar. 2019.
[8] Z. Xiong, Y. Zhang, D. Niyato, P. Wang, and Z. Han, “When Mobile
Blockchain Meets Edge Computing,” IEEE Commun. Mag., vol. 56,
no. 8, pp. 33–39, Aug. 2018.
[9] Z. Xiong, S. Feng, W. Wang, D. Niyato, P. Wang, and Z. Han,
“Cloud/fog computing resource management and pricing for blockchain
networks,” IEEE Internet Things J., vol. 6, no. 3, pp. 4585–4600,
Jun. 2019.
[10] A. Asheralieva, “Optimal computational offloading and content caching
in wireless heterogeneous mobile edge computing systems with hop-
field neural networks,” IEEE Trans. Emerg. Topics Comput. Intell., early
access, Feb. 6, 2019, doi: 10.1109/TETCI.2019.2892733.
[11] A. Asheralieva and D. Niyato, “Hierarchical game-theoretic and
reinforcement learning framework for computational offloading in
UAV-enabled mobile edge computing networks with multiple service
providers,” IEEE Interet Things J., vol. 6, no. 5, pp. 8753–8769,
Oct. 2019.
[12] R. Yang, R. Yu, P. Si, Z. Yang, and Y. Zhang, “Integrated blockchain
and edge computing systems: A survey, some research issues and chal-
lenges,” IEEE Commun. Surveys Tuts., vol. 21, no. 2, pp. 1508–1532,
2nd Quart., 2019.
[13] N. Houy, “The bitcoin mining game,” Ledger, vol. 1, pp. 53–68,
Dec. 2016. [Online]. Available: https://ledgerjournal.org/ojs/index.php/
ledger/ article/view/13
[14] A. Asheralieva and D. Niyato, “Distributed dynamic resource manage-
ment and pricing in the IoT systems with blockchain-as-a-service and
UAV-enabled mobile edge computing,” IEEE Internet Things J., vol. 7,
no. 3, pp. 1974–1993, Mar. 2020.
[15] K. Kotobi and S. G. Bilen, “Secure blockchains for dynamic spectrum
access: A decentralized database in moving cognitive radio networks
enhances security and user access,” IEEE Veh. Technol. Mag., vol. 13,
no. 1, pp. 32–39, Mar. 2018.
[16] M. Grissa, A. A. Yavuz, and B. Hamdaoui, “TrustSAS: A trust-
worthy spectrum access system for the 3.5 GHz CBRS band,” in
Proc. IEEE INFOCOM Conf. Comput. Commun., Paris, France, 2019,
pp. 1495–1503.
[17] S. Bayhan, A. Zubow, P. Gaw, and A. Wolisz, “Smart contracts for
spectrum sensing as a service,” IEEE Trans. Cogn. Commun. Netw.,
vol. 5, no. 3, pp. 648–660, Sep. 2019.
[18] S. Bayhan, A. Zubow, and A. Wolisz, “Spass: Spectrum sensing as a ser-
vice via smart contracts,” in Proc. IEEE Int. Symp. Dyn. Spectr. Access
Netw. (DySPAN), Seoul, South Korea, 2018, pp. 1–10.
[19] F. den Hartog, F. Bouhafs, and Q. Shi, “Toward secure trading of unli-
censed spectrum in cyber-physical systems,” in Proc. 16th IEEE Annu.
Consum. Commun. Netw. Conf. (CCNC), Las Vegas, NV, USA, 2019,
pp. 1–4.
[20] M. Condoluci and T. Mahmoodi, “Softwarization and virtualization in
5G mobile networks: Benefits, trends and challenges,” Comput. Netw.,
vol. 146, pp. 65–84, Dec. 2018.
[21] G. A. F. Rebello et al., “Providing a sliced, secure, and isolated software
infrastructure of virtual functions through blockchain technology,” in
Proc. IEEE 20th Int. Conf. High Perform. Switching Routing (HPSR),
Xi’An, China, 2019, pp. 1–6.
[22] M. F. Franco, E. J. Scheid, L. Z. Granville, and B. Stiller, “BRAIN:
Blockchain-based reverse auction for infrastructure supply in virtual
network functions-as-a -service,” in Proc. IFIP Netw. Conf., Warsaw,
Poland, 2019, pp. 1–9.
[23] F. D. Calabrese, L. Wang, E. Ghadimi, G. Peters, L. Hanzo, and
P. Soldati, “Learning radio resource management in RANs: Framework,
opportunities, and challenges,” IEEE Commun. Mag., vol. 56, no. 9,
pp. 138–145, Sep. 2018.
[24] Y. Le, X. Ling, J. Wang, and Z. Ding, “Prototype design and test of
blockchain radio access network,” in Proc. IEEE Int. Conf. Commun.
Workshops (ICC Workshops), Shanghai, China, 2019, pp. 1–6.
[25] D. Li, R. Du, Y. Fu, and M. H. Au, “Meta-key: A secure data-sharing
protocol under blockchain-based decentralized storage architecture,”
IEEE Netw. Lett., vol. 1, no. 1, pp. 30–33, Mar. 2019.
[26] S. Wang, Y. Zhang, and Y. Zhang, “A blockchain-based framework for
data sharing with fine-grained access control in decentralized storage
systems,” IEEE Access, vol. 6, pp. 38437–38450, 2018.
[27] W. Liang, M. Tang, J. Long, X. Peng, J. Xu and K. Li, “A secure FaBric
blockchain-based data transmission technique for industrial Internet-of-
Things,” IEEE Trans. Ind. Informat., vol. 15, no. 6, pp. 3582–3592,
Jun. 2019.
[28] Z. Zheng, S. Xie, H. Dai, X. Chen, and H. Wang, “Blockchain challenges
and opportunities: A survey,” Int. J. Web Grid Serv., vol. 14, no. 4,
pp. 352–375, 2018.
[29] A. P. Joshi, M. Han, Y. Wang, “A survey on security and privacy
issues of blockchain technology,” Math. Found. Comput., vol. 1, no. 2,
pp. 121–147, May 2018.
[30] Y. Xiao, N. Zhang, W. Lou, and Y. T. Hou, “A survey of distributed
consensus protocols for blockchain networks,” 2019. [Online]. Available:
arXiv:1904.04098.
[31] E. Kokoris-Kogias, P. Jovanovic, L. Gasser,N. Gailly, E. Syta, and
B. Ford, “OmniLedger: A secure, scale-out, decentralized ledger via
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
http://dx.doi.org/10.1109/TMC.2019.2959772
http://dx.doi.org/10.1109/TETCI.2019.2892733
ASHERALIEVA AND NIYATO: BAYESIAN REINFORCEMENT LEARNING AND BAYESIAN DEEP LEARNING 335
sharding,” in Proc. IEEE Symp. Security and Privacy (SP), San
Francisco, CA, USA, 2018, pp. 583–598.
[32] H. Yoo, J. Yim, and S. Kim, “The blockchain for domain based
static sharding,” in Proc. 17th IEEE Int. Conf. Trust Security
Privacy Comput. Commun./ 12th IEEE Int. Conf. Big Data Sci. Eng.
(TrustCom/BigDataSE), New York, NY, USA, 2018, pp. 1689–1692.
[33] S. Forestier, D. Vodenicarevic, and A. Laversanne-Finot “Blockclique:
scaling blockchains through transaction sharding in a multithreaded
block graph,” 2018. [Online]. Available: arXiv:1803.09029.
[34] M. H. Manshaei, M. Jadliwala, A. Maiti, and M. Fooladgar, “A game-
theoretic analysis of shard-based permissionless blockchains,” IEEE
Access, vol. 6, pp. 78100–78112, 2018.
[35] S. Li, M. Yu, C.-S. Yang, A. S. Avestimehr, S. Kannan, and
P. Viswanath, “PolyShard: Coded sharding achieves linearly scaling effi-
ciency and security simultaneously,” Sep. 2018. [Online]. Available:
arXiv:1809.10361.
[36] Y. Jiao, P. Wang, D. Niyato, and Z. Xiong, “Social welfare maximization
auction in edge computing resource allocation for mobile blockchain,”
in Proc. IEEE Int. Conf. Commun. (ICC), Kansas City, MO, USA,
May 2018, pp. 1–6.
[37] N. C. Luong, Z. Xiong, P. Wang, and D. Niyato, “Optimal auction for
edge computing resource management in mobile blockchain networks:
A deep learning approach,” in Proc. IEEE Int. Conf. Commun. (ICC),
Kansas City, MO, USA, May 2018, pp. 1–6.
[38] M. Liu, F. R. Yu, Y. Teng, V. C. M. Leung, and M. Song, “Distributed
resource allocation in blockchain-based video streaming systems with
mobile edge computing,” IEEE Trans. Wireless Commun., vol. 18, no. 1,
pp. 695–708, Jan. 2019.
[39] M. Liu, F. R. Yu, Y. Teng, V. C. M. Leung, and M. Song, “Computation
offloading and content caching in wireless blockchain networks with
mobile edge computing,” IEEE Trans. Veh. Technol., vol. 67, no. 11,
pp. 11008–11021, Nov. 2018.
[40] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.
Cambridge MA, USA: MIT Press, 1998, pp. 1–356.
[41] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Artificial neu-
ral networks-based machine learning for wireless networks: A tutorial,”
IEEE Commun. Surveys Tuts., vol. 21, no. 4, pp. 3039–3071, 4th Quart.,
2019.
[42] A. Asheralieva and Y. Miyanaga, “An autonomous learning-based algo-
rithm for joint channel and power level selection by D2D pairs in
heterogeneous cellular networks,” IEEE Trans. Commun., vol. 64, no. 9,
pp. 3996–4012, Sep. 2016.
[43] A. Asheralieva and D. Niyato, “Game theory and Lyapunov optimization
for cloud-based content delivery networks with device-to-device and
UAV-enabled caching,” IEEE Trans. Veh. Technol., vol. 68, no. 10,
pp. 10094–10110, Oct. 2019.
[44] M. J. Osborne and A. Rubenstein, A Course in Game Theory. Cambridge
MA, USA: MIT Press, 1994, pp. 1–374.
[45] A. Asheralieva, “Bayesian reinforcement learning-based coalition for-
mation for distributed resource sharing by device-to-device users in
heterogeneous cellular networks,” IEEE Trans. Wireless Commun.,
vol. 16, no. 8, pp. 5016–5032, Aug. 2017.
[46] M. Ghavamzadeh, S. Mannor, J. Pineau, and A. Tamar, “Bayesian rein-
forcement learning: A survey,” Found. Trends Mach. Learn., vol. 8,
nos. 5–6, pp. 359–483, 2016.
[47] P. Poupart, N. Vlassis, J. Hoey, and K. Regan, “An analytic solution to
discrete Bayesian reinforcement learning,” in Proc. ACM 23rd Int. Conf.
Mach. Learn. (ICML), Jun. 2006, pp. 697–704.
[48] Y. Sun, M. Peng, and S. Mao, “Deep reinforcement learning-based mode
selection and resource management for green fog radio access networks,”
IEEE Internet Things J., vol. 6, no. 2, pp. 1960–1971, Apr. 2019.
[49] Y. Gal, “Uncertainty in deep learning,” Ph.D. dissertation, Dept. Eng.,
Univ. Cambridge, Cambridge, U.K, 2016.
[50] M. Segù, A. Loquercio, D. Scaramuzza, “A general framework for
uncertainty estimation in deep learning,” Jul. 2019. [Online]. Available:
arXiv:1907.06890.
[51] A. Y. Kendall and Y. Gal, “What uncertainties do we need in bayesian
deep learning for computer vision?” in Proc. 31st Int. Conf. Neural Inf.
Process. Syst. (NIPS), 2017, pp. 5580–5590.
[52] Evolved Universal Terrestrial Radio Access (E-UTRA) and Evolved
Universal Terrestrial Radio Access Network (E-UTRAN); Overall
description; Stage 2, Release 13, 3GPP Standard TS 36.300, 2016.
[53] OPNET Simulation and Development Tool. Accessed: May 2020.
[Online]. Available: http://www.opnet.com
[54] C. Szepesvári, “The asymptotic convergence-rate of Q-learning,”
in Proc. 10th Int. Conf. Adv. Neural Inf. Process. Syst., 1998,
pp. 1064–1070.
[55] M. Hardt, B. Recht, Y. Singer, “Train faster, generalize better:
Stability of stochastic gradient descent,” Sep. 2015. [Online]. Available:
arXiv:1509.01240.
[56] T. Camp, J. Boleng, and V. Davies, “A survey of mobility models for
ad hoc network research,” Wireless Commun. Mobile Comput., vol. 2,
no. 5, pp. 483–502, 2002.
Alia Asheralieva received the B.S. degree from
Kyrgyz Technical University in 2004, the M.E.
degree from the Asian Institute of Technology,
Thailand, in 2007, and the Ph.D. degree from
the University of Newcastle, Australia, in 2015.
From 2015 to 2016, she was a Research Assistant
Professor with the Graduate School of Information
Science and Technology, Hokkaido University,
Japan. In 2017, she was a Postdoctoral Research
Fellow with the Information Systems Technology
and Design Pillar, Singapore University of
Technology and Design. She is currently an Assistant Professor with the
Department of Computer Science and Engineering, Southern University of
Science and Technology, China. Her main research interests span many areas
of communications and networking, including cognitive radio networks,
heterogeneous networks, D2D and IoT communications, cloud/edge/fog com-
puting, mobile blockchains, cross-layer resource allocation and optimization,
congestion control and routing, game theory, computational and artificial
intelligence for wireless networks, as well as queuing theory, simulation and
network modeling, QoS, and performance evaluation.
Dusit Niyato (Fellow, IEEE) received the B.Eng.
degree from the King Mongkut’s Institute of
Technology Ladkrabang, Thailand, in 1999, and the
Ph.D. degree in electrical and computer engineer-
ing from the University of Manitoba, Canada, in
2008. He is currently a Professor with the School
of Computer Science and Engineering, Nanyang
Technological University, Singapore. His research
interests include energy harvesting for wireless com-
munication, Internet of Things, and sensor networks.
Authorized licensed use limited to: CITY UNIV OF HONG KONG. Downloaded on April 15,2023 at 08:19:14 UTC from IEEE Xplore.  Restrictions apply. 
<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles false
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.4
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /LeaveColorUnchanged
  /DoThumbnails false
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo false
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts false
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
    /Arial-Black
    /Arial-BoldItalicMT
    /Arial-BoldMT
    /Arial-ItalicMT
    /ArialMT
    /ArialNarrow
    /ArialNarrow-Bold
    /ArialNarrow-BoldItalic
    /ArialNarrow-Italic
    /ArialUnicodeMS
    /BookAntiqua
    /BookAntiqua-Bold
    /BookAntiqua-BoldItalic
    /BookAntiqua-Italic
    /BookmanOldStyle
    /BookmanOldStyle-Bold
    /BookmanOldStyle-BoldItalic
    /BookmanOldStyle-Italic
    /BookshelfSymbolSeven
    /Century
    /CenturyGothic
    /CenturyGothic-Bold
    /CenturyGothic-BoldItalic
    /CenturyGothic-Italic
    /CenturySchoolbook
    /CenturySchoolbook-Bold
    /CenturySchoolbook-BoldItalic
    /CenturySchoolbook-Italic
    /ComicSansMS
    /ComicSansMS-Bold
    /CourierNewPS-BoldItalicMT
    /CourierNewPS-BoldMT
    /CourierNewPS-ItalicMT
    /CourierNewPSMT
    /EstrangeloEdessa
    /FranklinGothic-Medium
    /FranklinGothic-MediumItalic
    /Garamond
    /Garamond-Bold
    /Garamond-Italic
    /Gautami
    /Georgia
    /Georgia-Bold
    /Georgia-BoldItalic
    /Georgia-Italic
    /Haettenschweiler
    /Helvetica
    /Helvetica-Bold
    /HelveticaBolditalic-BoldOblique
    /Helvetica-BoldOblique
    /Impact
    /Kartika
    /Latha
    /LetterGothicMT
    /LetterGothicMT-Bold
    /LetterGothicMT-BoldOblique
    /LetterGothicMT-Oblique
    /LucidaConsole
    /LucidaSans
    /LucidaSans-Demi
    /LucidaSans-DemiItalic
    /LucidaSans-Italic
    /LucidaSansUnicode
    /Mangal-Regular
    /MicrosoftSansSerif
    /MonotypeCorsiva
    /MSReferenceSansSerif
    /MSReferenceSpecialty
    /MVBoli
    /PalatinoLinotype-Bold
    /PalatinoLinotype-BoldItalic
    /PalatinoLinotype-Italic
    /PalatinoLinotype-Roman
    /Raavi
    /Shruti
    /Sylfaen
    /SymbolMT
    /Tahoma
    /Tahoma-Bold
    /Times-Bold
    /Times-BoldItalic
    /Times-Italic
    /TimesNewRomanMT-ExtraBold
    /TimesNewRomanPS-BoldItalicMT
    /TimesNewRomanPS-BoldMT
    /TimesNewRomanPS-ItalicMT
    /TimesNewRomanPSMT
    /Times-Roman
    /Trebuchet-BoldItalic
    /TrebuchetMS
    /TrebuchetMS-Bold
    /TrebuchetMS-Italic
    /Tunga-Regular
    /Verdana
    /Verdana-Bold
    /Verdana-BoldItalic
    /Verdana-Italic
    /Vrinda
    /Webdings
    /Wingdings2
    /Wingdings3
    /Wingdings-Regular
    /ZapfChanceryITCbyBT-MediumItal
    /ZWAdobeF
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 200
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages false
  /ColorImageDownsampleType /Average
  /ColorImageResolution 300
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 200
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages false
  /GrayImageDownsampleType /Average
  /GrayImageResolution 300
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 400
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages false
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False
  /CreateJDFFile false
  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Recommended"  settings for PDF Specification 4.01)
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice